#!/usr/bin/env python3
"""
999 ìŠ¤í… ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ê°„ë‹¨í•œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
í‰ê°€ ì½œë°±ì„ ìµœì†Œí™”í•˜ì—¬ í•™ìŠµ ì¤‘ë‹¨ ë¬¸ì œ ë°©ì§€
"""

import os
import sys
import time
import datetime
import warnings

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore")

# ê²½ë¡œ ì„¤ì •
sys.path.append('src')
os.environ['PYTHONPATH'] = os.getcwd() + ':' + os.getcwd() + '/src'

from stable_baselines3 import PPO
from sb3_contrib import MaskablePPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_checker import check_env

# ë¡œì»¬ ëª¨ë“ˆ import
from packing_kernel import *
from train_maskable_ppo import make_env, get_action_masks

def quick_train(timesteps=3000, eval_freq=1500):
    """999 ìŠ¤í… ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë¹ ë¥¸ í•™ìŠµ"""
    
    print(f"=== ë¹ ë¥¸ í•™ìŠµ ì‹œì‘ ===")
    print(f"ëª©í‘œ ìŠ¤í…: {timesteps:,}")
    print(f"í‰ê°€ ì£¼ê¸°: {eval_freq:,}")
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í™˜ê²½ ìƒì„± (ê°„ë‹¨í•œ ì„¤ì •)
    env = make_env(
        container_size=[10, 10, 10],
        num_boxes=16,  # ì ì€ ë°•ìŠ¤ ìˆ˜ë¡œ ì‹œì‘
        num_visible_boxes=3,
        seed=42,
        render_mode=None,
        random_boxes=False,
        only_terminal_reward=False,
        improved_reward_shaping=True,
    )()
    
    # í‰ê°€ìš© í™˜ê²½
    eval_env = make_env(
        container_size=[10, 10, 10],
        num_boxes=16,
        num_visible_boxes=3,
        seed=43,
        render_mode=None,
        random_boxes=False,
        only_terminal_reward=False,
        improved_reward_shaping=True,
    )()
    
    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    env = Monitor(env, f"logs/quick_train_{timestamp}.csv")
    eval_env = Monitor(eval_env, f"logs/quick_eval_{timestamp}.csv")
    
    print("í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    print(f"ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤: {env.action_space}")
    print(f"ê´€ì°° ìŠ¤í˜ì´ìŠ¤: {env.observation_space}")
    
    # ê°„ë‹¨í•œ ì½œë°±ë§Œ ì‚¬ìš©
    callbacks = []
    
    # ìµœì†Œí•œì˜ í‰ê°€ ì½œë°±ë§Œ ì‚¬ìš©
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="models/best_quick",
        log_path="logs/quick_eval",
        eval_freq=eval_freq,
        n_eval_episodes=3,  # ì ì€ í‰ê°€ ì—í”¼ì†Œë“œ
        deterministic=True,
        render=False,
        verbose=1
    )
    callbacks.append(eval_callback)
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=eval_freq,
        save_path="models/quick_checkpoints",
        name_prefix=f"quick_model_{timestamp}",
        verbose=1
    )
    callbacks.append(checkpoint_callback)
    
    # ê°„ë‹¨í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    model = MaskablePPO(
        "MultiInputPolicy",
        env,
        learning_rate=3e-4,
        n_steps=512,
        batch_size=64,
        n_epochs=5,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=1,
        tensorboard_log="logs/quick_tensorboard"
    )
    
    print("\n=== í•™ìŠµ ì‹œì‘ ===")
    start_time = time.time()
    
    try:
        # í•™ìŠµ ì‹¤í–‰
        model.learn(
            total_timesteps=timesteps,
            callback=callbacks,
            progress_bar=True,
            tb_log_name=f"quick_maskable_ppo_{timestamp}",
        )
        
        training_time = time.time() - start_time
        print(f"\ní•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {training_time:.2f}ì´ˆ")
        
        # ëª¨ë¸ ì €ì¥
        model_path = f"models/quick_ppo_mask_{timestamp}"
        model.save(model_path)
        print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        # ê°„ë‹¨í•œ í‰ê°€
        print("\n=== ìµœì¢… í‰ê°€ ===")
        mean_reward, std_reward = evaluate_policy(
            model, eval_env, n_eval_episodes=5, deterministic=True
        )
        
        print(f"í‰ê°€ ì™„ë£Œ:")
        print(f"  í‰ê·  ë³´ìƒ: {mean_reward:.4f} Â± {std_reward:.4f}")
        
        # í™˜ê²½ ì •ë¦¬
        env.close()
        eval_env.close()
        
        return model_path, mean_reward
        
    except KeyboardInterrupt:
        print("\ní•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        model.save(f"models/interrupted_quick_{timestamp}")
        return None, None
    
    except Exception as e:
        print(f"\ní•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        model.save(f"models/error_quick_{timestamp}")
        raise e

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ë¹ ë¥¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--timesteps", type=int, default=3000, help="ì´ í•™ìŠµ ìŠ¤í…")
    parser.add_argument("--eval-freq", type=int, default=1500, help="í‰ê°€ ì£¼ê¸°")
    
    args = parser.parse_args()
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("models", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    print("ğŸš€ ë¹ ë¥¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    
    try:
        model_path, reward = quick_train(
            timesteps=args.timesteps,
            eval_freq=args.eval_freq
        )
        
        if model_path:
            print(f"\nğŸ‰ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            print(f"ëª¨ë¸: {model_path}")
            print(f"ì„±ëŠ¥: {reward:.4f}")
        else:
            print("âŒ í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1) 