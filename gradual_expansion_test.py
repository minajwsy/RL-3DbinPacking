#!/usr/bin/env python3
"""
ì ì§„ì  í™•ì¥ í…ŒìŠ¤íŠ¸ - ê¸°ì¡´ PyTorch í™˜ê²½ ìœ ì§€
"""

import os
import sys

os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
sys.path.append('src')

print("ğŸ“ˆ ì ì§„ì  í™•ì¥ í…ŒìŠ¤íŠ¸")

OPTIMAL_PARAMS = {
    'learning_rate': 2.6777169756959113e-06,
    'n_steps': 64,      # ì¡°ê¸ˆ ë” í¬ê²Œ
    'batch_size': 8,    # ì¡°ê¸ˆ ë” í¬ê²Œ
    'n_epochs': 2,      # 2 ì—í¬í¬
    'clip_range': 0.17716239549317803,
    'ent_coef': 0.06742268917730829,
    'vf_coef': 0.4545305173856873,
    'gae_lambda': 0.9449228658070746
}

def gradual_test():
    try:
        import torch
        torch.set_default_tensor_type('torch.FloatTensor')
        torch.set_num_threads(2)
        
        import gymnasium as gym
        from gymnasium.envs.registration import register
        from packing_env import PackingEnv, utils
        
        if 'PackingEnv-v0' not in gym.envs.registry:
            register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')
        
        # ì¡°ê¸ˆ ë” í° ë¬¸ì œ
        from utils import boxes_generator
        box_sizes = boxes_generator([6, 6, 6], 4, 42)  # 6x6x6, 4ê°œ ë°•ìŠ¤
        env = gym.make(
            "PackingEnv-v0",
            container_size=[6, 6, 6],
            box_sizes=box_sizes,
            num_visible_boxes=2,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
        )
        
        from sb3_contrib import MaskablePPO
        import torch.nn as nn
        
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=OPTIMAL_PARAMS['learning_rate'],
            n_steps=OPTIMAL_PARAMS['n_steps'],
            batch_size=OPTIMAL_PARAMS['batch_size'],
            n_epochs=OPTIMAL_PARAMS['n_epochs'],
            gamma=0.99,
            gae_lambda=OPTIMAL_PARAMS['gae_lambda'],
            clip_range=OPTIMAL_PARAMS['clip_range'],
            ent_coef=OPTIMAL_PARAMS['ent_coef'],
            vf_coef=OPTIMAL_PARAMS['vf_coef'],
            max_grad_norm=0.5,
            verbose=0,
            device='cpu',
            policy_kwargs=dict(
                net_arch=[32, 32],  # ì¡°ê¸ˆ ë” í° ë„¤íŠ¸ì›Œí¬
                activation_fn=nn.ReLU,
            )
        )
        
        print("ğŸ“ í™•ì¥ëœ í•™ìŠµ ì‹œì‘ (200 ìŠ¤í…)...")
        import time
        start_time = time.time()
        
        model.learn(total_timesteps=200, progress_bar=False)
        
        training_time = time.time() - start_time
        print(f"âœ… í™•ì¥ëœ í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")
        
        # 3íšŒ í‰ê°€
        rewards = []
        for i in range(3):
            obs, _ = env.reset()
            total_reward = 0
            for step in range(8):  # 8ìŠ¤í…ì”©
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                total_reward += reward
                if terminated or truncated:
                    break
            rewards.append(total_reward)
            print(f"   ì—í”¼ì†Œë“œ {i+1}: ë³´ìƒ={total_reward:.3f}")
        
        avg_reward = sum(rewards) / len(rewards)
        print(f"\nğŸ‰ í™•ì¥ëœ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"   í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
        print(f"   í•™ìŠµ ì‹œê°„: {training_time:.1f}ì´ˆ")
        
        env.close()
        del model
        import gc
        gc.collect()
        
        return True
        
    except Exception as e:
        print(f"âŒ í™•ì¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    success = gradual_test()
    if success:
        print("\nâœ… í™•ì¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ë” í° ì‹¤í—˜ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("\nâŒ í™•ì¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
