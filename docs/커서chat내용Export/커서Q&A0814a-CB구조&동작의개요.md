# ì½”ë“œë² ì´ìŠ¤ êµ¬ì¡° ë° ì£¼ì„ ì¶”ê°€ ìš”ì²­
_Exported on 2025. 8. 14. at 10ì‹œ 59ë¶„ 34ì´ˆ GMT+9 from Cursor (1.4.3)_

---

**User**

@RL-3DbinPacking/ @ì»¤ì„œchatë‚´ìš©Export/ ì²¨ë¶€ì˜ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ì—¬ ì´ ì½”ë“œë² ì´ìŠ¤ ì „ì²´ì˜ êµ¬ì¡°ì™€ ë™ì‘ì„ ì•Œê¸° ì‰½ê²Œ ì„¤ëª…í•œ ë‹¤ìŒì—, ìµœìƒìœ„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ì¸ enhanced_optimization.pyì™€ production_final_test.pyì— ì£¼ì„ì„ ë‹¬ì•„ì„œ ê°ê° ë³„ë„ì˜ íŒŒì¼ë¡œ ë§Œë“¤ì–´ì¤˜(Think hard!) 

---

**Cursor**

ë°©ê¸ˆ ì €ì¥ì†Œ êµ¬ì¡°ì™€ ë‘ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•´ ì„¤ëª…ê³¼ ì£¼ì„ ì‘ì—… ë²”ìœ„ë¥¼ ì¡ê² ìŠµë‹ˆë‹¤. ë¨¼ì € `enhanced_optimization.py`, `production_final_test.py`, `README.md`ë¥¼ ì½ì–´ ì „ì²´ íë¦„ê³¼ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ë¥¼ ì´í•´í•˜ê² ìŠµë‹ˆë‹¤.

ìƒìœ„ êµ¬ì¡°ë¥¼ ì •í™•íˆ ì„¤ëª…í•˜ê³  ì£¼ì„ ë²„ì „ì„ ë§Œë“¤ê¸° ìœ„í•´ í•µì‹¬ ëª¨ë“ˆ ìœ„ì¹˜ì™€ ë‚´ìš©ì„ í•œêº¼ë²ˆì— ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. `src/packing_env.py`, `src/train_maskable_ppo.py`, `src/utils.py`, `src/packing_engine.py`ë¥¼ ì°¾ê³  í•„ìš”í•œ ë¶€ë¶„ì„ ì½ê² ìŠµë‹ˆë‹¤.

ì§€ê¸ˆ ì½”ë“œë² ì´ìŠ¤ì˜ í•µì‹¬ ëª¨ë“ˆ íŒŒì¼ë“¤ì„ ë¹ ë¥´ê²Œ í™•ì¸í•´ ì „ì²´ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ê³ , ë‘ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ì— ì£¼ì„ì„ ì¶”ê°€í•œ ë³„ë„ íŒŒì¼ì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. ë¨¼ì € `src` ë””ë ‰í„°ë¦¬ì˜ êµ¬ì„± íŒŒì¼ì„ í™•ì¸í•©ë‹ˆë‹¤.

ë°©ê¸ˆ ì „ì²´ êµ¬ì¡°ë¥¼ íŒŒì•…í–ˆê³ , ì£¼ì„ì´ í’ë¶€í•œ ë³„ë„ ì‹¤í–‰ íŒŒì¼ ë‘ ê°œë¥¼ ë£¨íŠ¸ì— ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤: `enhanced_optimization_annotated.py`, `production_final_test_annotated.py`. ì›ë³¸ ë¡œì§ì€ ìœ ì§€í•˜ê³  í•œêµ­ì–´ ì„¤ëª…ê³¼ ì‹¤í–‰ íŒì„ ì¶”ê°€í•©ë‹ˆë‹¤. ë°”ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
"""
3D Bin Packing ê°•í™”í•™ìŠµ ìµœì í™”(Phase 4) ì£¼ì„ í™•ì¥íŒ

ê°œìš”
- ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” Maskable PPOë¥¼ í™œìš©í•´ 3D ë¹ˆíŒ¨í‚¹ í™˜ê²½(`src/packing_env.py`)ì—ì„œ
  ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° êµ¬ì„±ì˜ ì„±ëŠ¥ì„ ë¹„êµÂ·í‰ê°€í•©ë‹ˆë‹¤.
- ê´€ì°°(Observation)ì€ ì»¨í…Œì´ë„ˆ ìƒë‹¨ ë†’ì´ë§µê³¼ ê°€ì‹œ ë°•ìŠ¤ì˜ í¬ê¸° ë²¡í„°ì´ë©°,
  í–‰ë™(Action)ì€ [ê°€ì‹œ ë°•ìŠ¤ ì„ íƒ Ã— XY ì¢Œí‘œ]ì˜ ë‹¨ì¼ ì •ìˆ˜ indexë¡œ ë””ìŠ¤í¬ë¦¬íŠ¸í™”ë©ë‹ˆë‹¤.
- ë…¼ë¬¸ ë§¥ë½: ì²¨ë¶€ ë¬¸í—Œ(Transformer ê¸°ë°˜ DRL)ê³¼ ë‹¬ë¦¬ ë³¸ ì½”ë“œëŠ” MLP ê¸°ë°˜ MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  ê³µí†µì ìœ¼ë¡œ ë†’ì´ë§µ ê¸°ë°˜ ìƒíƒœí‘œí˜„, ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹, ë³´ìƒ ì‰ì´í•‘ì„ í†µí•´ íƒìƒ‰ ê³µê°„ì„ ì¤„ì´ê³  ì•ˆì •ì  í•™ìŠµì„ ì§€í–¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•(ì˜ˆ)
- ëª¨ë“  ì„¸íŠ¸ ì‹¤í—˜:  python enhanced_optimization_annotated.py --focus all --timesteps 35000
- ì•ˆì •ì„± ì¤‘ì‹¬:     python enhanced_optimization_annotated.py --focus stability
- ê²°ê³¼ ë¶„ì„ë§Œ:      python enhanced_optimization_annotated.py --analyze results/phase4_enhanced_*.json

ì¶œë ¥
- `results/phase4_enhanced_{focus}_{timestamp}.json` ì¢…í•© ê²°ê³¼ì™€ `_analysis.png` ì°¨íŠ¸ë¥¼ ìë™ ì €ì¥
"""

import os
import sys
import json
import time
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import warnings

# í™˜ê²½ ì„¤ì •: ì„œë²„/í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")

# src í´ë”ë¥¼ import ê²½ë¡œì— ì¶”ê°€
sys.path.append('src')

import gymnasium as gym
from gymnasium.envs.registration import register
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
import torch
import matplotlib.pyplot as plt
import seaborn as sns

# í”„ë¡œì íŠ¸ í•µì‹¬ ëª¨ë“ˆ ë¡œë“œ
try:
    from packing_env import PackingEnv  # ìƒíƒœÂ·í–‰ë™ ì •ì˜ëœ Gym í™˜ê²½
    from train_maskable_ppo import ImprovedRewardWrapper  # ê°œì„ í˜• ë³´ìƒ ë˜í¼
    from utils import boxes_generator  # ë¬¸ì œ ì¸ìŠ¤í„´ìŠ¤(ë°•ìŠ¤ ì‹œí€€ìŠ¤) ìƒì„±ê¸°
    print("âœ… ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ Import ì˜¤ë¥˜: {e}")
    print("src í´ë”ì™€ í•„ìš”í•œ ëª¨ë“ˆë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)


def get_env_info(env):
    """ë˜í¼ë¥¼ ë²—ê²¨ ì‹¤ì œ `PackingEnv`ì— ì ‘ê·¼í•´ ì»¨í…Œì´ë„ˆ í¬ê¸°/ë°•ìŠ¤ ê°œìˆ˜ ìš”ì•½.

    ë°˜í™˜ê°’
    - (container_size, box_count)
    """
    try:
        unwrapped_env = env
        while hasattr(unwrapped_env, 'env'):
            unwrapped_env = unwrapped_env.env

        if hasattr(unwrapped_env, 'container') and hasattr(unwrapped_env.container, 'size'):
            container_size = unwrapped_env.container.size
        else:
            container_size = [10, 10, 10]

        if hasattr(unwrapped_env, 'initial_boxes'):
            box_count = len(unwrapped_env.initial_boxes)
        elif hasattr(unwrapped_env, 'num_initial_boxes'):
            box_count = unwrapped_env.num_initial_boxes
        else:
            box_count = 12

        return container_size, box_count
    except Exception as e:
        print(f"âš ï¸ í™˜ê²½ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return [10, 10, 10], 12


def calculate_utilization_and_items(env):
    """ìµœì¢… ì‹œì  ì»¨í…Œì´ë„ˆ í™œìš©ë¥ ê³¼ ë°°ì¹˜ëœ ë°•ìŠ¤ ê°œìˆ˜ ì‚°ì¶œ.

    - í™œìš©ë¥  = (ë°°ì¹˜ëœ ë°•ìŠ¤ ì´ë¶€í”¼) / (ì»¨í…Œì´ë„ˆ ë¶€í”¼)
    - ë°°ì¹˜ íŒì •: positionì´ [-1,-1,-1]ì´ ì•„ë‹Œ ë°•ìŠ¤
    """
    try:
        unwrapped_env = env
        while hasattr(unwrapped_env, 'env'):
            unwrapped_env = unwrapped_env.env

        if hasattr(unwrapped_env, 'container'):
            placed_volume = 0
            placed_count = 0
            for box in unwrapped_env.container.boxes:
                if hasattr(box, 'position') and box.position is not None:
                    if not (box.position[0] == -1 and box.position[1] == -1 and box.position[2] == -1):
                        placed_volume += box.volume
                        placed_count += 1

            container_volume = unwrapped_env.container.volume
            utilization = placed_volume / container_volume if container_volume > 0 else 0.0
            return utilization, placed_count
        else:
            return 0.0, 0
    except Exception as e:
        print(f"âš ï¸ í™œìš©ë¥  ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0, 0


class EnhancedOptimizer:
    """Phase 4 ì •ë°€ ìµœì í™” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.

    - ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ìˆœíšŒ í•™ìŠµ/í‰ê°€í•˜ì—¬ ì¢…í•© ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
    - ì ìˆ˜ = 0.3Ã—í‰ê·  ë³´ìƒ + 0.7Ã—(í‰ê·  í™œìš©ë¥ [%])
    """

    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)

        # Phase 3 ìµœê³  ì„±ëŠ¥ ê¸°ì¤€ì  (ë¹„êµÂ·ê°œì„ ìœ¨ ê³„ì‚°ì— ì‚¬ìš©)
        self.phase3_best = {
            'score': 16.116,
            'params': {
                'learning_rate': 0.00015,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [256, 128, 64]
            }
        }

        self.target_score = 18.57
        self.improvement_needed = (self.target_score - self.phase3_best['score']) / self.phase3_best['score']

        print(f"ğŸ¯ Phase 4 Enhanced Optimization ì‹œì‘")
        print(f"ğŸ“Š ê¸°ì¤€ì : {self.phase3_best['score']:.3f}ì ")
        print(f"ğŸ† ëª©í‘œ: {self.target_score}ì  ({self.improvement_needed:.1%} ê°œì„  í•„ìš”)")

    def create_enhanced_environment(self, num_boxes: int = 12, container_size: List[int] = [10, 10, 10],
                                    enhanced_reward: bool = True, seed: int = 42) -> gym.Env:
        """ì‹¤í—˜ìš© í™˜ê²½ ìƒì„±.

        - `utils.boxes_generator`ë¡œ ê³ ì • ì»¨í…Œì´ë„ˆì— ìƒì‘í•˜ëŠ” ë°•ìŠ¤ ì‹œí€€ìŠ¤ ìƒì„±
        - `ImprovedRewardWrapper` ë˜ëŠ” ë³¸ íŒŒì¼ì˜ `EnhancedRewardWrapper` ì ìš©
        - `ActionMasker`ë¡œ ë¶ˆê°€ëŠ¥ í–‰ë™ì„ ë§ˆìŠ¤í‚¹í•˜ì—¬ ìƒ˜í”Œ íš¨ìœ¨ ê°œì„ 
        """
        try:
            print(f"ìƒì„±ëœ ë°•ìŠ¤ ê°œìˆ˜: {num_boxes}")
            print(f"ì»¨í…Œì´ë„ˆ í¬ê¸°: {container_size}")

            # Gym í™˜ê²½ ë“±ë¡ (ì¤‘ë³µ ë“±ë¡ ë°©ì§€)
            if 'PackingEnv-v0' not in gym.envs.registry:
                register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')

            # ë°•ìŠ¤ ì‹œí€€ìŠ¤ ìƒì„±
            box_sizes = boxes_generator(container_size, num_boxes, seed)

            # í™˜ê²½ ìƒì„±
            env = gym.make(
                "PackingEnv-v0",
                container_size=container_size,
                box_sizes=box_sizes,
                num_visible_boxes=min(3, num_boxes),
                render_mode=None,
                random_boxes=False,
                only_terminal_reward=False,
            )
            print("í™˜ê²½ ìƒì„± ì„±ê³µ: PackingEnv-v0")

            # ë³´ìƒ ë˜í¼ ì„ íƒ ì ìš©
            if enhanced_reward:
                env = EnhancedRewardWrapper(env)
                print("ê°•í™”ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")
            else:
                env = ImprovedRewardWrapper(env)
                print("ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")

            # ì•¡ì…˜ ë§ˆìŠ¤í‚¹ (ë¶ˆê°€ëŠ¥ ì¢Œí‘œ/ë°•ìŠ¤ ì„ íƒ ì œê±°)
            def get_action_masks(env):
                try:
                    unwrapped_env = env
                    while hasattr(unwrapped_env, 'env'):
                        unwrapped_env = unwrapped_env.env

                    if hasattr(unwrapped_env, 'action_masks'):
                        masks = unwrapped_env.action_masks()
                        if isinstance(masks, list):
                            return np.array(masks, dtype=bool)
                        return masks
                    return np.ones(env.action_space.n, dtype=bool)
                except Exception as e:
                    print(f"âš ï¸ ì•¡ì…˜ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
                    return np.ones(env.action_space.n, dtype=bool)

            env = ActionMasker(env, get_action_masks)
            print("ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ë˜í¼ ì ìš©ë¨")

            print(f"ì‹œë“œ ì„¤ì • ì™„ë£Œ: {seed}")
            return env

        except Exception as e:
            print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
            raise e

    def get_enhanced_parameter_sets(self) -> Dict[str, Dict]:
        """íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸íŠ¸ ì‚¬ì „.

        - ì•ˆì •ì„± ê°•í™”, ì•„í‚¤í…ì²˜ ë³€í™”, ì ê·¹/ë³´ìˆ˜ ìµœì í™” ë“± ì¹´í…Œê³ ë¦¬ë¡œ êµ¬ì„±
        - ê° í•­ëª©ì€ MaskablePPOì˜ ì£¼ìš” ì¸ìì™€ `policy_kwargs.net_arch`ë¥¼ í¬í•¨
        """
        # 1) í•™ìŠµ ì•ˆì •ì„± ê°•í™” ì„¸íŠ¸
        stability_sets = {
            'stability_conservative': {
                'learning_rate': 1.2e-04,
                'n_steps': 1024,
                'batch_size': 64,
                'n_epochs': 6,
                'clip_range': 0.15,
                'ent_coef': 0.005,
                'vf_coef': 0.5,
                'gae_lambda': 0.98,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            },
            'stability_balanced': {
                'learning_rate': 1.3e-04,
                'n_steps': 768,
                'batch_size': 96,
                'n_epochs': 5,
                'clip_range': 0.18,
                'ent_coef': 0.008,
                'vf_coef': 0.5,
                'gae_lambda': 0.96,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            }
        }

        # 2) ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ ë³€í™” ì„¸íŠ¸ (í­/ê¹Šì´/ê· í˜•)
        architecture_sets = {
            'arch_wide': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[512, 256, 128], vf=[512, 256, 128])]
            },
            'arch_deep': {
                'learning_rate': 1.4e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[256, 256, 128, 64], vf=[256, 256, 128, 64])]
            },
            'arch_balanced': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[384, 192, 96], vf=[384, 192, 96])]
            },
            'arch_reinforced': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[256, 128, 128, 64], vf=[256, 128, 128, 64])]
            }
        }

        # 3) í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¹ë‹¨/ì •ë°€ ì„¸íŠ¸
        optimized_sets = {
            'opt_precision': {
                'learning_rate': 1.1e-04,
                'n_steps': 1536,
                'batch_size': 192,
                'n_epochs': 8,
                'clip_range': 0.12,
                'ent_coef': 0.003,
                'vf_coef': 0.6,
                'gae_lambda': 0.99,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            },
            'opt_aggressive': {
                'learning_rate': 1.8e-04,
                'n_steps': 256,
                'batch_size': 64,
                'n_epochs': 3,
                'clip_range': 0.25,
                'ent_coef': 0.02,
                'vf_coef': 0.4,
                'gae_lambda': 0.92,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            }
        }

        all_sets = {}
        all_sets.update(stability_sets)
        all_sets.update(architecture_sets)
        all_sets.update(optimized_sets)
        return all_sets

    def train_and_evaluate(self, params: Dict, name: str, timesteps: int = 35000,
                           eval_episodes: int = 25, enhanced_reward: bool = True) -> Dict[str, Any]:
        """ë‹¨ì¼ êµ¬ì„± í•™ìŠµ ë° í‰ê°€ ë£¨í‹´.

        - MaskablePPOë¡œ timesteps ë§Œí¼ í•™ìŠµ í›„, ì—¬ëŸ¬ ì‹œë“œë¡œ ì§§ì€ ì—í”¼ì†Œë“œ í‰ê°€
        - ë…¼ë¬¸ ê´€ì : ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ íƒìƒ‰ ê³µê°„ì„ ì¶•ì†Œí•´ ìƒ˜í”Œ íš¨ìœ¨ê³¼ ì•ˆì •ì„±ì„ í–¥ìƒ
        """
        print(f"\nğŸ”§ {name} ìµœì í™” ì¤‘...")

        env = self.create_enhanced_environment(enhanced_reward=enhanced_reward, seed=42)
        container_size, box_count = get_env_info(env)
        print(f"âœ… í™˜ê²½ ìƒì„± ì„±ê³µ: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{box_count}ê°œ")

        model = MaskablePPO(
            'MultiInputPolicy',
            env,
            learning_rate=params['learning_rate'],
            n_steps=params['n_steps'],
            batch_size=params['batch_size'],
            n_epochs=params['n_epochs'],
            clip_range=params['clip_range'],
            ent_coef=params['ent_coef'],
            vf_coef=params['vf_coef'],
            gae_lambda=params['gae_lambda'],
            policy_kwargs={'net_arch': params['net_arch']},
            verbose=0,
            device='auto'
        )

        print(f"ğŸ“ {name} í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í… (LR: {params['learning_rate']:.2e}, Net: {params['net_arch']})")
        start_time = time.time()
        model.learn(total_timesteps=timesteps)
        training_time = time.time() - start_time
        print(f"â±ï¸ {name} í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")

        # í‰ê°€: ë™ì¼ í™˜ê²½ ì„¤ì •ì—ì„œ ì‹œë“œë§Œ ë‹¤ë¥´ê²Œ ë°˜ë³µ
        print(f"ğŸ” {name} í‰ê°€ ì‹œì‘ ({eval_episodes} ì—í”¼ì†Œë“œ, ìµœëŒ€ 25 ìŠ¤í…)")
        rewards, utilizations, placements = [], [], []
        for i in range(eval_episodes):
            eval_env = self.create_enhanced_environment(enhanced_reward=enhanced_reward, seed=100 + i * 5)
            container_size, box_count = get_env_info(eval_env)
            obs = eval_env.reset(seed=100 + i * 5)
            if isinstance(obs, tuple):
                obs = obs[0]

            episode_reward = 0
            step_count = 0
            max_steps = 25
            while step_count < max_steps:
                try:
                    # í‰ê°€ ì‹œì—ë„ í™•ë¥ ì  í–‰ë™ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´
                    action, _ = model.predict(obs, deterministic=False)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    step_count += 1
                    if terminated or truncated:
                        break
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    break

            final_utilization, placement_count = calculate_utilization_and_items(eval_env)
            rewards.append(episode_reward)
            utilizations.append(final_utilization)
            placements.append(placement_count)

            if i < 6 or i in [10, 15, 20] or i == eval_episodes - 1:
                print(f"   ì—í”¼ì†Œë“œ {i+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={final_utilization:.1%}, ë°•ìŠ¤={placement_count}ê°œ")
            eval_env.close()

        mean_reward = np.mean(rewards)
        std_reward = np.std(rewards)
        mean_utilization = np.mean(utilizations)
        std_utilization = np.std(utilizations)
        mean_placement = np.mean(placements)
        max_placement = np.max(placements)
        success_count = sum(1 for p in placements if p >= 5)
        success_rate = success_count / eval_episodes
        combined_score = mean_reward * 0.3 + mean_utilization * 100 * 0.7

        print(f"ğŸ“Š {name} ìµœì¢… ê²°ê³¼:")
        print(f"   í‰ê·  ë³´ìƒ: {mean_reward:.3f} Â± {std_reward:.3f}")
        print(f"   í‰ê·  í™œìš©ë¥ : {mean_utilization:.1%} Â± {std_utilization:.1%}")
        print(f"   í‰ê·  ë°°ì¹˜: {mean_placement:.1f}ê°œ (ìµœëŒ€: {max_placement}ê°œ)")
        print(f"   ì„±ê³µë¥ : {success_rate:.1%}")
        print(f"   ì¢…í•© ì ìˆ˜: {combined_score:.3f}")

        env.close()
        return {
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'mean_utilization': mean_utilization,
            'std_utilization': std_utilization,
            'mean_placement': mean_placement,
            'max_placement': max_placement,
            'success_rate': success_rate,
            'combined_score': combined_score,
            'episodes': eval_episodes,
            'training_time': training_time,
            'params': params
        }

    def run_phase4_optimization(self, focus: str = 'all', timesteps: int = 35000) -> Dict:
        """Phase 4 ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ ë° ê²°ê³¼ ì§‘ê³„/ì €ì¥."""
        print(f"\n{'='*60}")
        print(f"ğŸš€ Phase 4 Enhanced Optimization ì‹œì‘")
        print(f"ğŸ¯ í¬ì»¤ìŠ¤: {focus}")
        print(f"â±ï¸ í•™ìŠµ ìŠ¤í…: {timesteps:,}")
        print(f"{'='*60}")

        all_params = self.get_enhanced_parameter_sets()
        results: Dict[str, Any] = {}
        best_score = 0
        best_config = None

        # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
        if focus == 'stability':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('stability')}
        elif focus == 'architecture':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('arch')}
        elif focus == 'optimization':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('opt')}
        else:
            params_to_test = all_params

        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸í•  ì„¤ì •: {len(params_to_test)}ê°œ")

        total_start_time = time.time()
        for i, (name, params) in enumerate(params_to_test.items(), 1):
            print(f"\n[{i}/{len(params_to_test)}] {name} í…ŒìŠ¤íŠ¸ ì¤‘...")
            try:
                result = self.train_and_evaluate(
                    params, name, timesteps=timesteps, enhanced_reward=True
                )
                results[name] = result
                if result['combined_score'] > best_score:
                    best_score = result['combined_score']
                    best_config = name
                    print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥: {best_score:.3f}ì ")
            except Exception as e:
                print(f"âŒ {name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                continue

        total_time = time.time() - total_start_time

        if results:
            print(f"\n{'='*60}")
            print(f"ğŸ† Phase 4 ìµœì í™” ê²°ê³¼")
            print(f"{'='*60}")

            sorted_results = sorted(results.items(), key=lambda x: x[1]['combined_score'], reverse=True)
            print("ìˆœìœ„  ì„¤ì •ëª…                    ì ìˆ˜      ê°œì„ ìœ¨   í™œìš©ë¥    ì„±ê³µë¥ ")
            print("-" * 70)
            for rank, (name, result) in enumerate(sorted_results[:10], 1):
                improvement = (result['combined_score'] - self.phase3_best['score']) / self.phase3_best['score'] * 100
                print(f"{rank:2d}    {name:<22} {result['combined_score']:6.2f}   {improvement:+5.1f}%   "
                      f"{result['mean_utilization']:5.1%}   {result['success_rate']:5.1%}")

            best_result = sorted_results[0][1]
            target_achievement = best_score / self.target_score * 100
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_score:.3f}ì  ({best_config})")
            print(f"ğŸ“ˆ ëª©í‘œ ë‹¬ì„±ë„: {target_achievement:.1f}% (ëª©í‘œ {self.target_score} ëŒ€ë¹„)")
            if best_score >= self.target_score:
                print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„± ì„±ê³µ!")
            else:
                remaining = self.target_score - best_score
                print(f"ğŸ“Š ëª©í‘œê¹Œì§€ {remaining:.3f}ì  ë¶€ì¡±")

            # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ numpy -> ê¸°ë³¸íƒ€ì… ë³€í™˜ê¸°
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {key: convert_numpy_types(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj

            converted_results = convert_numpy_types(results)
            output_data = {
                'timestamp': self.timestamp,
                'phase': 'phase4_enhanced_optimization',
                'focus': focus,
                'timesteps': int(timesteps),
                'target_score': float(self.target_score),
                'phase3_baseline': float(self.phase3_best['score']),
                'best_score': float(best_score),
                'best_config': best_config,
                'target_achievement': float(target_achievement),
                'total_time_minutes': float(total_time / 60),
                'results': converted_results
            }

            output_file = os.path.join(self.results_dir, f'phase4_enhanced_{focus}_{self.timestamp}.json')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Phase 4 ê²°ê³¼: {output_file}")
            print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
            return output_data
        else:
            print("âŒ ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

    def create_performance_analysis(self, results_file: str):
        """ê²°ê³¼ JSONì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„± ë° ì €ì¥."""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if not data.get('results'):
                print("ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            configs, scores, utilizations, success_rates = [], [], [], []
            for name, result in data['results'].items():
                configs.append(name)
                scores.append(result['combined_score'])
                utilizations.append(result['mean_utilization'] * 100)
                success_rates.append(result['success_rate'] * 100)

            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Phase 4 Enhanced Optimization Analysis\n'
                         f'Best: {data["best_score"]:.2f} (Target: {data["target_score"]})',
                         fontsize=16, fontweight='bold')

            # 1) ì¢…í•© ì ìˆ˜ ë°”ì°¨íŠ¸
            axes[0,0].bar(range(len(configs)), scores, color='skyblue', alpha=0.7)
            axes[0,0].axhline(y=data['target_score'], color='red', linestyle='--', label=f'Target: {data["target_score"]}')
            axes[0,0].axhline(y=data['phase3_baseline'], color='orange', linestyle='--', label=f'Phase3: {data["phase3_baseline"]:.2f}')
            axes[0,0].set_title('Combined Scores')
            axes[0,0].set_ylabel('Score')
            axes[0,0].legend()
            axes[0,0].tick_params(axis='x', rotation=45)

            # 2) í™œìš©ë¥ 
            axes[0,1].bar(range(len(configs)), utilizations, color='lightgreen', alpha=0.7)
            axes[0,1].set_title('Space Utilization (%)')
            axes[0,1].set_ylabel('Utilization %')
            axes[0,1].tick_params(axis='x', rotation=45)

            # 3) ì„±ê³µë¥ 
            axes[1,0].bar(range(len(configs)), success_rates, color='lightcoral', alpha=0.7)
            axes[1,0].set_title('Success Rate (%)')
            axes[1,0].set_ylabel('Success Rate %')
            axes[1,0].tick_params(axis='x', rotation=45)

            # 4) ìƒê´€ê´€ê³„ ì‚°ì ë„ (í™œìš©ë¥  vs ì ìˆ˜)
            axes[1,1].scatter(utilizations, scores, alpha=0.7, s=100)
            axes[1,1].set_xlabel('Utilization %')
            axes[1,1].set_ylabel('Combined Score')
            axes[1,1].set_title('Utilization vs Score')

            for ax in axes.flat:
                if hasattr(ax, 'set_xticks'):
                    ax.set_xticks(range(len(configs)))
                    ax.set_xticklabels([c[:15] + '...' if len(c) > 15 else c for c in configs], rotation=45, ha='right')

            plt.tight_layout()
            chart_file = results_file.replace('.json', '_analysis.png')
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ë¶„ì„ ì°¨íŠ¸ ì €ì¥: {chart_file}")
            plt.close()
        except Exception as e:
            print(f"âŒ ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")


class EnhancedRewardWrapper(gym.RewardWrapper):
    """ê°•í™”ëœ ë³´ìƒ ë˜í¼(ê°„ë‹¨í˜•).

    - ì»¨í…Œì´ë„ˆ í™œìš©ë¥ , ì—°ì† ë°°ì¹˜, ì„ê³„ê°’ ëŒíŒŒ ë“±ì˜ ë³´ë„ˆìŠ¤ë¥¼ ì¶”ê°€í•´ í•™ìŠµì„ ê°€ì†
    - ë…¼ë¬¸ì  ê´€ì ì—ì„œì˜ Reward Shaping: íƒìƒ‰ì„ ìœ ë„í•˜ë˜ ê³¼ë„í•œ í¸í–¥ ë°©ì§€
    """

    def __init__(self, env):
        super().__init__(env)
        self.previous_utilization = 0.0
        self.consecutive_placements = 0

    def reset(self, **kwargs):
        self.previous_utilization = 0.0
        self.consecutive_placements = 0
        return self.env.reset(**kwargs)

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        enhanced_reward = self.reward(reward)
        return obs, enhanced_reward, terminated, truncated, info

    def reward(self, reward):
        current_utilization, placement_count = calculate_utilization_and_items(self.env)

        enhanced_reward = reward
        if current_utilization > 0:
            enhanced_reward += (current_utilization ** 1.5) * 3.0
        if current_utilization > self.previous_utilization:
            enhanced_reward += (current_utilization - self.previous_utilization) * 5.0
        if placement_count > 0:
            self.consecutive_placements += 1
            enhanced_reward += min(self.consecutive_placements * 0.1, 1.0)
        else:
            self.consecutive_placements = 0
        if current_utilization > 0.25:
            enhanced_reward += 2.0
        elif current_utilization > 0.20:
            enhanced_reward += 1.0
        if placement_count >= 5:
            enhanced_reward += (placement_count - 4) * 0.5
        if placement_count > 0:
            efficiency = current_utilization / placement_count
            enhanced_reward += efficiency * 2.0

        self.previous_utilization = current_utilization
        return enhanced_reward


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸: ì‹¤í—˜ ì‹¤í–‰ ë˜ëŠ” ë¶„ì„ ëª¨ë“œ."""
    import argparse

    parser = argparse.ArgumentParser(description='Enhanced Optimization for 3D Bin Packing')
    parser.add_argument('--focus', choices=['all', 'stability', 'architecture', 'optimization'],
                        default='all', help='Optimization focus area')
    parser.add_argument('--timesteps', type=int, default=35000, help='Training timesteps')
    parser.add_argument('--analyze', type=str, help='Analyze results from JSON file')

    args = parser.parse_args()
    optimizer = EnhancedOptimizer()

    if args.analyze:
        optimizer.create_performance_analysis(args.analyze)
    else:
        result = optimizer.run_phase4_optimization(focus=args.focus, timesteps=args.timesteps)
        if result and result.get('results'):
            output_file = os.path.join(optimizer.results_dir,
                                       f'phase4_enhanced_{args.focus}_{optimizer.timestamp}.json')
            optimizer.create_performance_analysis(output_file)

            best_score = result['best_score']
            target_score = result['target_score']
            if best_score >= target_score:
                print(f"\nğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ëª©í‘œ {target_score}ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
                print(f"ğŸ† ìµœì¢… ì„±ëŠ¥: {best_score:.3f}ì ")
            else:
                remaining = target_score - best_score
                print(f"\nğŸ“Š ì¶”ê°€ ê°œì„  ê¶Œì¥ì‚¬í•­:")
                print(f"   ëª©í‘œê¹Œì§€ {remaining:.3f}ì  ë¶€ì¡±")
                if remaining > 1.0:
                    print(f"   â¡ï¸ í•™ìŠµ ì‹œê°„ì„ 50,000ìŠ¤í…ìœ¼ë¡œ ì¦ê°€ ì‹œë„")
                    print(f"   â¡ï¸ ì•™ìƒë¸” ëª¨ë¸ë§ ì‹œë„")
                else:
                    print(f"   â¡ï¸ ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ ë‹¬ì„± ê°€ëŠ¥")
                    print(f"   â¡ï¸ ë³´ìƒ í•¨ìˆ˜ ì¶”ê°€ ìµœì í™” ê¶Œì¥")


if __name__ == "__main__":
    main()


```

```python
#!/usr/bin/env python3
"""
í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸(ì£¼ì„ í™•ì¥íŒ)

ê°œìš”
- Phase 4 íƒìƒ‰ ê²°ê³¼ë¡œ ì–»ì€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°(`PRODUCTION_OPTIMAL`)ë¡œ í•™ìŠµ/í‰ê°€ë¥¼ ìˆ˜í–‰í•´
  ì¬í˜„ì„±Â·ì•ˆì •ì„±ì„ ì ê²€í•©ë‹ˆë‹¤.
- í™˜ê²½ì€ `src/train_maskable_ppo.make_env`ë¥¼ í†µí•´ ìƒì„±ë˜ë©°, ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ê³¼ ê°œì„ í˜• ë³´ìƒ
  ì‰ì´í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ë…¼ë¬¸ ë§¥ë½: Transformer ê¸°ë°˜ DRLê³¼ ë‹¬ë¦¬ ë³¸ ì½”ë“œëŠ” MLP+MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  ìƒíƒœí‘œí˜„(ë†’ì´ë§µ+ê°€ì‹œë°•ìŠ¤), ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹, ë³´ìƒ ì„¤ê³„ë¥¼ í†µí•´ íš¨ìœ¨ì  íƒìƒ‰ì´ë¼ëŠ” ê³µí†µ ëª©í‘œë¥¼ ì§€í–¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•(ì˜ˆ)
- ì™„ì „ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --timesteps 50000 --episodes 50
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --quick

ì¶œë ¥
- ëª¨ë¸:  models/production_optimal_{timestamp}
- ê²°ê³¼:  results/production_final_{timestamp}.json (ì¢…í•© ì ìˆ˜, í™œìš©ë¥ /ì„±ê³µë¥  ë“±)
"""

import os
import sys
import json
import time
import numpy as np
from datetime import datetime
import warnings

# ì„œë²„/í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ ì•ˆì „ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

# Phase 4ì—ì„œ í™•ì •ëœ í”„ë¡œë•ì…˜ ìµœì  êµ¬ì„±
PRODUCTION_OPTIMAL = {
    "learning_rate": 0.00013,
    "n_steps": 768,
    "batch_size": 96,
    "n_epochs": 5,
    "clip_range": 0.18,
    "ent_coef": 0.008,
    "vf_coef": 0.5,
    "gae_lambda": 0.96,
    "net_arch": {"pi": [256, 128, 64], "vf": [256, 128, 64]}
}


def create_production_env(container_size=[10, 10, 10], num_boxes=12, seed=42):
    """í”„ë¡œë•ì…˜ í™˜ê²½ íŒ©í† ë¦¬.

    - `train_maskable_ppo.make_env`ë¥¼ í†µí•´ Gym í™˜ê²½(`PackingEnv-v0`) ìƒì„±
    - ê°œì„ í˜• ë³´ìƒ(`improved_reward_shaping=True`)ê³¼ ActionMasker ì ìš©
    """
    try:
        from train_maskable_ppo import make_env
        env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=seed,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        print(f"âœ… í”„ë¡œë•ì…˜ í™˜ê²½ ìƒì„±: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{num_boxes}ê°œ")
        return env
    except Exception as e:
        print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def train_production_model(env, timesteps=50000):
    """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ MaskablePPO í•™ìŠµ.

    ë°˜í™˜ê°’
    - (model, duration_seconds)
    """
    try:
        import torch
        import torch.nn as nn
        from sb3_contrib import MaskablePPO

        print(f"ğŸš€ í”„ë¡œë•ì…˜ í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í…")
        print(f"ğŸ“Š ìµœì  ì„¤ì •: LR={PRODUCTION_OPTIMAL['learning_rate']:.2e}, "
              f"Steps={PRODUCTION_OPTIMAL['n_steps']}, "
              f"Batch={PRODUCTION_OPTIMAL['batch_size']}")

        start_time = time.time()
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=PRODUCTION_OPTIMAL['learning_rate'],
            n_steps=PRODUCTION_OPTIMAL['n_steps'],
            batch_size=PRODUCTION_OPTIMAL['batch_size'],
            n_epochs=PRODUCTION_OPTIMAL['n_epochs'],
            gamma=0.99,
            gae_lambda=PRODUCTION_OPTIMAL['gae_lambda'],
            clip_range=PRODUCTION_OPTIMAL['clip_range'],
            ent_coef=PRODUCTION_OPTIMAL['ent_coef'],
            vf_coef=PRODUCTION_OPTIMAL['vf_coef'],
            max_grad_norm=0.5,
            verbose=1,
            seed=42,
            policy_kwargs=dict(
                net_arch=PRODUCTION_OPTIMAL['net_arch'],
                activation_fn=nn.ReLU,
                share_features_extractor=True,
            )
        )
        model.learn(total_timesteps=timesteps, progress_bar=True)
        duration = time.time() - start_time
        print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ: {duration/60:.1f}ë¶„")
        return model, duration
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
        return None, 0


def evaluate_production_model(model, container_size=[10, 10, 10], num_boxes=12, n_episodes=50):
    """ê°•í™”ëœ í”„ë¡œë•ì…˜ í‰ê°€ ë£¨í‹´.

    - ë‹¤ì–‘í•œ ì‹œë“œë¡œ ë‹¤ìˆ˜ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë³´ìƒÂ·í™œìš©ë¥ Â·ì„±ê³µë¥ ì„ ì¸¡ì •
    - ì„±ê³µ ê¸°ì¤€: í™œìš©ë¥  25% ì´ìƒ ë˜ëŠ” ë°•ìŠ¤ 50% ì´ìƒ ë°°ì¹˜
    """
    print(f"ğŸ” í”„ë¡œë•ì…˜ í‰ê°€ ì‹œì‘: {n_episodes} ì—í”¼ì†Œë“œ")

    all_rewards, all_utilizations, placement_counts = [], [], []
    success_count = 0

    for ep in range(n_episodes):
        seed = 200 + ep * 3
        eval_env = create_production_env(container_size, num_boxes, seed)
        if eval_env is None:
            continue

        obs, _ = eval_env.reset(seed=seed)
        episode_reward = 0.0

        for step in range(50):  # ìµœëŒ€ 50ìŠ¤í…
            try:
                action, _ = model.predict(obs, deterministic=False)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                if terminated or truncated:
                    break
            except Exception as e:
                break

        # ì„±ê³¼ ê³„ì‚°(ë³´ìƒê³¼ í™œìš©ë¥ ì€ í™˜ê²½ì— ë”°ë¼ ìƒì´í•  ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì¬ê³„ì‚°)
        utilization = 0.0
        placed_boxes = 0
        try:
            if hasattr(eval_env.unwrapped, 'container'):
                placed_volume = sum(
                    box.volume for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
                container_volume = eval_env.unwrapped.container.volume
                utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                placed_boxes = sum(
                    1 for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
        except:
            pass

        if utilization >= 0.25 or placed_boxes >= num_boxes * 0.5:
            success_count += 1

        all_rewards.append(episode_reward)
        all_utilizations.append(utilization)
        placement_counts.append(placed_boxes)

        if ep < 10 or ep % 10 == 0:
            print(f"   ì—í”¼ì†Œë“œ {ep+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={utilization:.1%}, ë°•ìŠ¤={placed_boxes}ê°œ")

        eval_env.close()

    if not all_rewards:
        return None

    results = {
        'mean_reward': np.mean(all_rewards),
        'std_reward': np.std(all_rewards),
        'mean_utilization': np.mean(all_utilizations),
        'std_utilization': np.std(all_utilizations),
        'mean_placement': np.mean(placement_counts),
        'max_placement': max(placement_counts),
        'success_rate': success_count / len(all_rewards),
        'combined_score': np.mean(all_rewards) * 0.3 + np.mean(all_utilizations) * 100 * 0.7,
        'episodes': len(all_rewards),
        'all_rewards': all_rewards,
        'all_utilizations': all_utilizations
    }
    return results


def production_final_test(timesteps=50000, eval_episodes=50):
    """ì—”ë“œíˆ¬ì—”ë“œ í”„ë¡œë•ì…˜ ê²€ì¦: í•™ìŠµâ†’ì €ì¥â†’í‰ê°€â†’ìš”ì•½ ì €ì¥."""
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ì‹œì‘")
    print(f"ğŸ“Š ëª©í‘œ: 20.591ì  ì¬í˜„ ë° ì•ˆì •ì„± ê²€ì¦")
    print("="*60)

    container_size = [10, 10, 10]
    num_boxes = 12

    env = create_production_env(container_size, num_boxes, 42)
    if env is None:
        return False

    print(f"\nğŸ“ 1ë‹¨ê³„: í”„ë¡œë•ì…˜ ëª¨ë¸ í•™ìŠµ ({timesteps:,} ìŠ¤í…)")
    model, train_time = train_production_model(env, timesteps)
    if model is None:
        env.close()
        return False

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"models/production_optimal_{timestamp}"
    os.makedirs('models', exist_ok=True)
    model.save(model_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")

    print(f"\nğŸ“Š 2ë‹¨ê³„: ê°•í™”ëœ í‰ê°€ ({eval_episodes} ì—í”¼ì†Œë“œ)")
    results = evaluate_production_model(model, container_size, num_boxes, eval_episodes)
    env.close()
    if results is None:
        return False

    print("\n" + "="*60)
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {results['combined_score']:.3f}")
    print(f"ğŸ¯ ëª©í‘œ ëŒ€ë¹„: {(results['combined_score']/20.591*100):.1f}% (ëª©í‘œ: 20.591)")
    print(f"ğŸ’° í‰ê·  ë³´ìƒ: {results['mean_reward']:.3f} Â± {results['std_reward']:.3f}")
    print(f"ğŸ“¦ í‰ê·  í™œìš©ë¥ : {results['mean_utilization']:.1%} Â± {results['std_utilization']:.1%}")
    print(f"ğŸ² í‰ê·  ë°°ì¹˜: {results['mean_placement']:.1f}ê°œ (ìµœëŒ€: {results['max_placement']}ê°œ)")
    print(f"âœ… ì„±ê³µë¥ : {results['success_rate']:.1%}")
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time/60:.1f}ë¶„")

    if results['combined_score'] >= 20.0:
        print(f"ğŸ‰ ìš°ìˆ˜! ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± ë˜ëŠ” ê·¼ì ‘")
    elif results['combined_score'] >= 18.57:
        print(f"âœ… ì„±ê³µ! Phase 3 ëª©í‘œ ë‹¬ì„±")
    else:
        print(f"ğŸ“ˆ ê°œì„  í•„ìš”: ì¶”ê°€ íŠœë‹ ê¶Œì¥")

    final_results = {
        'timestamp': timestamp,
        'test_type': 'production_final',
        'params': PRODUCTION_OPTIMAL,
        'config': {
            'container_size': container_size,
            'num_boxes': num_boxes,
            'timesteps': timesteps,
            'eval_episodes': eval_episodes
        },
        'performance': results,
        'training_time_minutes': train_time/60,
        'model_path': model_path,
        'target_score': 20.591,
        'achievement_rate': results['combined_score']/20.591*100
    }
    os.makedirs('results', exist_ok=True)
    results_file = f"results/production_final_{timestamp}.json"
    with open(results_file, 'w') as f:
        json.dump(final_results, f, indent=2, default=str)
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")

    return results['combined_score'] >= 18.57


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸: ì™„ì „ í…ŒìŠ¤íŠ¸/ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›."""
    import argparse

    parser = argparse.ArgumentParser(description='í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… í…ŒìŠ¤íŠ¸')
    parser.add_argument('--timesteps', type=int, default=50000, help='í•™ìŠµ ìŠ¤í… ìˆ˜')
    parser.add_argument('--episodes', type=int, default=50, help='í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (25000 ìŠ¤í…)')
    args = parser.parse_args()

    if args.quick:
        timesteps = 25000
        episodes = 30
        print("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    else:
        timesteps = args.timesteps
        episodes = args.episodes
        print("ğŸ† ì™„ì „ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")

    print(f"ğŸš€ ì„¤ì •: {timesteps:,} ìŠ¤í…, {episodes} ì—í”¼ì†Œë“œ")

    start_time = time.time()
    success = production_final_test(timesteps, episodes)
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    print("ğŸ‰ í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ê³µ!" if success else "ğŸ“ˆ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()


```

ë°©ê¸ˆ ì „ì²´ êµ¬ì¡° ìš”ì•½ê³¼ ì£¼ì„ í™•ì¥ íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ì œ ìƒˆë¡œ ë§Œë“  `enhanced_optimization_annotated.py`, `production_final_test_annotated.py`ì— ë¦°íŠ¸ ì˜¤ë¥˜ê°€ ì—†ëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.

```python
#!/usr/bin/env python3
"""
3D Bin Packing ê°•í™”í•™ìŠµ ìµœì í™”(Phase 4) ì£¼ì„ í™•ì¥íŒ

ê°œìš”
- ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” Maskable PPOë¥¼ í™œìš©í•´ 3D ë¹ˆíŒ¨í‚¹ í™˜ê²½(`src/packing_env.py`)ì—ì„œ
  ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° êµ¬ì„±ì˜ ì„±ëŠ¥ì„ ë¹„êµÂ·í‰ê°€í•©ë‹ˆë‹¤.
- ê´€ì°°(Observation)ì€ ì»¨í…Œì´ë„ˆ ìƒë‹¨ ë†’ì´ë§µê³¼ ê°€ì‹œ ë°•ìŠ¤ì˜ í¬ê¸° ë²¡í„°ì´ë©°,
  í–‰ë™(Action)ì€ [ê°€ì‹œ ë°•ìŠ¤ ì„ íƒ Ã— XY ì¢Œí‘œ]ì˜ ë‹¨ì¼ ì •ìˆ˜ indexë¡œ ë””ìŠ¤í¬ë¦¬íŠ¸í™”ë©ë‹ˆë‹¤.
- ë…¼ë¬¸ ë§¥ë½: ì²¨ë¶€ ë¬¸í—Œ(Transformer ê¸°ë°˜ DRL)ê³¼ ë‹¬ë¦¬ ë³¸ ì½”ë“œëŠ” MLP ê¸°ë°˜ MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  ê³µí†µì ìœ¼ë¡œ ë†’ì´ë§µ ê¸°ë°˜ ìƒíƒœí‘œí˜„, ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹, ë³´ìƒ ì‰ì´í•‘ì„ í†µí•´ íƒìƒ‰ ê³µê°„ì„ ì¤„ì´ê³  ì•ˆì •ì  í•™ìŠµì„ ì§€í–¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•(ì˜ˆ)
- ëª¨ë“  ì„¸íŠ¸ ì‹¤í—˜:  python enhanced_optimization_annotated.py --focus all --timesteps 35000
- ì•ˆì •ì„± ì¤‘ì‹¬:     python enhanced_optimization_annotated.py --focus stability
- ê²°ê³¼ ë¶„ì„ë§Œ:      python enhanced_optimization_annotated.py --analyze results/phase4_enhanced_*.json

ì¶œë ¥
- `results/phase4_enhanced_{focus}_{timestamp}.json` ì¢…í•© ê²°ê³¼ì™€ `_analysis.png` ì°¨íŠ¸ë¥¼ ìë™ ì €ì¥
"""

import os
import sys
import json
import time
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
import warnings

# í™˜ê²½ ì„¤ì •: ì„œë²„/í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")

# src í´ë”ë¥¼ import ê²½ë¡œì— ì¶”ê°€
sys.path.append('src')

import gymnasium as gym
from gymnasium.envs.registration import register
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
import matplotlib.pyplot as plt

# í”„ë¡œì íŠ¸ í•µì‹¬ ëª¨ë“ˆ ë¡œë“œ
try:
    from packing_env import PackingEnv  # ìƒíƒœÂ·í–‰ë™ ì •ì˜ëœ Gym í™˜ê²½
    from train_maskable_ppo import ImprovedRewardWrapper  # ê°œì„ í˜• ë³´ìƒ ë˜í¼
    from utils import boxes_generator  # ë¬¸ì œ ì¸ìŠ¤í„´ìŠ¤(ë°•ìŠ¤ ì‹œí€€ìŠ¤) ìƒì„±ê¸°
    print("âœ… ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ Import ì˜¤ë¥˜: {e}")
    print("src í´ë”ì™€ í•„ìš”í•œ ëª¨ë“ˆë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)


def get_env_info(env):
    """ë˜í¼ë¥¼ ë²—ê²¨ ì‹¤ì œ `PackingEnv`ì— ì ‘ê·¼í•´ ì»¨í…Œì´ë„ˆ í¬ê¸°/ë°•ìŠ¤ ê°œìˆ˜ ìš”ì•½.

    ë°˜í™˜ê°’
    - (container_size, box_count)
    """
    try:
        unwrapped_env = env
        while hasattr(unwrapped_env, 'env'):
            unwrapped_env = unwrapped_env.env

        if hasattr(unwrapped_env, 'container') and hasattr(unwrapped_env.container, 'size'):
            container_size = unwrapped_env.container.size
        else:
            container_size = [10, 10, 10]

        if hasattr(unwrapped_env, 'initial_boxes'):
            box_count = len(unwrapped_env.initial_boxes)
        elif hasattr(unwrapped_env, 'num_initial_boxes'):
            box_count = unwrapped_env.num_initial_boxes
        else:
            box_count = 12

        return container_size, box_count
    except Exception as e:
        print(f"âš ï¸ í™˜ê²½ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return [10, 10, 10], 12


def calculate_utilization_and_items(env):
    """ìµœì¢… ì‹œì  ì»¨í…Œì´ë„ˆ í™œìš©ë¥ ê³¼ ë°°ì¹˜ëœ ë°•ìŠ¤ ê°œìˆ˜ ì‚°ì¶œ.

    - í™œìš©ë¥  = (ë°°ì¹˜ëœ ë°•ìŠ¤ ì´ë¶€í”¼) / (ì»¨í…Œì´ë„ˆ ë¶€í”¼)
    - ë°°ì¹˜ íŒì •: positionì´ [-1,-1,-1]ì´ ì•„ë‹Œ ë°•ìŠ¤
    """
    try:
        unwrapped_env = env
        while hasattr(unwrapped_env, 'env'):
            unwrapped_env = unwrapped_env.env

        if hasattr(unwrapped_env, 'container'):
            placed_volume = 0
            placed_count = 0
            for box in unwrapped_env.container.boxes:
                if hasattr(box, 'position') and box.position is not None:
                    if not (box.position[0] == -1 and box.position[1] == -1 and box.position[2] == -1):
                        placed_volume += box.volume
                        placed_count += 1

            container_volume = unwrapped_env.container.volume
            utilization = placed_volume / container_volume if container_volume > 0 else 0.0
            return utilization, placed_count
        else:
            return 0.0, 0
    except Exception as e:
        print(f"âš ï¸ í™œìš©ë¥  ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0, 0


class EnhancedOptimizer:
    """Phase 4 ì •ë°€ ìµœì í™” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.

    - ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ìˆœíšŒ í•™ìŠµ/í‰ê°€í•˜ì—¬ ì¢…í•© ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
    - ì ìˆ˜ = 0.3Ã—í‰ê·  ë³´ìƒ + 0.7Ã—(í‰ê·  í™œìš©ë¥ [%])
    """

    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)

        # Phase 3 ìµœê³  ì„±ëŠ¥ ê¸°ì¤€ì  (ë¹„êµÂ·ê°œì„ ìœ¨ ê³„ì‚°ì— ì‚¬ìš©)
        self.phase3_best = {
            'score': 16.116,
            'params': {
                'learning_rate': 0.00015,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [256, 128, 64]
            }
        }

        self.target_score = 18.57
        self.improvement_needed = (self.target_score - self.phase3_best['score']) / self.phase3_best['score']

        print(f"ğŸ¯ Phase 4 Enhanced Optimization ì‹œì‘")
        print(f"ğŸ“Š ê¸°ì¤€ì : {self.phase3_best['score']:.3f}ì ")
        print(f"ğŸ† ëª©í‘œ: {self.target_score}ì  ({self.improvement_needed:.1%} ê°œì„  í•„ìš”)")

    def create_enhanced_environment(self, num_boxes: int = 12, container_size: Optional[List[int]] = None,
                                    enhanced_reward: bool = True, seed: int = 42) -> gym.Env:
        """ì‹¤í—˜ìš© í™˜ê²½ ìƒì„±.

        - `utils.boxes_generator`ë¡œ ê³ ì • ì»¨í…Œì´ë„ˆì— ìƒì‘í•˜ëŠ” ë°•ìŠ¤ ì‹œí€€ìŠ¤ ìƒì„±
        - `ImprovedRewardWrapper` ë˜ëŠ” ë³¸ íŒŒì¼ì˜ `EnhancedRewardWrapper` ì ìš©
        - `ActionMasker`ë¡œ ë¶ˆê°€ëŠ¥ í–‰ë™ì„ ë§ˆìŠ¤í‚¹í•˜ì—¬ ìƒ˜í”Œ íš¨ìœ¨ ê°œì„ 
        """
        try:
            if container_size is None:
                container_size = [10, 10, 10]
            print(f"ìƒì„±ëœ ë°•ìŠ¤ ê°œìˆ˜: {num_boxes}")
            print(f"ì»¨í…Œì´ë„ˆ í¬ê¸°: {container_size}")

            # Gym í™˜ê²½ ë“±ë¡ (ì¤‘ë³µ ë“±ë¡ ë°©ì§€)
            if 'PackingEnv-v0' not in gym.envs.registry:
                register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')

            # ë°•ìŠ¤ ì‹œí€€ìŠ¤ ìƒì„±
            box_sizes = boxes_generator(container_size, num_boxes, seed)

            # í™˜ê²½ ìƒì„±
            env = gym.make(
                "PackingEnv-v0",
                container_size=container_size,
                box_sizes=box_sizes,
                num_visible_boxes=min(3, num_boxes),
                render_mode=None,
                random_boxes=False,
                only_terminal_reward=False,
            )
            print("í™˜ê²½ ìƒì„± ì„±ê³µ: PackingEnv-v0")

            # ë³´ìƒ ë˜í¼ ì„ íƒ ì ìš©
            if enhanced_reward:
                env = EnhancedRewardWrapper(env)
                print("ê°•í™”ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")
            else:
                env = ImprovedRewardWrapper(env)
                print("ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")

            # ì•¡ì…˜ ë§ˆìŠ¤í‚¹ (ë¶ˆê°€ëŠ¥ ì¢Œí‘œ/ë°•ìŠ¤ ì„ íƒ ì œê±°)
            def get_action_masks(env):
                try:
                    unwrapped_env = env
                    while hasattr(unwrapped_env, 'env'):
                        unwrapped_env = unwrapped_env.env

                    if hasattr(unwrapped_env, 'action_masks'):
                        masks = unwrapped_env.action_masks()
                        if isinstance(masks, list):
                            return np.array(masks, dtype=bool)
                        return masks
                    return np.ones(env.action_space.n, dtype=bool)
                except Exception as e:
                    print(f"âš ï¸ ì•¡ì…˜ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
                    return np.ones(env.action_space.n, dtype=bool)

            env = ActionMasker(env, get_action_masks)
            print("ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ë˜í¼ ì ìš©ë¨")

            print(f"ì‹œë“œ ì„¤ì • ì™„ë£Œ: {seed}")
            return env

        except Exception as e:
            print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
            raise e

    def get_enhanced_parameter_sets(self) -> Dict[str, Dict]:
        """íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸íŠ¸ ì‚¬ì „.

        - ì•ˆì •ì„± ê°•í™”, ì•„í‚¤í…ì²˜ ë³€í™”, ì ê·¹/ë³´ìˆ˜ ìµœì í™” ë“± ì¹´í…Œê³ ë¦¬ë¡œ êµ¬ì„±
        - ê° í•­ëª©ì€ MaskablePPOì˜ ì£¼ìš” ì¸ìì™€ `policy_kwargs.net_arch`ë¥¼ í¬í•¨
        """
        # 1) í•™ìŠµ ì•ˆì •ì„± ê°•í™” ì„¸íŠ¸
        stability_sets = {
            'stability_conservative': {
                'learning_rate': 1.2e-04,
                'n_steps': 1024,
                'batch_size': 64,
                'n_epochs': 6,
                'clip_range': 0.15,
                'ent_coef': 0.005,
                'vf_coef': 0.5,
                'gae_lambda': 0.98,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            },
            'stability_balanced': {
                'learning_rate': 1.3e-04,
                'n_steps': 768,
                'batch_size': 96,
                'n_epochs': 5,
                'clip_range': 0.18,
                'ent_coef': 0.008,
                'vf_coef': 0.5,
                'gae_lambda': 0.96,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            }
        }

        # 2) ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ ë³€í™” ì„¸íŠ¸ (í­/ê¹Šì´/ê· í˜•)
        architecture_sets = {
            'arch_wide': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[512, 256, 128], vf=[512, 256, 128])]
            },
            'arch_deep': {
                'learning_rate': 1.4e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[256, 256, 128, 64], vf=[256, 256, 128, 64])]
            },
            'arch_balanced': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[384, 192, 96], vf=[384, 192, 96])]
            },
            'arch_reinforced': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[256, 128, 128, 64], vf=[256, 128, 128, 64])]
            }
        }

        # 3) í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¹ë‹¨/ì •ë°€ ì„¸íŠ¸
        optimized_sets = {
            'opt_precision': {
                'learning_rate': 1.1e-04,
                'n_steps': 1536,
                'batch_size': 192,
                'n_epochs': 8,
                'clip_range': 0.12,
                'ent_coef': 0.003,
                'vf_coef': 0.6,
                'gae_lambda': 0.99,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            },
            'opt_aggressive': {
                'learning_rate': 1.8e-04,
                'n_steps': 256,
                'batch_size': 64,
                'n_epochs': 3,
                'clip_range': 0.25,
                'ent_coef': 0.02,
                'vf_coef': 0.4,
                'gae_lambda': 0.92,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            }
        }

        all_sets = {}
        all_sets.update(stability_sets)
        all_sets.update(architecture_sets)
        all_sets.update(optimized_sets)
        return all_sets

    def train_and_evaluate(self, params: Dict, name: str, timesteps: int = 35000,
                           eval_episodes: int = 25, enhanced_reward: bool = True) -> Dict[str, Any]:
        """ë‹¨ì¼ êµ¬ì„± í•™ìŠµ ë° í‰ê°€ ë£¨í‹´.

        - MaskablePPOë¡œ timesteps ë§Œí¼ í•™ìŠµ í›„, ì—¬ëŸ¬ ì‹œë“œë¡œ ì§§ì€ ì—í”¼ì†Œë“œ í‰ê°€
        - ë…¼ë¬¸ ê´€ì : ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ íƒìƒ‰ ê³µê°„ì„ ì¶•ì†Œí•´ ìƒ˜í”Œ íš¨ìœ¨ê³¼ ì•ˆì •ì„±ì„ í–¥ìƒ
        """
        print(f"\nğŸ”§ {name} ìµœì í™” ì¤‘...")

        env = self.create_enhanced_environment(enhanced_reward=enhanced_reward, seed=42)
        container_size, box_count = get_env_info(env)
        print(f"âœ… í™˜ê²½ ìƒì„± ì„±ê³µ: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{box_count}ê°œ")

        model = MaskablePPO(
            'MultiInputPolicy',
            env,
            learning_rate=params['learning_rate'],
            n_steps=params['n_steps'],
            batch_size=params['batch_size'],
            n_epochs=params['n_epochs'],
            clip_range=params['clip_range'],
            ent_coef=params['ent_coef'],
            vf_coef=params['vf_coef'],
            gae_lambda=params['gae_lambda'],
            policy_kwargs={'net_arch': params['net_arch']},
            verbose=0,
            device='auto'
        )

        print(f"ğŸ“ {name} í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í… (LR: {params['learning_rate']:.2e}, Net: {params['net_arch']})")
        start_time = time.time()
        model.learn(total_timesteps=timesteps)
        training_time = time.time() - start_time
        print(f"â±ï¸ {name} í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")

        # í‰ê°€: ë™ì¼ í™˜ê²½ ì„¤ì •ì—ì„œ ì‹œë“œë§Œ ë‹¤ë¥´ê²Œ ë°˜ë³µ
        print(f"ğŸ” {name} í‰ê°€ ì‹œì‘ ({eval_episodes} ì—í”¼ì†Œë“œ, ìµœëŒ€ 25 ìŠ¤í…)")
        rewards, utilizations, placements = [], [], []
        for i in range(eval_episodes):
            eval_env = self.create_enhanced_environment(enhanced_reward=enhanced_reward, seed=100 + i * 5)
            container_size, box_count = get_env_info(eval_env)
            obs = eval_env.reset(seed=100 + i * 5)
            if isinstance(obs, tuple):
                obs = obs[0]

            episode_reward = 0
            step_count = 0
            max_steps = 25
            while step_count < max_steps:
                try:
                    # í‰ê°€ ì‹œì—ë„ í™•ë¥ ì  í–‰ë™ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´
                    action, _ = model.predict(obs, deterministic=False)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    step_count += 1
                    if terminated or truncated:
                        break
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    break

            final_utilization, placement_count = calculate_utilization_and_items(eval_env)
            rewards.append(episode_reward)
            utilizations.append(final_utilization)
            placements.append(placement_count)

            if i < 6 or i in [10, 15, 20] or i == eval_episodes - 1:
                print(f"   ì—í”¼ì†Œë“œ {i+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={final_utilization:.1%}, ë°•ìŠ¤={placement_count}ê°œ")
            eval_env.close()

        mean_reward = np.mean(rewards)
        std_reward = np.std(rewards)
        mean_utilization = np.mean(utilizations)
        std_utilization = np.std(utilizations)
        mean_placement = np.mean(placements)
        max_placement = np.max(placements)
        success_count = sum(1 for p in placements if p >= 5)
        success_rate = success_count / eval_episodes
        combined_score = mean_reward * 0.3 + mean_utilization * 100 * 0.7

        print(f"ğŸ“Š {name} ìµœì¢… ê²°ê³¼:")
        print(f"   í‰ê·  ë³´ìƒ: {mean_reward:.3f} Â± {std_reward:.3f}")
        print(f"   í‰ê·  í™œìš©ë¥ : {mean_utilization:.1%} Â± {std_utilization:.1%}")
        print(f"   í‰ê·  ë°°ì¹˜: {mean_placement:.1f}ê°œ (ìµœëŒ€: {max_placement}ê°œ)")
        print(f"   ì„±ê³µë¥ : {success_rate:.1%}")
        print(f"   ì¢…í•© ì ìˆ˜: {combined_score:.3f}")

        env.close()
        return {
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'mean_utilization': mean_utilization,
            'std_utilization': std_utilization,
            'mean_placement': mean_placement,
            'max_placement': max_placement,
            'success_rate': success_rate,
            'combined_score': combined_score,
            'episodes': eval_episodes,
            'training_time': training_time,
            'params': params
        }

    def run_phase4_optimization(self, focus: str = 'all', timesteps: int = 35000) -> Dict:
        """Phase 4 ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ ë° ê²°ê³¼ ì§‘ê³„/ì €ì¥."""
        print(f"\n{'='*60}")
        print(f"ğŸš€ Phase 4 Enhanced Optimization ì‹œì‘")
        print(f"ğŸ¯ í¬ì»¤ìŠ¤: {focus}")
        print(f"â±ï¸ í•™ìŠµ ìŠ¤í…: {timesteps:,}")
        print(f"{'='*60}")

        all_params = self.get_enhanced_parameter_sets()
        results: Dict[str, Any] = {}
        best_score = 0
        best_config = None

        # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
        if focus == 'stability':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('stability')}
        elif focus == 'architecture':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('arch')}
        elif focus == 'optimization':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('opt')}
        else:
            params_to_test = all_params

        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸í•  ì„¤ì •: {len(params_to_test)}ê°œ")

        total_start_time = time.time()
        for i, (name, params) in enumerate(params_to_test.items(), 1):
            print(f"\n[{i}/{len(params_to_test)}] {name} í…ŒìŠ¤íŠ¸ ì¤‘...")
            try:
                result = self.train_and_evaluate(
                    params, name, timesteps=timesteps, enhanced_reward=True
                )
                results[name] = result
                if result['combined_score'] > best_score:
                    best_score = result['combined_score']
                    best_config = name
                    print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥: {best_score:.3f}ì ")
            except Exception as e:
                print(f"âŒ {name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                continue

        total_time = time.time() - total_start_time

        if results:
            print(f"\n{'='*60}")
            print(f"ğŸ† Phase 4 ìµœì í™” ê²°ê³¼")
            print(f"{'='*60}")

            sorted_results = sorted(results.items(), key=lambda x: x[1]['combined_score'], reverse=True)
            print("ìˆœìœ„  ì„¤ì •ëª…                    ì ìˆ˜      ê°œì„ ìœ¨   í™œìš©ë¥    ì„±ê³µë¥ ")
            print("-" * 70)
            for rank, (name, result) in enumerate(sorted_results[:10], 1):
                improvement = (result['combined_score'] - self.phase3_best['score']) / self.phase3_best['score'] * 100
                print(f"{rank:2d}    {name:<22} {result['combined_score']:6.2f}   {improvement:+5.1f}%   "
                      f"{result['mean_utilization']:5.1%}   {result['success_rate']:5.1%}")

            best_result = sorted_results[0][1]
            target_achievement = best_score / self.target_score * 100
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_score:.3f}ì  ({best_config})")
            print(f"ğŸ“ˆ ëª©í‘œ ë‹¬ì„±ë„: {target_achievement:.1f}% (ëª©í‘œ {self.target_score} ëŒ€ë¹„)")
            if best_score >= self.target_score:
                print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„± ì„±ê³µ!")
            else:
                remaining = self.target_score - best_score
                print(f"ğŸ“Š ëª©í‘œê¹Œì§€ {remaining:.3f}ì  ë¶€ì¡±")

            # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ numpy -> ê¸°ë³¸íƒ€ì… ë³€í™˜ê¸°
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {key: convert_numpy_types(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj

            converted_results = convert_numpy_types(results)
            output_data = {
                'timestamp': self.timestamp,
                'phase': 'phase4_enhanced_optimization',
                'focus': focus,
                'timesteps': int(timesteps),
                'target_score': float(self.target_score),
                'phase3_baseline': float(self.phase3_best['score']),
                'best_score': float(best_score),
                'best_config': best_config,
                'target_achievement': float(target_achievement),
                'total_time_minutes': float(total_time / 60),
                'results': converted_results
            }

            output_file = os.path.join(self.results_dir, f'phase4_enhanced_{focus}_{self.timestamp}.json')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Phase 4 ê²°ê³¼: {output_file}")
            print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
            return output_data
        else:
            print("âŒ ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

    def create_performance_analysis(self, results_file: str):
        """ê²°ê³¼ JSONì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„± ë° ì €ì¥."""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if not data.get('results'):
                print("ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            configs, scores, utilizations, success_rates = [], [], [], []
            for name, result in data['results'].items():
                configs.append(name)
                scores.append(result['combined_score'])
                utilizations.append(result['mean_utilization'] * 100)
                success_rates.append(result['success_rate'] * 100)

            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Phase 4 Enhanced Optimization Analysis\n'
                         f'Best: {data["best_score"]:.2f} (Target: {data["target_score"]})',
                         fontsize=16, fontweight='bold')

            # 1) ì¢…í•© ì ìˆ˜ ë°”ì°¨íŠ¸
            axes[0,0].bar(range(len(configs)), scores, color='skyblue', alpha=0.7)
            axes[0,0].axhline(y=data['target_score'], color='red', linestyle='--', label=f'Target: {data["target_score"]}')
            axes[0,0].axhline(y=data['phase3_baseline'], color='orange', linestyle='--', label=f'Phase3: {data["phase3_baseline"]:.2f}')
            axes[0,0].set_title('Combined Scores')
            axes[0,0].set_ylabel('Score')
            axes[0,0].legend()
            axes[0,0].tick_params(axis='x', rotation=45)

            # 2) í™œìš©ë¥ 
            axes[0,1].bar(range(len(configs)), utilizations, color='lightgreen', alpha=0.7)
            axes[0,1].set_title('Space Utilization (%)')
            axes[0,1].set_ylabel('Utilization %')
            axes[0,1].tick_params(axis='x', rotation=45)

            # 3) ì„±ê³µë¥ 
            axes[1,0].bar(range(len(configs)), success_rates, color='lightcoral', alpha=0.7)
            axes[1,0].set_title('Success Rate (%)')
            axes[1,0].set_ylabel('Success Rate %')
            axes[1,0].tick_params(axis='x', rotation=45)

            # 4) ìƒê´€ê´€ê³„ ì‚°ì ë„ (í™œìš©ë¥  vs ì ìˆ˜)
            axes[1,1].scatter(utilizations, scores, alpha=0.7, s=100)
            axes[1,1].set_xlabel('Utilization %')
            axes[1,1].set_ylabel('Combined Score')
            axes[1,1].set_title('Utilization vs Score')

            for ax in axes.flat:
                if hasattr(ax, 'set_xticks'):
                    ax.set_xticks(range(len(configs)))
                    ax.set_xticklabels([c[:15] + '...' if len(c) > 15 else c for c in configs], rotation=45, ha='right')

            plt.tight_layout()
            chart_file = results_file.replace('.json', '_analysis.png')
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ë¶„ì„ ì°¨íŠ¸ ì €ì¥: {chart_file}")
            plt.close()
        except Exception as e:
            print(f"âŒ ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")


class EnhancedRewardWrapper(gym.RewardWrapper):
    """ê°•í™”ëœ ë³´ìƒ ë˜í¼(ê°„ë‹¨í˜•).

    - ì»¨í…Œì´ë„ˆ í™œìš©ë¥ , ì—°ì† ë°°ì¹˜, ì„ê³„ê°’ ëŒíŒŒ ë“±ì˜ ë³´ë„ˆìŠ¤ë¥¼ ì¶”ê°€í•´ í•™ìŠµì„ ê°€ì†
    - ë…¼ë¬¸ì  ê´€ì ì—ì„œì˜ Reward Shaping: íƒìƒ‰ì„ ìœ ë„í•˜ë˜ ê³¼ë„í•œ í¸í–¥ ë°©ì§€
    """

    def __init__(self, env):
        super().__init__(env)
        self.previous_utilization = 0.0
        self.consecutive_placements = 0

    def reset(self, **kwargs):
        self.previous_utilization = 0.0
        self.consecutive_placements = 0
        return self.env.reset(**kwargs)

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        enhanced_reward = self.reward(reward)
        return obs, enhanced_reward, terminated, truncated, info

    def reward(self, reward):
        current_utilization, placement_count = calculate_utilization_and_items(self.env)

        enhanced_reward = reward
        if current_utilization > 0:
            enhanced_reward += (current_utilization ** 1.5) * 3.0
        if current_utilization > self.previous_utilization:
            enhanced_reward += (current_utilization - self.previous_utilization) * 5.0
        if placement_count > 0:
            self.consecutive_placements += 1
            enhanced_reward += min(self.consecutive_placements * 0.1, 1.0)
        else:
            self.consecutive_placements = 0
        if current_utilization > 0.25:
            enhanced_reward += 2.0
        elif current_utilization > 0.20:
            enhanced_reward += 1.0
        if placement_count >= 5:
            enhanced_reward += (placement_count - 4) * 0.5
        if placement_count > 0:
            efficiency = current_utilization / placement_count
            enhanced_reward += efficiency * 2.0

        self.previous_utilization = current_utilization
        return enhanced_reward


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸: ì‹¤í—˜ ì‹¤í–‰ ë˜ëŠ” ë¶„ì„ ëª¨ë“œ."""
    import argparse

    parser = argparse.ArgumentParser(description='Enhanced Optimization for 3D Bin Packing')
    parser.add_argument('--focus', choices=['all', 'stability', 'architecture', 'optimization'],
                        default='all', help='Optimization focus area')
    parser.add_argument('--timesteps', type=int, default=35000, help='Training timesteps')
    parser.add_argument('--analyze', type=str, help='Analyze results from JSON file')

    args = parser.parse_args()
    optimizer = EnhancedOptimizer()

    if args.analyze:
        optimizer.create_performance_analysis(args.analyze)
    else:
        result = optimizer.run_phase4_optimization(focus=args.focus, timesteps=args.timesteps)
        if result and result.get('results'):
            output_file = os.path.join(optimizer.results_dir,
                                       f'phase4_enhanced_{args.focus}_{optimizer.timestamp}.json')
            optimizer.create_performance_analysis(output_file)

            best_score = result['best_score']
            target_score = result['target_score']
            if best_score >= target_score:
                print(f"\nğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ëª©í‘œ {target_score}ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
                print(f"ğŸ† ìµœì¢… ì„±ëŠ¥: {best_score:.3f}ì ")
            else:
                remaining = target_score - best_score
                print(f"\nğŸ“Š ì¶”ê°€ ê°œì„  ê¶Œì¥ì‚¬í•­:")
                print(f"   ëª©í‘œê¹Œì§€ {remaining:.3f}ì  ë¶€ì¡±")
                if remaining > 1.0:
                    print(f"   â¡ï¸ í•™ìŠµ ì‹œê°„ì„ 50,000ìŠ¤í…ìœ¼ë¡œ ì¦ê°€ ì‹œë„")
                    print(f"   â¡ï¸ ì•™ìƒë¸” ëª¨ë¸ë§ ì‹œë„")
                else:
                    print(f"   â¡ï¸ ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ ë‹¬ì„± ê°€ëŠ¥")
                    print(f"   â¡ï¸ ë³´ìƒ í•¨ìˆ˜ ì¶”ê°€ ìµœì í™” ê¶Œì¥")


if __name__ == "__main__":
    main()



```

```python
#!/usr/bin/env python3
"""
í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸(ì£¼ì„ í™•ì¥íŒ)

ê°œìš”
- Phase 4 íƒìƒ‰ ê²°ê³¼ë¡œ ì–»ì€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°(`PRODUCTION_OPTIMAL`)ë¡œ í•™ìŠµ/í‰ê°€ë¥¼ ìˆ˜í–‰í•´
  ì¬í˜„ì„±Â·ì•ˆì •ì„±ì„ ì ê²€í•©ë‹ˆë‹¤.
- í™˜ê²½ì€ `src/train_maskable_ppo.make_env`ë¥¼ í†µí•´ ìƒì„±ë˜ë©°, ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ê³¼ ê°œì„ í˜• ë³´ìƒ
  ì‰ì´í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ë…¼ë¬¸ ë§¥ë½: Transformer ê¸°ë°˜ DRLê³¼ ë‹¬ë¦¬ ë³¸ ì½”ë“œëŠ” MLP+MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  ìƒíƒœí‘œí˜„(ë†’ì´ë§µ+ê°€ì‹œë°•ìŠ¤), ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹, ë³´ìƒ ì„¤ê³„ë¥¼ í†µí•´ íš¨ìœ¨ì  íƒìƒ‰ì´ë¼ëŠ” ê³µí†µ ëª©í‘œë¥¼ ì§€í–¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•(ì˜ˆ)
- ì™„ì „ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --timesteps 50000 --episodes 50
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --quick

ì¶œë ¥
- ëª¨ë¸:  models/production_optimal_{timestamp}
- ê²°ê³¼:  results/production_final_{timestamp}.json (ì¢…í•© ì ìˆ˜, í™œìš©ë¥ /ì„±ê³µë¥  ë“±)
"""

import os
import sys
import json
import time
import numpy as np
from datetime import datetime
import warnings

# ì„œë²„/í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ ì•ˆì „ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

# Phase 4ì—ì„œ í™•ì •ëœ í”„ë¡œë•ì…˜ ìµœì  êµ¬ì„±
PRODUCTION_OPTIMAL = {
    "learning_rate": 0.00013,
    "n_steps": 768,
    "batch_size": 96,
    "n_epochs": 5,
    "clip_range": 0.18,
    "ent_coef": 0.008,
    "vf_coef": 0.5,
    "gae_lambda": 0.96,
    "net_arch": {"pi": [256, 128, 64], "vf": [256, 128, 64]}
}


def create_production_env(container_size=None, num_boxes=12, seed=42):
    """í”„ë¡œë•ì…˜ í™˜ê²½ íŒ©í† ë¦¬.

    - `train_maskable_ppo.make_env`ë¥¼ í†µí•´ Gym í™˜ê²½(`PackingEnv-v0`) ìƒì„±
    - ê°œì„ í˜• ë³´ìƒ(`improved_reward_shaping=True`)ê³¼ ActionMasker ì ìš©
    """
    try:
        from train_maskable_ppo import make_env
        if container_size is None:
            container_size = [10, 10, 10]
        env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=seed,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        print(f"âœ… í”„ë¡œë•ì…˜ í™˜ê²½ ìƒì„±: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{num_boxes}ê°œ")
        return env
    except Exception as e:
        print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def train_production_model(env, timesteps=50000):
    """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ MaskablePPO í•™ìŠµ.

    ë°˜í™˜ê°’
    - (model, duration_seconds)
    """
    try:
        import torch
        import torch.nn as nn
        from sb3_contrib import MaskablePPO

        print(f"ğŸš€ í”„ë¡œë•ì…˜ í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í…")
        print(f"ğŸ“Š ìµœì  ì„¤ì •: LR={PRODUCTION_OPTIMAL['learning_rate']:.2e}, "
              f"Steps={PRODUCTION_OPTIMAL['n_steps']}, "
              f"Batch={PRODUCTION_OPTIMAL['batch_size']}")

        start_time = time.time()
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=PRODUCTION_OPTIMAL['learning_rate'],
            n_steps=PRODUCTION_OPTIMAL['n_steps'],
            batch_size=PRODUCTION_OPTIMAL['batch_size'],
            n_epochs=PRODUCTION_OPTIMAL['n_epochs'],
            gamma=0.99,
            gae_lambda=PRODUCTION_OPTIMAL['gae_lambda'],
            clip_range=PRODUCTION_OPTIMAL['clip_range'],
            ent_coef=PRODUCTION_OPTIMAL['ent_coef'],
            vf_coef=PRODUCTION_OPTIMAL['vf_coef'],
            max_grad_norm=0.5,
            verbose=1,
            seed=42,
            policy_kwargs=dict(
                net_arch=PRODUCTION_OPTIMAL['net_arch'],
                activation_fn=nn.ReLU,
                share_features_extractor=True,
            )
        )
        model.learn(total_timesteps=timesteps, progress_bar=True)
        duration = time.time() - start_time
        print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ: {duration/60:.1f}ë¶„")
        return model, duration
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
        return None, 0


def evaluate_production_model(model, container_size=None, num_boxes=12, n_episodes=50):
    """ê°•í™”ëœ í”„ë¡œë•ì…˜ í‰ê°€ ë£¨í‹´.

    - ë‹¤ì–‘í•œ ì‹œë“œë¡œ ë‹¤ìˆ˜ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë³´ìƒÂ·í™œìš©ë¥ Â·ì„±ê³µë¥ ì„ ì¸¡ì •
    - ì„±ê³µ ê¸°ì¤€: í™œìš©ë¥  25% ì´ìƒ ë˜ëŠ” ë°•ìŠ¤ 50% ì´ìƒ ë°°ì¹˜
    """
    print(f"ğŸ” í”„ë¡œë•ì…˜ í‰ê°€ ì‹œì‘: {n_episodes} ì—í”¼ì†Œë“œ")

    all_rewards, all_utilizations, placement_counts = [], [], []
    success_count = 0

    for ep in range(n_episodes):
        seed = 200 + ep * 3
        eval_env = create_production_env(container_size, num_boxes, seed)
        if eval_env is None:
            continue

        obs, _ = eval_env.reset(seed=seed)
        episode_reward = 0.0

        for step in range(50):  # ìµœëŒ€ 50ìŠ¤í…
            try:
                action, _ = model.predict(obs, deterministic=False)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                if terminated or truncated:
                    break
            except Exception:
                break

        # ì„±ê³¼ ê³„ì‚°(ë³´ìƒê³¼ í™œìš©ë¥ ì€ í™˜ê²½ì— ë”°ë¼ ìƒì´í•  ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì¬ê³„ì‚°)
        utilization = 0.0
        placed_boxes = 0
        try:
            if hasattr(eval_env.unwrapped, 'container'):
                placed_volume = sum(
                    box.volume for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
                container_volume = eval_env.unwrapped.container.volume
                utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                placed_boxes = sum(
                    1 for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
        except Exception:
            pass

        if utilization >= 0.25 or placed_boxes >= num_boxes * 0.5:
            success_count += 1

        all_rewards.append(episode_reward)
        all_utilizations.append(utilization)
        placement_counts.append(placed_boxes)

        if ep < 10 or ep % 10 == 0:
            print(f"   ì—í”¼ì†Œë“œ {ep+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={utilization:.1%}, ë°•ìŠ¤={placed_boxes}ê°œ")

        eval_env.close()

    if not all_rewards:
        return None

    results = {
        'mean_reward': np.mean(all_rewards),
        'std_reward': np.std(all_rewards),
        'mean_utilization': np.mean(all_utilizations),
        'std_utilization': np.std(all_utilizations),
        'mean_placement': np.mean(placement_counts),
        'max_placement': max(placement_counts),
        'success_rate': success_count / len(all_rewards),
        'combined_score': np.mean(all_rewards) * 0.3 + np.mean(all_utilizations) * 100 * 0.7,
        'episodes': len(all_rewards),
        'all_rewards': all_rewards,
        'all_utilizations': all_utilizations
    }
    return results


def production_final_test(timesteps=50000, eval_episodes=50):
    """ì—”ë“œíˆ¬ì—”ë“œ í”„ë¡œë•ì…˜ ê²€ì¦: í•™ìŠµâ†’ì €ì¥â†’í‰ê°€â†’ìš”ì•½ ì €ì¥."""
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ì‹œì‘")
    print(f"ğŸ“Š ëª©í‘œ: 20.591ì  ì¬í˜„ ë° ì•ˆì •ì„± ê²€ì¦")
    print("="*60)

    container_size = [10, 10, 10]
    num_boxes = 12

    env = create_production_env(container_size, num_boxes, 42)
    if env is None:
        return False

    print(f"\nğŸ“ 1ë‹¨ê³„: í”„ë¡œë•ì…˜ ëª¨ë¸ í•™ìŠµ ({timesteps:,} ìŠ¤í…)")
    model, train_time = train_production_model(env, timesteps)
    if model is None:
        env.close()
        return False

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"models/production_optimal_{timestamp}"
    os.makedirs('models', exist_ok=True)
    model.save(model_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")

    print(f"\nğŸ“Š 2ë‹¨ê³„: ê°•í™”ëœ í‰ê°€ ({eval_episodes} ì—í”¼ì†Œë“œ)")
    results = evaluate_production_model(model, container_size, num_boxes, eval_episodes)
    env.close()
    if results is None:
        return False

    print("\n" + "="*60)
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {results['combined_score']:.3f}")
    print(f"ğŸ¯ ëª©í‘œ ëŒ€ë¹„: {(results['combined_score']/20.591*100):.1f}% (ëª©í‘œ: 20.591)")
    print(f"ğŸ’° í‰ê·  ë³´ìƒ: {results['mean_reward']:.3f} Â± {results['std_reward']:.3f}")
    print(f"ğŸ“¦ í‰ê·  í™œìš©ë¥ : {results['mean_utilization']:.1%} Â± {results['std_utilization']:.1%}")
    print(f"ğŸ² í‰ê·  ë°°ì¹˜: {results['mean_placement']:.1f}ê°œ (ìµœëŒ€: {results['max_placement']}ê°œ)")
    print(f"âœ… ì„±ê³µë¥ : {results['success_rate']:.1%}")
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time/60:.1f}ë¶„")

    if results['combined_score'] >= 20.0:
        print(f"ğŸ‰ ìš°ìˆ˜! ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± ë˜ëŠ” ê·¼ì ‘")
    elif results['combined_score'] >= 18.57:
        print(f"âœ… ì„±ê³µ! Phase 3 ëª©í‘œ ë‹¬ì„±")
    else:
        print(f"ğŸ“ˆ ê°œì„  í•„ìš”: ì¶”ê°€ íŠœë‹ ê¶Œì¥")

    final_results = {
        'timestamp': timestamp,
        'test_type': 'production_final',
        'params': PRODUCTION_OPTIMAL,
        'config': {
            'container_size': container_size,
            'num_boxes': num_boxes,
            'timesteps': timesteps,
            'eval_episodes': eval_episodes
        },
        'performance': results,
        'training_time_minutes': train_time/60,
        'model_path': model_path,
        'target_score': 20.591,
        'achievement_rate': results['combined_score']/20.591*100
    }
    os.makedirs('results', exist_ok=True)
    results_file = f"results/production_final_{timestamp}.json"
    with open(results_file, 'w') as f:
        json.dump(final_results, f, indent=2, default=str)
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")

    return results['combined_score'] >= 18.57


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸: ì™„ì „ í…ŒìŠ¤íŠ¸/ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›."""
    import argparse

    parser = argparse.ArgumentParser(description='í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… í…ŒìŠ¤íŠ¸')
    parser.add_argument('--timesteps', type=int, default=50000, help='í•™ìŠµ ìŠ¤í… ìˆ˜')
    parser.add_argument('--episodes', type=int, default=50, help='í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (25000 ìŠ¤í…)')
    args = parser.parse_args()

    if args.quick:
        timesteps = 25000
        episodes = 30
        print("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    else:
        timesteps = args.timesteps
        episodes = args.episodes
        print("ğŸ† ì™„ì „ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")

    print(f"ğŸš€ ì„¤ì •: {timesteps:,} ìŠ¤í…, {episodes} ì—í”¼ì†Œë“œ")

    start_time = time.time()
    success = production_final_test(timesteps, episodes)
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    print("ğŸ‰ í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ê³µ!" if success else "ğŸ“ˆ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()



```

```python
#!/usr/bin/env python3
"""
í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸(ì£¼ì„ í™•ì¥íŒ)

ê°œìš”
- Phase 4 íƒìƒ‰ ê²°ê³¼ë¡œ ì–»ì€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°(`PRODUCTION_OPTIMAL`)ë¡œ í•™ìŠµ/í‰ê°€ë¥¼ ìˆ˜í–‰í•´
  ì¬í˜„ì„±Â·ì•ˆì •ì„±ì„ ì ê²€í•©ë‹ˆë‹¤.
- í™˜ê²½ì€ `src/train_maskable_ppo.make_env`ë¥¼ í†µí•´ ìƒì„±ë˜ë©°, ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ê³¼ ê°œì„ í˜• ë³´ìƒ
  ì‰ì´í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ë…¼ë¬¸ ë§¥ë½: Transformer ê¸°ë°˜ DRLê³¼ ë‹¬ë¦¬ ë³¸ ì½”ë“œëŠ” MLP+MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  ìƒíƒœí‘œí˜„(ë†’ì´ë§µ+ê°€ì‹œë°•ìŠ¤), ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹, ë³´ìƒ ì„¤ê³„ë¥¼ í†µí•´ íš¨ìœ¨ì  íƒìƒ‰ì´ë¼ëŠ” ê³µí†µ ëª©í‘œë¥¼ ì§€í–¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•(ì˜ˆ)
- ì™„ì „ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --timesteps 50000 --episodes 50
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --quick

ì¶œë ¥
- ëª¨ë¸:  models/production_optimal_{timestamp}
- ê²°ê³¼:  results/production_final_{timestamp}.json (ì¢…í•© ì ìˆ˜, í™œìš©ë¥ /ì„±ê³µë¥  ë“±)
"""

import os
import sys
import json
import time
import numpy as np
from datetime import datetime
import warnings

# ì„œë²„/í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ ì•ˆì „ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

# Phase 4ì—ì„œ í™•ì •ëœ í”„ë¡œë•ì…˜ ìµœì  êµ¬ì„±
PRODUCTION_OPTIMAL = {
    "learning_rate": 0.00013,
    "n_steps": 768,
    "batch_size": 96,
    "n_epochs": 5,
    "clip_range": 0.18,
    "ent_coef": 0.008,
    "vf_coef": 0.5,
    "gae_lambda": 0.96,
    "net_arch": {"pi": [256, 128, 64], "vf": [256, 128, 64]}
}


def create_production_env(container_size=None, num_boxes=12, seed=42):
    """í”„ë¡œë•ì…˜ í™˜ê²½ íŒ©í† ë¦¬.

    - `train_maskable_ppo.make_env`ë¥¼ í†µí•´ Gym í™˜ê²½(`PackingEnv-v0`) ìƒì„±
    - ê°œì„ í˜• ë³´ìƒ(`improved_reward_shaping=True`)ê³¼ ActionMasker ì ìš©
    """
    try:
        from train_maskable_ppo import make_env
        if container_size is None:
            container_size = [10, 10, 10]
        env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=seed,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        print(f"âœ… í”„ë¡œë•ì…˜ í™˜ê²½ ìƒì„±: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{num_boxes}ê°œ")
        return env
    except Exception as e:
        # srcê°€ ê²½ë¡œì— ì—†ê±°ë‚˜ ëŸ°íƒ€ì„ ëª¨ë“ˆ ë¬¸ì œì¼ ë•Œ ì¹œì ˆ ì•ˆë‚´
        print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return None


def train_production_model(env, timesteps=50000):
    """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ MaskablePPO í•™ìŠµ.

    ë°˜í™˜ê°’
    - (model, duration_seconds)
    """
    try:
        import torch
        import torch.nn as nn
        from sb3_contrib import MaskablePPO

        print(f"ğŸš€ í”„ë¡œë•ì…˜ í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í…")
        print(f"ğŸ“Š ìµœì  ì„¤ì •: LR={PRODUCTION_OPTIMAL['learning_rate']:.2e}, "
              f"Steps={PRODUCTION_OPTIMAL['n_steps']}, "
              f"Batch={PRODUCTION_OPTIMAL['batch_size']}")

        start_time = time.time()
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=PRODUCTION_OPTIMAL['learning_rate'],
            n_steps=PRODUCTION_OPTIMAL['n_steps'],
            batch_size=PRODUCTION_OPTIMAL['batch_size'],
            n_epochs=PRODUCTION_OPTIMAL['n_epochs'],
            gamma=0.99,
            gae_lambda=PRODUCTION_OPTIMAL['gae_lambda'],
            clip_range=PRODUCTION_OPTIMAL['clip_range'],
            ent_coef=PRODUCTION_OPTIMAL['ent_coef'],
            vf_coef=PRODUCTION_OPTIMAL['vf_coef'],
            max_grad_norm=0.5,
            verbose=1,
            seed=42,
            policy_kwargs=dict(
                net_arch=PRODUCTION_OPTIMAL['net_arch'],
                activation_fn=nn.ReLU,
                share_features_extractor=True,
            )
        )
        model.learn(total_timesteps=timesteps, progress_bar=True)
        duration = time.time() - start_time
        print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ: {duration/60:.1f}ë¶„")
        return model, duration
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
        return None, 0


def evaluate_production_model(model, container_size=None, num_boxes=12, n_episodes=50):
    """ê°•í™”ëœ í”„ë¡œë•ì…˜ í‰ê°€ ë£¨í‹´.

    - ë‹¤ì–‘í•œ ì‹œë“œë¡œ ë‹¤ìˆ˜ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë³´ìƒÂ·í™œìš©ë¥ Â·ì„±ê³µë¥ ì„ ì¸¡ì •
    - ì„±ê³µ ê¸°ì¤€: í™œìš©ë¥  25% ì´ìƒ ë˜ëŠ” ë°•ìŠ¤ 50% ì´ìƒ ë°°ì¹˜
    """
    print(f"ğŸ” í”„ë¡œë•ì…˜ í‰ê°€ ì‹œì‘: {n_episodes} ì—í”¼ì†Œë“œ")

    all_rewards, all_utilizations, placement_counts = [], [], []
    success_count = 0

    for ep in range(n_episodes):
        seed = 200 + ep * 3
        eval_env = create_production_env(container_size, num_boxes, seed)
        if eval_env is None:
            continue

        obs, _ = eval_env.reset(seed=seed)
        episode_reward = 0.0

        for step in range(50):  # ìµœëŒ€ 50ìŠ¤í…
            try:
                action, _ = model.predict(obs, deterministic=False)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                if terminated or truncated:
                    break
            except Exception:
                break

        # ì„±ê³¼ ê³„ì‚°(ë³´ìƒê³¼ í™œìš©ë¥ ì€ í™˜ê²½ì— ë”°ë¼ ìƒì´í•  ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì¬ê³„ì‚°)
        utilization = 0.0
        placed_boxes = 0
        try:
            if hasattr(eval_env.unwrapped, 'container'):
                placed_volume = sum(
                    box.volume for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
                container_volume = eval_env.unwrapped.container.volume
                utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                placed_boxes = sum(
                    1 for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
        except Exception:
            pass

        if utilization >= 0.25 or placed_boxes >= num_boxes * 0.5:
            success_count += 1

        all_rewards.append(episode_reward)
        all_utilizations.append(utilization)
        placement_counts.append(placed_boxes)

        if ep < 10 or ep % 10 == 0:
            print(f"   ì—í”¼ì†Œë“œ {ep+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={utilization:.1%}, ë°•ìŠ¤={placed_boxes}ê°œ")

        eval_env.close()

    if not all_rewards:
        return None

    results = {
        'mean_reward': np.mean(all_rewards),
        'std_reward': np.std(all_rewards),
        'mean_utilization': np.mean(all_utilizations),
        'std_utilization': np.std(all_utilizations),
        'mean_placement': np.mean(placement_counts),
        'max_placement': max(placement_counts),
        'success_rate': success_count / len(all_rewards),
        'combined_score': np.mean(all_rewards) * 0.3 + np.mean(all_utilizations) * 100 * 0.7,
        'episodes': len(all_rewards),
        'all_rewards': all_rewards,
        'all_utilizations': all_utilizations
    }
    return results


def production_final_test(timesteps=50000, eval_episodes=50):
    """ì—”ë“œíˆ¬ì—”ë“œ í”„ë¡œë•ì…˜ ê²€ì¦: í•™ìŠµâ†’ì €ì¥â†’í‰ê°€â†’ìš”ì•½ ì €ì¥."""
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ì‹œì‘")
    print(f"ğŸ“Š ëª©í‘œ: 20.591ì  ì¬í˜„ ë° ì•ˆì •ì„± ê²€ì¦")
    print("="*60)

    container_size = [10, 10, 10]
    num_boxes = 12

    env = create_production_env(container_size, num_boxes, 42)
    if env is None:
        return False

    print(f"\nğŸ“ 1ë‹¨ê³„: í”„ë¡œë•ì…˜ ëª¨ë¸ í•™ìŠµ ({timesteps:,} ìŠ¤í…)")
    model, train_time = train_production_model(env, timesteps)
    if model is None:
        env.close()
        return False

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"models/production_optimal_{timestamp}"
    os.makedirs('models', exist_ok=True)
    model.save(model_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")

    print(f"\nğŸ“Š 2ë‹¨ê³„: ê°•í™”ëœ í‰ê°€ ({eval_episodes} ì—í”¼ì†Œë“œ)")
    results = evaluate_production_model(model, container_size, num_boxes, eval_episodes)
    env.close()
    if results is None:
        return False

    print("\n" + "="*60)
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {results['combined_score']:.3f}")
    print(f"ğŸ¯ ëª©í‘œ ëŒ€ë¹„: {(results['combined_score']/20.591*100):.1f}% (ëª©í‘œ: 20.591)")
    print(f"ğŸ’° í‰ê·  ë³´ìƒ: {results['mean_reward']:.3f} Â± {results['std_reward']:.3f}")
    print(f"ğŸ“¦ í‰ê·  í™œìš©ë¥ : {results['mean_utilization']:.1%} Â± {results['std_utilization']:.1%}")
    print(f"ğŸ² í‰ê·  ë°°ì¹˜: {results['mean_placement']:.1f}ê°œ (ìµœëŒ€: {results['max_placement']}ê°œ)")
    print(f"âœ… ì„±ê³µë¥ : {results['success_rate']:.1%}")
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time/60:.1f}ë¶„")

    if results['combined_score'] >= 20.0:
        print(f"ğŸ‰ ìš°ìˆ˜! ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± ë˜ëŠ” ê·¼ì ‘")
    elif results['combined_score'] >= 18.57:
        print(f"âœ… ì„±ê³µ! Phase 3 ëª©í‘œ ë‹¬ì„±")
    else:
        print(f"ğŸ“ˆ ê°œì„  í•„ìš”: ì¶”ê°€ íŠœë‹ ê¶Œì¥")

    final_results = {
        'timestamp': timestamp,
        'test_type': 'production_final',
        'params': PRODUCTION_OPTIMAL,
        'config': {
            'container_size': container_size,
            'num_boxes': num_boxes,
            'timesteps': timesteps,
            'eval_episodes': eval_episodes
        },
        'performance': results,
        'training_time_minutes': train_time/60,
        'model_path': model_path,
        'target_score': 20.591,
        'achievement_rate': results['combined_score']/20.591*100
    }
    os.makedirs('results', exist_ok=True)
    results_file = f"results/production_final_{timestamp}.json"
    with open(results_file, 'w') as f:
        json.dump(final_results, f, indent=2, default=str)
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")

    return results['combined_score'] >= 18.57


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸: ì™„ì „ í…ŒìŠ¤íŠ¸/ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›."""
    import argparse

    parser = argparse.ArgumentParser(description='í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… í…ŒìŠ¤íŠ¸')
    parser.add_argument('--timesteps', type=int, default=50000, help='í•™ìŠµ ìŠ¤í… ìˆ˜')
    parser.add_argument('--episodes', type=int, default=50, help='í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (25000 ìŠ¤í…)')
    args = parser.parse_args()

    if args.quick:
        timesteps = 25000
        episodes = 30
        print("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    else:
        timesteps = args.timesteps
        episodes = args.episodes
        print("ğŸ† ì™„ì „ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")

    print(f"ğŸš€ ì„¤ì •: {timesteps:,} ìŠ¤í…, {episodes} ì—í”¼ì†Œë“œ")

    start_time = time.time()
    success = production_final_test(timesteps, episodes)
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    print("ğŸ‰ í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ê³µ!" if success else "ğŸ“ˆ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()



```

```python
#!/usr/bin/env python3
"""
í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸(ì£¼ì„ í™•ì¥íŒ)

ê°œìš”
- Phase 4 íƒìƒ‰ ê²°ê³¼ë¡œ ì–»ì€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°(`PRODUCTION_OPTIMAL`)ë¡œ í•™ìŠµ/í‰ê°€ë¥¼ ìˆ˜í–‰í•´
  ì¬í˜„ì„±Â·ì•ˆì •ì„±ì„ ì ê²€í•©ë‹ˆë‹¤.
- í™˜ê²½ì€ `src/train_maskable_ppo.make_env`ë¥¼ í†µí•´ ìƒì„±ë˜ë©°, ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ê³¼ ê°œì„ í˜• ë³´ìƒ
  ì‰ì´í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ë…¼ë¬¸ ë§¥ë½: Transformer ê¸°ë°˜ DRLê³¼ ë‹¬ë¦¬ ë³¸ ì½”ë“œëŠ” MLP+MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  ìƒíƒœí‘œí˜„(ë†’ì´ë§µ+ê°€ì‹œë°•ìŠ¤), ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹, ë³´ìƒ ì„¤ê³„ë¥¼ í†µí•´ íš¨ìœ¨ì  íƒìƒ‰ì´ë¼ëŠ” ê³µí†µ ëª©í‘œë¥¼ ì§€í–¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•(ì˜ˆ)
- ì™„ì „ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --timesteps 50000 --episodes 50
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --quick

ì¶œë ¥
- ëª¨ë¸:  models/production_optimal_{timestamp}
- ê²°ê³¼:  results/production_final_{timestamp}.json (ì¢…í•© ì ìˆ˜, í™œìš©ë¥ /ì„±ê³µë¥  ë“±)
"""

import os
import sys
import json
import time
import numpy as np
from datetime import datetime
import warnings

# ì„œë²„/í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ ì•ˆì „ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

# Phase 4ì—ì„œ í™•ì •ëœ í”„ë¡œë•ì…˜ ìµœì  êµ¬ì„±
PRODUCTION_OPTIMAL = {
    "learning_rate": 0.00013,
    "n_steps": 768,
    "batch_size": 96,
    "n_epochs": 5,
    "clip_range": 0.18,
    "ent_coef": 0.008,
    "vf_coef": 0.5,
    "gae_lambda": 0.96,
    "net_arch": {"pi": [256, 128, 64], "vf": [256, 128, 64]}
}


def create_production_env(container_size=None, num_boxes=12, seed=42):
    """í”„ë¡œë•ì…˜ í™˜ê²½ íŒ©í† ë¦¬.

    - `train_maskable_ppo.make_env`ë¥¼ í†µí•´ Gym í™˜ê²½(`PackingEnv-v0`) ìƒì„±
    - ê°œì„ í˜• ë³´ìƒ(`improved_reward_shaping=True`)ê³¼ ActionMasker ì ìš©
    """
    try:
        from train_maskable_ppo import make_env
        if container_size is None:
            container_size = [10, 10, 10]
        env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=seed,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        print(f"âœ… í”„ë¡œë•ì…˜ í™˜ê²½ ìƒì„±: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{num_boxes}ê°œ")
        return env
    except Exception as e:
        # srcê°€ ê²½ë¡œì— ì—†ê±°ë‚˜ ëŸ°íƒ€ì„ ëª¨ë“ˆ ë¬¸ì œì¼ ë•Œ ì¹œì ˆ ì•ˆë‚´
        print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return None


def train_production_model(env, timesteps=50000):
    """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ MaskablePPO í•™ìŠµ.

    ë°˜í™˜ê°’
    - (model, duration_seconds)
    """
    try:
        import torch
        import torch.nn as nn
        from sb3_contrib import MaskablePPO

        print(f"ğŸš€ í”„ë¡œë•ì…˜ í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í…")
        print(f"ğŸ“Š ìµœì  ì„¤ì •: LR={PRODUCTION_OPTIMAL['learning_rate']:.2e}, "
              f"Steps={PRODUCTION_OPTIMAL['n_steps']}, "
              f"Batch={PRODUCTION_OPTIMAL['batch_size']}")

        start_time = time.time()
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=PRODUCTION_OPTIMAL['learning_rate'],
            n_steps=PRODUCTION_OPTIMAL['n_steps'],
            batch_size=PRODUCTION_OPTIMAL['batch_size'],
            n_epochs=PRODUCTION_OPTIMAL['n_epochs'],
            gamma=0.99,
            gae_lambda=PRODUCTION_OPTIMAL['gae_lambda'],
            clip_range=PRODUCTION_OPTIMAL['clip_range'],
            ent_coef=PRODUCTION_OPTIMAL['ent_coef'],
            vf_coef=PRODUCTION_OPTIMAL['vf_coef'],
            max_grad_norm=0.5,
            verbose=1,
            seed=42,
            policy_kwargs=dict(
                net_arch=PRODUCTION_OPTIMAL['net_arch'],
                activation_fn=nn.ReLU,
                share_features_extractor=True,
            )
        )
        model.learn(total_timesteps=timesteps, progress_bar=True)
        duration = time.time() - start_time
        print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ: {duration/60:.1f}ë¶„")
        return model, duration
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
        return None, 0


def evaluate_production_model(model, container_size=None, num_boxes=12, n_episodes=50):
    """ê°•í™”ëœ í”„ë¡œë•ì…˜ í‰ê°€ ë£¨í‹´.

    - ë‹¤ì–‘í•œ ì‹œë“œë¡œ ë‹¤ìˆ˜ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë³´ìƒÂ·í™œìš©ë¥ Â·ì„±ê³µë¥ ì„ ì¸¡ì •
    - ì„±ê³µ ê¸°ì¤€: í™œìš©ë¥  25% ì´ìƒ ë˜ëŠ” ë°•ìŠ¤ 50% ì´ìƒ ë°°ì¹˜
    """
    print(f"ğŸ” í”„ë¡œë•ì…˜ í‰ê°€ ì‹œì‘: {n_episodes} ì—í”¼ì†Œë“œ")

    all_rewards, all_utilizations, placement_counts = [], [], []
    success_count = 0

    for ep in range(n_episodes):
        seed = 200 + ep * 3
        eval_env = create_production_env(container_size, num_boxes, seed)
        if eval_env is None:
            continue

        obs, _ = eval_env.reset(seed=seed)
        episode_reward = 0.0

        for _ in range(50):  # ìµœëŒ€ 50ìŠ¤í…
            try:
                action, _ = model.predict(obs, deterministic=False)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                if terminated or truncated:
                    break
            except Exception:
                break

        # ì„±ê³¼ ê³„ì‚°(ë³´ìƒê³¼ í™œìš©ë¥ ì€ í™˜ê²½ì— ë”°ë¼ ìƒì´í•  ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì¬ê³„ì‚°)
        utilization = 0.0
        placed_boxes = 0
        try:
            if hasattr(eval_env.unwrapped, 'container'):
                placed_volume = sum(
                    box.volume for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
                container_volume = eval_env.unwrapped.container.volume
                utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                placed_boxes = sum(
                    1 for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
        except Exception:
            pass

        if utilization >= 0.25 or placed_boxes >= num_boxes * 0.5:
            success_count += 1

        all_rewards.append(episode_reward)
        all_utilizations.append(utilization)
        placement_counts.append(placed_boxes)

        if ep < 10 or ep % 10 == 0:
            print(f"   ì—í”¼ì†Œë“œ {ep+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={utilization:.1%}, ë°•ìŠ¤={placed_boxes}ê°œ")

        eval_env.close()

    if not all_rewards:
        return None

    results = {
        'mean_reward': np.mean(all_rewards),
        'std_reward': np.std(all_rewards),
        'mean_utilization': np.mean(all_utilizations),
        'std_utilization': np.std(all_utilizations),
        'mean_placement': np.mean(placement_counts),
        'max_placement': max(placement_counts),
        'success_rate': success_count / len(all_rewards),
        'combined_score': np.mean(all_rewards) * 0.3 + np.mean(all_utilizations) * 100 * 0.7,
        'episodes': len(all_rewards),
        'all_rewards': all_rewards,
        'all_utilizations': all_utilizations
    }
    return results


def production_final_test(timesteps=50000, eval_episodes=50):
    """ì—”ë“œíˆ¬ì—”ë“œ í”„ë¡œë•ì…˜ ê²€ì¦: í•™ìŠµâ†’ì €ì¥â†’í‰ê°€â†’ìš”ì•½ ì €ì¥."""
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ì‹œì‘")
    print(f"ğŸ“Š ëª©í‘œ: 20.591ì  ì¬í˜„ ë° ì•ˆì •ì„± ê²€ì¦")
    print("="*60)

    container_size = [10, 10, 10]
    num_boxes = 12

    env = create_production_env(container_size, num_boxes, 42)
    if env is None:
        return False

    print(f"\nğŸ“ 1ë‹¨ê³„: í”„ë¡œë•ì…˜ ëª¨ë¸ í•™ìŠµ ({timesteps:,} ìŠ¤í…)")
    model, train_time = train_production_model(env, timesteps)
    if model is None:
        env.close()
        return False

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"models/production_optimal_{timestamp}"
    os.makedirs('models', exist_ok=True)
    model.save(model_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")

    print(f"\nğŸ“Š 2ë‹¨ê³„: ê°•í™”ëœ í‰ê°€ ({eval_episodes} ì—í”¼ì†Œë“œ)")
    results = evaluate_production_model(model, container_size, num_boxes, eval_episodes)
    env.close()
    if results is None:
        return False

    print("\n" + "="*60)
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {results['combined_score']:.3f}")
    print(f"ğŸ¯ ëª©í‘œ ëŒ€ë¹„: {(results['combined_score']/20.591*100):.1f}% (ëª©í‘œ: 20.591)")
    print(f"ğŸ’° í‰ê·  ë³´ìƒ: {results['mean_reward']:.3f} Â± {results['std_reward']:.3f}")
    print(f"ğŸ“¦ í‰ê·  í™œìš©ë¥ : {results['mean_utilization']:.1%} Â± {results['std_utilization']:.1%}")
    print(f"ğŸ² í‰ê·  ë°°ì¹˜: {results['mean_placement']:.1f}ê°œ (ìµœëŒ€: {results['max_placement']}ê°œ)")
    print(f"âœ… ì„±ê³µë¥ : {results['success_rate']:.1%}")
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time/60:.1f}ë¶„")

    if results['combined_score'] >= 20.0:
        print(f"ğŸ‰ ìš°ìˆ˜! ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± ë˜ëŠ” ê·¼ì ‘")
    elif results['combined_score'] >= 18.57:
        print(f"âœ… ì„±ê³µ! Phase 3 ëª©í‘œ ë‹¬ì„±")
    else:
        print(f"ğŸ“ˆ ê°œì„  í•„ìš”: ì¶”ê°€ íŠœë‹ ê¶Œì¥")

    final_results = {
        'timestamp': timestamp,
        'test_type': 'production_final',
        'params': PRODUCTION_OPTIMAL,
        'config': {
            'container_size': container_size,
            'num_boxes': num_boxes,
            'timesteps': timesteps,
            'eval_episodes': eval_episodes
        },
        'performance': results,
        'training_time_minutes': train_time/60,
        'model_path': model_path,
        'target_score': 20.591,
        'achievement_rate': results['combined_score']/20.591*100
    }
    os.makedirs('results', exist_ok=True)
    results_file = f"results/production_final_{timestamp}.json"
    with open(results_file, 'w') as f:
        json.dump(final_results, f, indent=2, default=str)
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")

    return results['combined_score'] >= 18.57


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸: ì™„ì „ í…ŒìŠ¤íŠ¸/ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›."""
    import argparse

    parser = argparse.ArgumentParser(description='í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… í…ŒìŠ¤íŠ¸')
    parser.add_argument('--timesteps', type=int, default=50000, help='í•™ìŠµ ìŠ¤í… ìˆ˜')
    parser.add_argument('--episodes', type=int, default=50, help='í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (25000 ìŠ¤í…)')
    args = parser.parse_args()

    if args.quick:
        timesteps = 25000
        episodes = 30
        print("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    else:
        timesteps = args.timesteps
        episodes = args.episodes
        print("ğŸ† ì™„ì „ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")

    print(f"ğŸš€ ì„¤ì •: {timesteps:,} ìŠ¤í…, {episodes} ì—í”¼ì†Œë“œ")

    start_time = time.time()
    success = production_final_test(timesteps, episodes)
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    print("ğŸ‰ í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ê³µ!" if success else "ğŸ“ˆ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()



```

```python
#!/usr/bin/env python3
"""
í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸(ì£¼ì„ í™•ì¥íŒ)

ê°œìš”
- Phase 4 íƒìƒ‰ ê²°ê³¼ë¡œ ì–»ì€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°(`PRODUCTION_OPTIMAL`)ë¡œ í•™ìŠµ/í‰ê°€ë¥¼ ìˆ˜í–‰í•´
  ì¬í˜„ì„±Â·ì•ˆì •ì„±ì„ ì ê²€í•©ë‹ˆë‹¤.
- í™˜ê²½ì€ `src/train_maskable_ppo.make_env`ë¥¼ í†µí•´ ìƒì„±ë˜ë©°, ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ê³¼ ê°œì„ í˜• ë³´ìƒ
  ì‰ì´í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ë…¼ë¬¸ ë§¥ë½: Transformer ê¸°ë°˜ DRLê³¼ ë‹¬ë¦¬ ë³¸ ì½”ë“œëŠ” MLP+MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  ìƒíƒœí‘œí˜„(ë†’ì´ë§µ+ê°€ì‹œë°•ìŠ¤), ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹, ë³´ìƒ ì„¤ê³„ë¥¼ í†µí•´ íš¨ìœ¨ì  íƒìƒ‰ì´ë¼ëŠ” ê³µí†µ ëª©í‘œë¥¼ ì§€í–¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•(ì˜ˆ)
- ì™„ì „ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --timesteps 50000 --episodes 50
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --quick

ì¶œë ¥
- ëª¨ë¸:  models/production_optimal_{timestamp}
- ê²°ê³¼:  results/production_final_{timestamp}.json (ì¢…í•© ì ìˆ˜, í™œìš©ë¥ /ì„±ê³µë¥  ë“±)
"""

import os
import sys
import json
import time
import numpy as np
from datetime import datetime
import warnings

# ì„œë²„/í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ ì•ˆì „ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

# Phase 4ì—ì„œ í™•ì •ëœ í”„ë¡œë•ì…˜ ìµœì  êµ¬ì„±
PRODUCTION_OPTIMAL = {
    "learning_rate": 0.00013,
    "n_steps": 768,
    "batch_size": 96,
    "n_epochs": 5,
    "clip_range": 0.18,
    "ent_coef": 0.008,
    "vf_coef": 0.5,
    "gae_lambda": 0.96,
    "net_arch": {"pi": [256, 128, 64], "vf": [256, 128, 64]}
}


def create_production_env(container_size=None, num_boxes=12, seed=42):
    """í”„ë¡œë•ì…˜ í™˜ê²½ íŒ©í† ë¦¬.

    - `train_maskable_ppo.make_env`ë¥¼ í†µí•´ Gym í™˜ê²½(`PackingEnv-v0`) ìƒì„±
    - ê°œì„ í˜• ë³´ìƒ(`improved_reward_shaping=True`)ê³¼ ActionMasker ì ìš©
    """
    try:
        from train_maskable_ppo import make_env
        if container_size is None:
            container_size = [10, 10, 10]
        env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=seed,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        print(f"âœ… í”„ë¡œë•ì…˜ í™˜ê²½ ìƒì„±: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{num_boxes}ê°œ")
        return env
    except Exception as e:
        # srcê°€ ê²½ë¡œì— ì—†ê±°ë‚˜ ëŸ°íƒ€ì„ ëª¨ë“ˆ ë¬¸ì œì¼ ë•Œ ì¹œì ˆ ì•ˆë‚´
        print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return None


def train_production_model(env, timesteps=50000):
    """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ MaskablePPO í•™ìŠµ.

    ë°˜í™˜ê°’
    - (model, duration_seconds)
    """
    try:
        import torch
        import torch.nn as nn
        from sb3_contrib import MaskablePPO

        print(f"ğŸš€ í”„ë¡œë•ì…˜ í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í…")
        print(f"ğŸ“Š ìµœì  ì„¤ì •: LR={PRODUCTION_OPTIMAL['learning_rate']:.2e}, "
              f"Steps={PRODUCTION_OPTIMAL['n_steps']}, "
              f"Batch={PRODUCTION_OPTIMAL['batch_size']}")

        start_time = time.time()
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=PRODUCTION_OPTIMAL['learning_rate'],
            n_steps=PRODUCTION_OPTIMAL['n_steps'],
            batch_size=PRODUCTION_OPTIMAL['batch_size'],
            n_epochs=PRODUCTION_OPTIMAL['n_epochs'],
            gamma=0.99,
            gae_lambda=PRODUCTION_OPTIMAL['gae_lambda'],
            clip_range=PRODUCTION_OPTIMAL['clip_range'],
            ent_coef=PRODUCTION_OPTIMAL['ent_coef'],
            vf_coef=PRODUCTION_OPTIMAL['vf_coef'],
            max_grad_norm=0.5,
            verbose=1,
            seed=42,
            policy_kwargs=dict(
                net_arch=PRODUCTION_OPTIMAL['net_arch'],
                activation_fn=nn.ReLU,
                share_features_extractor=True,
            )
        )
        model.learn(total_timesteps=timesteps, progress_bar=True)
        duration = time.time() - start_time
        print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ: {duration/60:.1f}ë¶„")
        return model, duration
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
        return None, 0


def evaluate_production_model(model, container_size=None, num_boxes=12, n_episodes=50):
    """ê°•í™”ëœ í”„ë¡œë•ì…˜ í‰ê°€ ë£¨í‹´.

    - ë‹¤ì–‘í•œ ì‹œë“œë¡œ ë‹¤ìˆ˜ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë³´ìƒÂ·í™œìš©ë¥ Â·ì„±ê³µë¥ ì„ ì¸¡ì •
    - ì„±ê³µ ê¸°ì¤€: í™œìš©ë¥  25% ì´ìƒ ë˜ëŠ” ë°•ìŠ¤ 50% ì´ìƒ ë°°ì¹˜
    """
    print(f"ğŸ” í”„ë¡œë•ì…˜ í‰ê°€ ì‹œì‘: {n_episodes} ì—í”¼ì†Œë“œ")

    all_rewards, all_utilizations, placement_counts = [], [], []
    success_count = 0

    for ep in range(n_episodes):
        seed = 200 + ep * 3
        eval_env = create_production_env(container_size, num_boxes, seed)
        if eval_env is None:
            continue

        obs, _ = eval_env.reset(seed=seed)
        episode_reward = 0.0

        for _ in range(50):  # ìµœëŒ€ 50ìŠ¤í…
            try:
                action, _ = model.predict(obs, deterministic=False)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                if terminated or truncated:
                    break
            except Exception:
                break

        # ì„±ê³¼ ê³„ì‚°(ë³´ìƒê³¼ í™œìš©ë¥ ì€ í™˜ê²½ì— ë”°ë¼ ìƒì´í•  ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì¬ê³„ì‚°)
        utilization = 0.0
        placed_boxes = 0
        try:
            if hasattr(eval_env.unwrapped, 'container'):
                placed_volume = sum(
                    box.volume for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
                container_volume = eval_env.unwrapped.container.volume
                utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                placed_boxes = sum(
                    1 for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
        except Exception:
            pass

        if utilization >= 0.25 or placed_boxes >= num_boxes * 0.5:
            success_count += 1

        all_rewards.append(episode_reward)
        all_utilizations.append(utilization)
        placement_counts.append(placed_boxes)

        if ep < 10 or ep % 10 == 0:
            print(f"   ì—í”¼ì†Œë“œ {ep+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={utilization:.1%}, ë°•ìŠ¤={placed_boxes}ê°œ")

        eval_env.close()

    if not all_rewards:
        return None

    results = {
        'mean_reward': np.mean(all_rewards),
        'std_reward': np.std(all_rewards),
        'mean_utilization': np.mean(all_utilizations),
        'std_utilization': np.std(all_utilizations),
        'mean_placement': np.mean(placement_counts),
        'max_placement': max(placement_counts),
        'success_rate': success_count / len(all_rewards),
        'combined_score': np.mean(all_rewards) * 0.3 + np.mean(all_utilizations) * 100 * 0.7,
        'episodes': len(all_rewards),
        'all_rewards': all_rewards,
        'all_utilizations': all_utilizations
    }
    return results


def production_final_test(timesteps=50000, eval_episodes=50):
    """ì—”ë“œíˆ¬ì—”ë“œ í”„ë¡œë•ì…˜ ê²€ì¦: í•™ìŠµâ†’ì €ì¥â†’í‰ê°€â†’ìš”ì•½ ì €ì¥."""
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ì‹œì‘")
    print(f"ğŸ“Š ëª©í‘œ: 20.591ì  ì¬í˜„ ë° ì•ˆì •ì„± ê²€ì¦")
    print("="*60)

    container_size = [10, 10, 10]
    num_boxes = 12

    env = create_production_env(container_size, num_boxes, 42)
    if env is None:
        return False

    print(f"\nğŸ“ 1ë‹¨ê³„: í”„ë¡œë•ì…˜ ëª¨ë¸ í•™ìŠµ ({timesteps:,} ìŠ¤í…)")
    model, train_time = train_production_model(env, timesteps)
    if model is None:
        env.close()
        return False

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"models/production_optimal_{timestamp}"
    os.makedirs('models', exist_ok=True)
    model.save(model_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")

    print(f"\nğŸ“Š 2ë‹¨ê³„: ê°•í™”ëœ í‰ê°€ ({eval_episodes} ì—í”¼ì†Œë“œ)")
    results = evaluate_production_model(model, container_size, num_boxes, eval_episodes)
    env.close()
    if results is None:
        return False

    print("\n" + "="*60)
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {results['combined_score']:.3f}")
    print(f"ğŸ¯ ëª©í‘œ ëŒ€ë¹„: {(results['combined_score']/20.591*100):.1f}% (ëª©í‘œ: 20.591)")
    print(f"ğŸ’° í‰ê·  ë³´ìƒ: {results['mean_reward']:.3f} Â± {results['std_reward']:.3f}")
    print(f"ğŸ“¦ í‰ê·  í™œìš©ë¥ : {results['mean_utilization']:.1%} Â± {results['std_utilization']:.1%}")
    print(f"ğŸ² í‰ê·  ë°°ì¹˜: {results['mean_placement']:.1f}ê°œ (ìµœëŒ€: {results['max_placement']}ê°œ)")
    print(f"âœ… ì„±ê³µë¥ : {results['success_rate']:.1%}")
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time/60:.1f}ë¶„")

    if results['combined_score'] >= 20.0:
        print(f"ğŸ‰ ìš°ìˆ˜! ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± ë˜ëŠ” ê·¼ì ‘")
    elif results['combined_score'] >= 18.57:
        print(f"âœ… ì„±ê³µ! Phase 3 ëª©í‘œ ë‹¬ì„±")
    else:
        print(f"ğŸ“ˆ ê°œì„  í•„ìš”: ì¶”ê°€ íŠœë‹ ê¶Œì¥")

    final_results = {
        'timestamp': timestamp,
        'test_type': 'production_final',
        'params': PRODUCTION_OPTIMAL,
        'config': {
            'container_size': container_size,
            'num_boxes': num_boxes,
            'timesteps': timesteps,
            'eval_episodes': eval_episodes
        },
        'performance': results,
        'training_time_minutes': train_time/60,
        'model_path': model_path,
        'target_score': 20.591,
        'achievement_rate': results['combined_score']/20.591*100
    }
    os.makedirs('results', exist_ok=True)
    results_file = f"results/production_final_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, default=str)
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")

    return results['combined_score'] >= 18.57


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸: ì™„ì „ í…ŒìŠ¤íŠ¸/ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›."""
    import argparse

    parser = argparse.ArgumentParser(description='í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… í…ŒìŠ¤íŠ¸')
    parser.add_argument('--timesteps', type=int, default=50000, help='í•™ìŠµ ìŠ¤í… ìˆ˜')
    parser.add_argument('--episodes', type=int, default=50, help='í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (25000 ìŠ¤í…)')
    args = parser.parse_args()

    if args.quick:
        timesteps = 25000
        episodes = 30
        print("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    else:
        timesteps = args.timesteps
        episodes = args.episodes
        print("ğŸ† ì™„ì „ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")

    print(f"ğŸš€ ì„¤ì •: {timesteps:,} ìŠ¤í…, {episodes} ì—í”¼ì†Œë“œ")

    start_time = time.time()
    success = production_final_test(timesteps, episodes)
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    print("ğŸ‰ í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ê³µ!" if success else "ğŸ“ˆ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()



```

```python
#!/usr/bin/env python3
"""
3D Bin Packing ê°•í™”í•™ìŠµ ìµœì í™”(Phase 4) ì£¼ì„ í™•ì¥íŒ

ê°œìš”
- ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” Maskable PPOë¥¼ í™œìš©í•´ 3D ë¹ˆíŒ¨í‚¹ í™˜ê²½(`src/packing_env.py`)ì—ì„œ
  ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° êµ¬ì„±ì˜ ì„±ëŠ¥ì„ ë¹„êµÂ·í‰ê°€í•©ë‹ˆë‹¤.
- ê´€ì°°(Observation)ì€ ì»¨í…Œì´ë„ˆ ìƒë‹¨ ë†’ì´ë§µê³¼ ê°€ì‹œ ë°•ìŠ¤ì˜ í¬ê¸° ë²¡í„°ì´ë©°,
  í–‰ë™(Action)ì€ [ê°€ì‹œ ë°•ìŠ¤ ì„ íƒ Ã— XY ì¢Œí‘œ]ì˜ ë‹¨ì¼ ì •ìˆ˜ indexë¡œ ë””ìŠ¤í¬ë¦¬íŠ¸í™”ë©ë‹ˆë‹¤.
- ë…¼ë¬¸ ë§¥ë½: ì²¨ë¶€ ë¬¸í—Œ(Transformer ê¸°ë°˜ DRL)ê³¼ ë‹¬ë¦¬ ë³¸ ì½”ë“œëŠ” MLP ê¸°ë°˜ MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  ê³µí†µì ìœ¼ë¡œ ë†’ì´ë§µ ê¸°ë°˜ ìƒíƒœí‘œí˜„, ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹, ë³´ìƒ ì‰ì´í•‘ì„ í†µí•´ íƒìƒ‰ ê³µê°„ì„ ì¤„ì´ê³  ì•ˆì •ì  í•™ìŠµì„ ì§€í–¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•(ì˜ˆ)
- ëª¨ë“  ì„¸íŠ¸ ì‹¤í—˜:  python enhanced_optimization_annotated.py --focus all --timesteps 35000
- ì•ˆì •ì„± ì¤‘ì‹¬:     python enhanced_optimization_annotated.py --focus stability
- ê²°ê³¼ ë¶„ì„ë§Œ:      python enhanced_optimization_annotated.py --analyze results/phase4_enhanced_*.json

ì¶œë ¥
- `results/phase4_enhanced_{focus}_{timestamp}.json` ì¢…í•© ê²°ê³¼ì™€ `_analysis.png` ì°¨íŠ¸ë¥¼ ìë™ ì €ì¥
"""

import os
import sys
import json
import time
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
import warnings

# í™˜ê²½ ì„¤ì •: ì„œë²„/í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")

# src í´ë”ë¥¼ import ê²½ë¡œì— ì¶”ê°€
sys.path.append('src')

import gymnasium as gym
from gymnasium.envs.registration import register
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
import matplotlib.pyplot as plt

# í”„ë¡œì íŠ¸ í•µì‹¬ ëª¨ë“ˆ ë¡œë“œ
try:
    from packing_env import PackingEnv  # ìƒíƒœÂ·í–‰ë™ ì •ì˜ëœ Gym í™˜ê²½
    from train_maskable_ppo import ImprovedRewardWrapper  # ê°œì„ í˜• ë³´ìƒ ë˜í¼
    from utils import boxes_generator  # ë¬¸ì œ ì¸ìŠ¤í„´ìŠ¤(ë°•ìŠ¤ ì‹œí€€ìŠ¤) ìƒì„±ê¸°
    print("âœ… ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ Import ì˜¤ë¥˜: {e}")
    print("src í´ë”ì™€ í•„ìš”í•œ ëª¨ë“ˆë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)


def get_env_info(env):
    """ë˜í¼ë¥¼ ë²—ê²¨ ì‹¤ì œ `PackingEnv`ì— ì ‘ê·¼í•´ ì»¨í…Œì´ë„ˆ í¬ê¸°/ë°•ìŠ¤ ê°œìˆ˜ ìš”ì•½.

    ë°˜í™˜ê°’
    - (container_size, box_count)
    """
    try:
        unwrapped_env = env
        while hasattr(unwrapped_env, 'env'):
            unwrapped_env = unwrapped_env.env

        if hasattr(unwrapped_env, 'container') and hasattr(unwrapped_env.container, 'size'):
            container_size = unwrapped_env.container.size
        else:
            container_size = [10, 10, 10]

        if hasattr(unwrapped_env, 'initial_boxes'):
            box_count = len(unwrapped_env.initial_boxes)
        elif hasattr(unwrapped_env, 'num_initial_boxes'):
            box_count = unwrapped_env.num_initial_boxes
        else:
            box_count = 12

        return container_size, box_count
    except Exception as e:
        print(f"âš ï¸ í™˜ê²½ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return [10, 10, 10], 12


def calculate_utilization_and_items(env):
    """ìµœì¢… ì‹œì  ì»¨í…Œì´ë„ˆ í™œìš©ë¥ ê³¼ ë°°ì¹˜ëœ ë°•ìŠ¤ ê°œìˆ˜ ì‚°ì¶œ.

    - í™œìš©ë¥  = (ë°°ì¹˜ëœ ë°•ìŠ¤ ì´ë¶€í”¼) / (ì»¨í…Œì´ë„ˆ ë¶€í”¼)
    - ë°°ì¹˜ íŒì •: positionì´ [-1,-1,-1]ì´ ì•„ë‹Œ ë°•ìŠ¤
    """
    try:
        unwrapped_env = env
        while hasattr(unwrapped_env, 'env'):
            unwrapped_env = unwrapped_env.env

        if hasattr(unwrapped_env, 'container'):
            placed_volume = 0
            placed_count = 0
            for box in unwrapped_env.container.boxes:
                if hasattr(box, 'position') and box.position is not None:
                    if not (box.position[0] == -1 and box.position[1] == -1 and box.position[2] == -1):
                        placed_volume += box.volume
                        placed_count += 1

            container_volume = unwrapped_env.container.volume
            utilization = placed_volume / container_volume if container_volume > 0 else 0.0
            return utilization, placed_count
        else:
            return 0.0, 0
    except Exception as e:
        print(f"âš ï¸ í™œìš©ë¥  ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0, 0


class EnhancedOptimizer:
    """Phase 4 ì •ë°€ ìµœì í™” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.

    - ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ìˆœíšŒ í•™ìŠµ/í‰ê°€í•˜ì—¬ ì¢…í•© ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
    - ì ìˆ˜ = 0.3Ã—í‰ê·  ë³´ìƒ + 0.7Ã—(í‰ê·  í™œìš©ë¥ [%])
    """

    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)

        # Phase 3 ìµœê³  ì„±ëŠ¥ ê¸°ì¤€ì  (ë¹„êµÂ·ê°œì„ ìœ¨ ê³„ì‚°ì— ì‚¬ìš©)
        self.phase3_best = {
            'score': 16.116,
            'params': {
                'learning_rate': 0.00015,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [256, 128, 64]
            }
        }

        self.target_score = 18.57
        self.improvement_needed = (self.target_score - self.phase3_best['score']) / self.phase3_best['score']

        print(f"ğŸ¯ Phase 4 Enhanced Optimization ì‹œì‘")
        print(f"ğŸ“Š ê¸°ì¤€ì : {self.phase3_best['score']:.3f}ì ")
        print(f"ğŸ† ëª©í‘œ: {self.target_score}ì  ({self.improvement_needed:.1%} ê°œì„  í•„ìš”)")

    def create_enhanced_environment(self, num_boxes: int = 12, container_size: Optional[List[int]] = None,
                                    enhanced_reward: bool = True, seed: int = 42) -> gym.Env:
        """ì‹¤í—˜ìš© í™˜ê²½ ìƒì„±.

        - `utils.boxes_generator`ë¡œ ê³ ì • ì»¨í…Œì´ë„ˆì— ìƒì‘í•˜ëŠ” ë°•ìŠ¤ ì‹œí€€ìŠ¤ ìƒì„±
        - `ImprovedRewardWrapper` ë˜ëŠ” ë³¸ íŒŒì¼ì˜ `EnhancedRewardWrapper` ì ìš©
        - `ActionMasker`ë¡œ ë¶ˆê°€ëŠ¥ í–‰ë™ì„ ë§ˆìŠ¤í‚¹í•˜ì—¬ ìƒ˜í”Œ íš¨ìœ¨ ê°œì„ 
        """
        try:
            if container_size is None:
                container_size = [10, 10, 10]
            print(f"ìƒì„±ëœ ë°•ìŠ¤ ê°œìˆ˜: {num_boxes}")
            print(f"ì»¨í…Œì´ë„ˆ í¬ê¸°: {container_size}")

            # Gym í™˜ê²½ ë“±ë¡ (ì¤‘ë³µ ë“±ë¡ ë°©ì§€)
            if 'PackingEnv-v0' not in gym.envs.registry:
                register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')

            # ë°•ìŠ¤ ì‹œí€€ìŠ¤ ìƒì„±
            box_sizes = boxes_generator(container_size, num_boxes, seed)

            # í™˜ê²½ ìƒì„±
            env = gym.make(
                "PackingEnv-v0",
                container_size=container_size,
                box_sizes=box_sizes,
                num_visible_boxes=min(3, num_boxes),
                render_mode=None,
                random_boxes=False,
                only_terminal_reward=False,
            )
            print("í™˜ê²½ ìƒì„± ì„±ê³µ: PackingEnv-v0")

            # ë³´ìƒ ë˜í¼ ì„ íƒ ì ìš©
            if enhanced_reward:
                env = EnhancedRewardWrapper(env)
                print("ê°•í™”ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")
            else:
                env = ImprovedRewardWrapper(env)
                print("ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")

            # ì•¡ì…˜ ë§ˆìŠ¤í‚¹ (ë¶ˆê°€ëŠ¥ ì¢Œí‘œ/ë°•ìŠ¤ ì„ íƒ ì œê±°)
            def get_action_masks(env):
                try:
                    unwrapped_env = env
                    while hasattr(unwrapped_env, 'env'):
                        unwrapped_env = unwrapped_env.env

                    if hasattr(unwrapped_env, 'action_masks'):
                        masks = unwrapped_env.action_masks()
                        if isinstance(masks, list):
                            return np.array(masks, dtype=bool)
                        return masks
                    return np.ones(env.action_space.n, dtype=bool)
                except Exception as e:
                    print(f"âš ï¸ ì•¡ì…˜ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
                    return np.ones(env.action_space.n, dtype=bool)

            env = ActionMasker(env, get_action_masks)
            print("ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ë˜í¼ ì ìš©ë¨")

            print(f"ì‹œë“œ ì„¤ì • ì™„ë£Œ: {seed}")
            return env

        except Exception as e:
            print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
            raise e

    def get_enhanced_parameter_sets(self) -> Dict[str, Dict]:
        """íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸íŠ¸ ì‚¬ì „.

        - ì•ˆì •ì„± ê°•í™”, ì•„í‚¤í…ì²˜ ë³€í™”, ì ê·¹/ë³´ìˆ˜ ìµœì í™” ë“± ì¹´í…Œê³ ë¦¬ë¡œ êµ¬ì„±
        - ê° í•­ëª©ì€ MaskablePPOì˜ ì£¼ìš” ì¸ìì™€ `policy_kwargs.net_arch`ë¥¼ í¬í•¨
        """
        # 1) í•™ìŠµ ì•ˆì •ì„± ê°•í™” ì„¸íŠ¸
        stability_sets = {
            'stability_conservative': {
                'learning_rate': 1.2e-04,
                'n_steps': 1024,
                'batch_size': 64,
                'n_epochs': 6,
                'clip_range': 0.15,
                'ent_coef': 0.005,
                'vf_coef': 0.5,
                'gae_lambda': 0.98,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            },
            'stability_balanced': {
                'learning_rate': 1.3e-04,
                'n_steps': 768,
                'batch_size': 96,
                'n_epochs': 5,
                'clip_range': 0.18,
                'ent_coef': 0.008,
                'vf_coef': 0.5,
                'gae_lambda': 0.96,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            }
        }

        # 2) ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ ë³€í™” ì„¸íŠ¸ (í­/ê¹Šì´/ê· í˜•)
        architecture_sets = {
            'arch_wide': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[512, 256, 128], vf=[512, 256, 128])]
            },
            'arch_deep': {
                'learning_rate': 1.4e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[256, 256, 128, 64], vf=[256, 256, 128, 64])]
            },
            'arch_balanced': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[384, 192, 96], vf=[384, 192, 96])]
            },
            'arch_reinforced': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[256, 128, 128, 64], vf=[256, 128, 128, 64])]
            }
        }

        # 3) í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¹ë‹¨/ì •ë°€ ì„¸íŠ¸
        optimized_sets = {
            'opt_precision': {
                'learning_rate': 1.1e-04,
                'n_steps': 1536,
                'batch_size': 192,
                'n_epochs': 8,
                'clip_range': 0.12,
                'ent_coef': 0.003,
                'vf_coef': 0.6,
                'gae_lambda': 0.99,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            },
            'opt_aggressive': {
                'learning_rate': 1.8e-04,
                'n_steps': 256,
                'batch_size': 64,
                'n_epochs': 3,
                'clip_range': 0.25,
                'ent_coef': 0.02,
                'vf_coef': 0.4,
                'gae_lambda': 0.92,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            }
        }

        all_sets = {}
        all_sets.update(stability_sets)
        all_sets.update(architecture_sets)
        all_sets.update(optimized_sets)
        return all_sets

    def train_and_evaluate(self, params: Dict, name: str, timesteps: int = 35000,
                           eval_episodes: int = 25, enhanced_reward: bool = True) -> Dict[str, Any]:
        """ë‹¨ì¼ êµ¬ì„± í•™ìŠµ ë° í‰ê°€ ë£¨í‹´.

        - MaskablePPOë¡œ timesteps ë§Œí¼ í•™ìŠµ í›„, ì—¬ëŸ¬ ì‹œë“œë¡œ ì§§ì€ ì—í”¼ì†Œë“œ í‰ê°€
        - ë…¼ë¬¸ ê´€ì : ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ íƒìƒ‰ ê³µê°„ì„ ì¶•ì†Œí•´ ìƒ˜í”Œ íš¨ìœ¨ê³¼ ì•ˆì •ì„±ì„ í–¥ìƒ
        """
        print(f"\nğŸ”§ {name} ìµœì í™” ì¤‘...")

        env = self.create_enhanced_environment(enhanced_reward=enhanced_reward, seed=42)
        container_size, box_count = get_env_info(env)
        print(f"âœ… í™˜ê²½ ìƒì„± ì„±ê³µ: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{box_count}ê°œ")

        model = MaskablePPO(
            'MultiInputPolicy',
            env,
            learning_rate=params['learning_rate'],
            n_steps=params['n_steps'],
            batch_size=params['batch_size'],
            n_epochs=params['n_epochs'],
            clip_range=params['clip_range'],
            ent_coef=params['ent_coef'],
            vf_coef=params['vf_coef'],
            gae_lambda=params['gae_lambda'],
            policy_kwargs={'net_arch': params['net_arch']},
            verbose=0,
            device='auto'
        )

        print(f"ğŸ“ {name} í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í… (LR: {params['learning_rate']:.2e}, Net: {params['net_arch']})")
        start_time = time.time()
        model.learn(total_timesteps=timesteps)
        training_time = time.time() - start_time
        print(f"â±ï¸ {name} í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")

        # í‰ê°€: ë™ì¼ í™˜ê²½ ì„¤ì •ì—ì„œ ì‹œë“œë§Œ ë‹¤ë¥´ê²Œ ë°˜ë³µ
        print(f"ğŸ” {name} í‰ê°€ ì‹œì‘ ({eval_episodes} ì—í”¼ì†Œë“œ, ìµœëŒ€ 25 ìŠ¤í…)")
        rewards, utilizations, placements = [], [], []
        for i in range(eval_episodes):
            eval_env = self.create_enhanced_environment(enhanced_reward=enhanced_reward, seed=100 + i * 5)
            container_size, box_count = get_env_info(eval_env)
            obs = eval_env.reset(seed=100 + i * 5)
            if isinstance(obs, tuple):
                obs = obs[0]

            episode_reward = 0
            step_count = 0
            max_steps = 25
            while step_count < max_steps:
                try:
                    # í‰ê°€ ì‹œì—ë„ í™•ë¥ ì  í–‰ë™ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´
                    action, _ = model.predict(obs, deterministic=False)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    step_count += 1
                    if terminated or truncated:
                        break
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    break

            final_utilization, placement_count = calculate_utilization_and_items(eval_env)
            rewards.append(episode_reward)
            utilizations.append(final_utilization)
            placements.append(placement_count)

            if i < 6 or i in [10, 15, 20] or i == eval_episodes - 1:
                print(f"   ì—í”¼ì†Œë“œ {i+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={final_utilization:.1%}, ë°•ìŠ¤={placement_count}ê°œ")
            eval_env.close()

        mean_reward = np.mean(rewards)
        std_reward = np.std(rewards)
        mean_utilization = np.mean(utilizations)
        std_utilization = np.std(utilizations)
        mean_placement = np.mean(placements)
        max_placement = np.max(placements)
        success_count = sum(1 for p in placements if p >= 5)
        success_rate = success_count / eval_episodes
        combined_score = mean_reward * 0.3 + mean_utilization * 100 * 0.7

        print(f"ğŸ“Š {name} ìµœì¢… ê²°ê³¼:")
        print(f"   í‰ê·  ë³´ìƒ: {mean_reward:.3f} Â± {std_reward:.3f}")
        print(f"   í‰ê·  í™œìš©ë¥ : {mean_utilization:.1%} Â± {std_utilization:.1%}")
        print(f"   í‰ê·  ë°°ì¹˜: {mean_placement:.1f}ê°œ (ìµœëŒ€: {max_placement}ê°œ)")
        print(f"   ì„±ê³µë¥ : {success_rate:.1%}")
        print(f"   ì¢…í•© ì ìˆ˜: {combined_score:.3f}")

        env.close()
        return {
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'mean_utilization': mean_utilization,
            'std_utilization': std_utilization,
            'mean_placement': mean_placement,
            'max_placement': max_placement,
            'success_rate': success_rate,
            'combined_score': combined_score,
            'episodes': eval_episodes,
            'training_time': training_time,
            'params': params
        }

    def run_phase4_optimization(self, focus: str = 'all', timesteps: int = 35000) -> Dict:
        """Phase 4 ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ ë° ê²°ê³¼ ì§‘ê³„/ì €ì¥."""
        print(f"\n{'='*60}")
        print(f"ğŸš€ Phase 4 Enhanced Optimization ì‹œì‘")
        print(f"ğŸ¯ í¬ì»¤ìŠ¤: {focus}")
        print(f"â±ï¸ í•™ìŠµ ìŠ¤í…: {timesteps:,}")
        print(f"{'='*60}")

        all_params = self.get_enhanced_parameter_sets()
        results: Dict[str, Any] = {}
        best_score = 0
        best_config = None

        # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
        if focus == 'stability':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('stability')}
        elif focus == 'architecture':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('arch')}
        elif focus == 'optimization':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('opt')}
        else:
            params_to_test = all_params

        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸í•  ì„¤ì •: {len(params_to_test)}ê°œ")

        total_start_time = time.time()
        for i, (name, params) in enumerate(params_to_test.items(), 1):
            print(f"\n[{i}/{len(params_to_test)}] {name} í…ŒìŠ¤íŠ¸ ì¤‘...")
            try:
                result = self.train_and_evaluate(
                    params, name, timesteps=timesteps, enhanced_reward=True
                )
                results[name] = result
                if result['combined_score'] > best_score:
                    best_score = result['combined_score']
                    best_config = name
                    print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥: {best_score:.3f}ì ")
            except Exception as e:
                print(f"âŒ {name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                continue

        total_time = time.time() - total_start_time

        if results:
            print(f"\n{'='*60}")
            print(f"ğŸ† Phase 4 ìµœì í™” ê²°ê³¼")
            print(f"{'='*60}")

            sorted_results = sorted(results.items(), key=lambda x: x[1]['combined_score'], reverse=True)
            print("ìˆœìœ„  ì„¤ì •ëª…                    ì ìˆ˜      ê°œì„ ìœ¨   í™œìš©ë¥    ì„±ê³µë¥ ")
            print("-" * 70)
            for rank, (name, result) in enumerate(sorted_results[:10], 1):
                improvement = (result['combined_score'] - self.phase3_best['score']) / self.phase3_best['score'] * 100
                print(f"{rank:2d}    {name:<22} {result['combined_score']:6.2f}   {improvement:+5.1f}%   "
                      f"{result['mean_utilization']:5.1%}   {result['success_rate']:5.1%}")

            # ìƒìœ„ 1ê°œ ê²°ê³¼ëŠ” ì¶œë ¥ìš©ìœ¼ë¡œë§Œ ì‚¬ìš© (ë¯¸ì‚¬ìš© ë³€ìˆ˜ ê²½ê³  ë°©ì§€ ìœ„í•´ ì§ì ‘ ì‚¬ìš© ìƒëµ)
            target_achievement = best_score / self.target_score * 100
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_score:.3f}ì  ({best_config})")
            print(f"ğŸ“ˆ ëª©í‘œ ë‹¬ì„±ë„: {target_achievement:.1f}% (ëª©í‘œ {self.target_score} ëŒ€ë¹„)")
            if best_score >= self.target_score:
                print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„± ì„±ê³µ!")
            else:
                remaining = self.target_score - best_score
                print(f"ğŸ“Š ëª©í‘œê¹Œì§€ {remaining:.3f}ì  ë¶€ì¡±")

            # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ numpy -> ê¸°ë³¸íƒ€ì… ë³€í™˜ê¸°
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {key: convert_numpy_types(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj

            converted_results = convert_numpy_types(results)
            output_data = {
                'timestamp': self.timestamp,
                'phase': 'phase4_enhanced_optimization',
                'focus': focus,
                'timesteps': int(timesteps),
                'target_score': float(self.target_score),
                'phase3_baseline': float(self.phase3_best['score']),
                'best_score': float(best_score),
                'best_config': best_config,
                'target_achievement': float(target_achievement),
                'total_time_minutes': float(total_time / 60),
                'results': converted_results
            }

            output_file = os.path.join(self.results_dir, f'phase4_enhanced_{focus}_{self.timestamp}.json')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Phase 4 ê²°ê³¼: {output_file}")
            print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
            return output_data
        else:
            print("âŒ ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

    def create_performance_analysis(self, results_file: str):
        """ê²°ê³¼ JSONì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„± ë° ì €ì¥."""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if not data.get('results'):
                print("ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            configs, scores, utilizations, success_rates = [], [], [], []
            for name, result in data['results'].items():
                configs.append(name)
                scores.append(result['combined_score'])
                utilizations.append(result['mean_utilization'] * 100)
                success_rates.append(result['success_rate'] * 100)

            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Phase 4 Enhanced Optimization Analysis\n'
                         f'Best: {data["best_score"]:.2f} (Target: {data["target_score"]})',
                         fontsize=16, fontweight='bold')

            # 1) ì¢…í•© ì ìˆ˜ ë°”ì°¨íŠ¸
            axes[0,0].bar(range(len(configs)), scores, color='skyblue', alpha=0.7)
            axes[0,0].axhline(y=data['target_score'], color='red', linestyle='--', label=f'Target: {data["target_score"]}')
            axes[0,0].axhline(y=data['phase3_baseline'], color='orange', linestyle='--', label=f'Phase3: {data["phase3_baseline"]:.2f}')
            axes[0,0].set_title('Combined Scores')
            axes[0,0].set_ylabel('Score')
            axes[0,0].legend()
            axes[0,0].tick_params(axis='x', rotation=45)

            # 2) í™œìš©ë¥ 
            axes[0,1].bar(range(len(configs)), utilizations, color='lightgreen', alpha=0.7)
            axes[0,1].set_title('Space Utilization (%)')
            axes[0,1].set_ylabel('Utilization %')
            axes[0,1].tick_params(axis='x', rotation=45)

            # 3) ì„±ê³µë¥ 
            axes[1,0].bar(range(len(configs)), success_rates, color='lightcoral', alpha=0.7)
            axes[1,0].set_title('Success Rate (%)')
            axes[1,0].set_ylabel('Success Rate %')
            axes[1,0].tick_params(axis='x', rotation=45)

            # 4) ìƒê´€ê´€ê³„ ì‚°ì ë„ (í™œìš©ë¥  vs ì ìˆ˜)
            axes[1,1].scatter(utilizations, scores, alpha=0.7, s=100)
            axes[1,1].set_xlabel('Utilization %')
            axes[1,1].set_ylabel('Combined Score')
            axes[1,1].set_title('Utilization vs Score')

            for ax in axes.flat:
                if hasattr(ax, 'set_xticks'):
                    ax.set_xticks(range(len(configs)))
                    ax.set_xticklabels([c[:15] + '...' if len(c) > 15 else c for c in configs], rotation=45, ha='right')

            plt.tight_layout()
            chart_file = results_file.replace('.json', '_analysis.png')
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ë¶„ì„ ì°¨íŠ¸ ì €ì¥: {chart_file}")
            plt.close()
        except Exception as e:
            print(f"âŒ ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")


class EnhancedRewardWrapper(gym.RewardWrapper):
    """ê°•í™”ëœ ë³´ìƒ ë˜í¼(ê°„ë‹¨í˜•).

    - ì»¨í…Œì´ë„ˆ í™œìš©ë¥ , ì—°ì† ë°°ì¹˜, ì„ê³„ê°’ ëŒíŒŒ ë“±ì˜ ë³´ë„ˆìŠ¤ë¥¼ ì¶”ê°€í•´ í•™ìŠµì„ ê°€ì†
    - ë…¼ë¬¸ì  ê´€ì ì—ì„œì˜ Reward Shaping: íƒìƒ‰ì„ ìœ ë„í•˜ë˜ ê³¼ë„í•œ í¸í–¥ ë°©ì§€
    """

    def __init__(self, env):
        super().__init__(env)
        self.previous_utilization = 0.0
        self.consecutive_placements = 0

    def reset(self, **kwargs):
        self.previous_utilization = 0.0
        self.consecutive_placements = 0
        return self.env.reset(**kwargs)

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        enhanced_reward = self.reward(reward)
        return obs, enhanced_reward, terminated, truncated, info

    def reward(self, reward):
        current_utilization, placement_count = calculate_utilization_and_items(self.env)

        enhanced_reward = reward
        if current_utilization > 0:
            enhanced_reward += (current_utilization ** 1.5) * 3.0
        if current_utilization > self.previous_utilization:
            enhanced_reward += (current_utilization - self.previous_utilization) * 5.0
        if placement_count > 0:
            self.consecutive_placements += 1
            enhanced_reward += min(self.consecutive_placements * 0.1, 1.0)
        else:
            self.consecutive_placements = 0
        if current_utilization > 0.25:
            enhanced_reward += 2.0
        elif current_utilization > 0.20:
            enhanced_reward += 1.0
        if placement_count >= 5:
            enhanced_reward += (placement_count - 4) * 0.5
        if placement_count > 0:
            efficiency = current_utilization / placement_count
            enhanced_reward += efficiency * 2.0

        self.previous_utilization = current_utilization
        return enhanced_reward


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸: ì‹¤í—˜ ì‹¤í–‰ ë˜ëŠ” ë¶„ì„ ëª¨ë“œ."""
    import argparse

    parser = argparse.ArgumentParser(description='Enhanced Optimization for 3D Bin Packing')
    parser.add_argument('--focus', choices=['all', 'stability', 'architecture', 'optimization'],
                        default='all', help='Optimization focus area')
    parser.add_argument('--timesteps', type=int, default=35000, help='Training timesteps')
    parser.add_argument('--analyze', type=str, help='Analyze results from JSON file')

    args = parser.parse_args()
    optimizer = EnhancedOptimizer()

    if args.analyze:
        optimizer.create_performance_analysis(args.analyze)
    else:
        result = optimizer.run_phase4_optimization(focus=args.focus, timesteps=args.timesteps)
        if result and result.get('results'):
            output_file = os.path.join(optimizer.results_dir,
                                       f'phase4_enhanced_{args.focus}_{optimizer.timestamp}.json')
            optimizer.create_performance_analysis(output_file)

            best_score = result['best_score']
            target_score = result['target_score']
            if best_score >= target_score:
                print(f"\nğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ëª©í‘œ {target_score}ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
                print(f"ğŸ† ìµœì¢… ì„±ëŠ¥: {best_score:.3f}ì ")
            else:
                remaining = target_score - best_score
                print(f"\nğŸ“Š ì¶”ê°€ ê°œì„  ê¶Œì¥ì‚¬í•­:")
                print(f"   ëª©í‘œê¹Œì§€ {remaining:.3f}ì  ë¶€ì¡±")
                if remaining > 1.0:
                    print(f"   â¡ï¸ í•™ìŠµ ì‹œê°„ì„ 50,000ìŠ¤í…ìœ¼ë¡œ ì¦ê°€ ì‹œë„")
                    print(f"   â¡ï¸ ì•™ìƒë¸” ëª¨ë¸ë§ ì‹œë„")
                else:
                    print(f"   â¡ï¸ ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ ë‹¬ì„± ê°€ëŠ¥")
                    print(f"   â¡ï¸ ë³´ìƒ í•¨ìˆ˜ ì¶”ê°€ ìµœì í™” ê¶Œì¥")


if __name__ == "__main__":
    main()



```
### ì½”ë“œë² ì´ìŠ¤ êµ¬ì¡°ì™€ ë™ì‘ ê°œìš”

- í•µì‹¬ ëª©ì 
  - 3D Bin Packing í™˜ê²½ì—ì„œ ê°•í™”í•™ìŠµ(Maskable PPO)ë¡œ ì»¨í…Œì´ë„ˆ í™œìš©ë¥ ì„ ìµœëŒ€í™”.
  - ìƒíƒœëŠ” ì»¨í…Œì´ë„ˆ ë†’ì´ë§µ + ê°€ì‹œ ë°•ìŠ¤ í¬ê¸°, í–‰ë™ì€ [ê°€ì‹œ ë°•ìŠ¤ ì„ íƒ Ã— XYì¢Œí‘œ]ì˜ ë‹¨ì¼ ì •ìˆ˜ ì¸ë±ìŠ¤.

- ìµœìƒìœ„
  - `enhanced_optimization.py`: Phase 4 í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰/ë¹„êµ ìë™í™”.
  - `production_final_test.py`: ìµœì  ì„¤ì •(Phase 4 ê²°ê³¼)ìœ¼ë¡œ í•™ìŠµâ†’í‰ê°€â†’ê²°ê³¼ì €ì¥.
  - `README.md`: ë¬¸ì œì •ì˜, ì„¤ì¹˜, í™˜ê²½ ì†Œê°œ.
  - `results/`, `models/`, `logs/`, `gifs/`: ì‹¤í–‰ ì‚°ì¶œë¬¼ ì €ì¥.

- src ëª¨ë“ˆ
  - `packing_env.py`
    - Gymnasium í™˜ê²½ `PackingEnv`: ê´€ì°°ê³µê°„ Dict(height_map, visible_box_sizes), í–‰ë™ê³µê°„ Discrete(lx*ly*visible).
    - `action_masks()`: ë¶ˆê°€ëŠ¥ ì¢Œí‘œ/ë°•ìŠ¤ ì¡°í•©ì„ ë°°ì œí•˜ëŠ” ë§ˆìŠ¤í¬ ì œê³µ.
    - `calculate_reward(...)`: ì¢…ë£Œ/ì¤‘ê°„ ë³´ìƒ(í™œìš©ë¥  ê¸°ë°˜).
  - `train_maskable_ppo.py`
    - `make_env(...)`: í™˜ê²½ ë“±ë¡/ìƒì„±, `ActionMasker` ë° ê°œì„ í˜• ë³´ìƒ(`ImprovedRewardWrapper`) ì ìš©.
    - ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ/ëª¨ë‹ˆí„°ë§ ì½œë°±, í•™ìŠµ/í‰ê°€ ìœ í‹¸ í¬í•¨.
  - `utils.py`
    - `boxes_generator(...)`: ì»¨í…Œì´ë„ˆ í¬ê¸°ì— ë§ê²Œ ë°•ìŠ¤ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±(ë¶„í•  ê¸°ë°˜).
  - `packing_kernel.py`
    - `Container`, `Box` ë“± ì—”ì§„ ë ˆë²¨ ë¡œì§(ë°°ì¹˜, ë†’ì´ë§µ, ìœ íš¨ì„± ê²€ì‚¬ ë“±).
  - ê¸°íƒ€: `device_utils.py`, `train.py`, `ultimate_train_fix.py`, `vectorized_training.py`, `agents.py` ë“± ë³´ì¡°/ëŒ€ì•ˆ í•™ìŠµ ë£¨í‹´.

- ì „ì²´ ë™ì‘ íë¦„
  1) `utils.boxes_generator`ë¡œ ë¬¸ì œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± â†’ 2) `PackingEnv`ë¡œ Gym í™˜ê²½ êµ¬ì„± â†’ 3) `ActionMasker`ë¡œ ë¶ˆê°€ëŠ¥ í–‰ë™ ì œê±° â†’ 4) ë³´ìƒ ë˜í¼(ê°œì„ í˜•/ê°•í™”í˜•)ë¡œ ë³´ìƒ ì‰ì´í•‘ â†’ 5) `MaskablePPO` í•™ìŠµ â†’ 6) ë‹¤ì¤‘ ì—í”¼ì†Œë“œ í‰ê°€ ë° `results/` ì €ì¥ â†’ 7) ë¶„ì„ ì°¨íŠ¸/ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±.

- ì²¨ë¶€ ë…¼ë¬¸ê³¼ì˜ ì—°ê²°
  - ë³¸ ì½”ë“œëŠ” Transformer ì •ì±… ëŒ€ì‹  MLP ì •ì±… + MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  - ê³µí†µì : ë†’ì´ë§µ ê¸°ë°˜ ìƒíƒœí‘œí˜„, ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ íƒìƒ‰ ê³µê°„ ì¶•ì†Œ, í™œìš©ë¥  ì¤‘ì‹¬ ë³´ìƒ ì„¤ê³„, ë©€í‹° ì‹œë“œ ë‹¤ì¤‘ ì—í”¼ì†Œë“œ í‰ê°€.
  - ì°¨ì´ì : ë…¼ë¬¸ì€ Transformer ê¸°ë°˜ ì •ì±…/ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì„ í™œìš©í•˜ëŠ” ë°˜ë©´, ë³¸ ì½”ë“œëŠ” MLP `net_arch`ë¡œ ì •ì±…/ê°€ì¹˜ë§ì„ êµ¬ì„±.

### ì£¼ì„ í™•ì¥ íŒŒì¼ ìƒì„±ë¨

- ìƒì„± íŒŒì¼
  - `enhanced_optimization_annotated.py`
  - `production_final_test_annotated.py`

- ì£¼ìš” ì£¼ì„/ê°œì„  í¬ì¸íŠ¸
  - ìƒë‹¨ì— ê°œìš”/ì‚¬ìš©ë²•/ì¶œë ¥ë¬¼ ì •ë¦¬.
  - í™˜ê²½ ìƒì„±Â·ë³´ìƒÂ·ë§ˆìŠ¤í‚¹Â·í‰ê°€ ë£¨í‹´ì— ë‹¨ê³„ë³„ í•œêµ­ì–´ ì„¤ëª….
  - ê¸°ë³¸ ì¸ìì—ì„œ ê°€ë³€ ê°ì²´ ì‚¬ìš© ì§€ì–‘(None ì²˜ë¦¬) ë“± ì•ˆì „ì„± ê°œì„ .
  - ê²°ê³¼ ì €ì¥ ì‹œ UTF-8 ì¸ì½”ë”© ì§€ì •.

- ì‹¤í–‰ ë°©ë²•
  - ìµœì í™” íƒìƒ‰:
    - ì „ì²´: `python enhanced_optimization_annotated.py --focus all --timesteps 35000`
    - ì•ˆì •ì„±/ì•„í‚¤í…ì²˜/ìµœì í™”ë§Œ: `--focus stability|architecture|optimization`
    - ê²°ê³¼ ë¶„ì„: `--analyze results/phase4_enhanced_*.json`
  - í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸:
    - ì™„ì „: `python production_final_test_annotated.py --timesteps 50000 --episodes 50`
    - ë¹ ë¥¸: `python production_final_test_annotated.py --quick`
  - ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•´ì•¼ `sys.path.append('src')`ê°€ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•©ë‹ˆë‹¤.

- ë¦°íŠ¸ ì°¸ê³ 
  - ë™ì  ê²½ë¡œ(`sys.path.append('src')`)ë¡œ ì¸í•´ ì •ì  ë¶„ì„ê¸°ì—ì„œ `train_maskable_ppo` ì„í¬íŠ¸ ê²½ê³ ê°€ ë‚¨ì„ ìˆ˜ ìˆìœ¼ë‚˜ ëŸ°íƒ€ì„ì—ëŠ” ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤.
  - í•„ìš” ì‹œ IDEì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì„¤ì •í•˜ê±°ë‚˜, ì‹¤í–‰ì€ í•­ìƒ ì €ì¥ì†Œ ë£¨íŠ¸ì—ì„œ í•´ì£¼ì„¸ìš”.

- íŒŒì¼ ìœ„ì¹˜
  - ë£¨íŠ¸: `C:\Users\ë°•ì •ìš°\Documents\RL-3DbinPacking\enhanced_optimization_annotated.py`
  - ë£¨íŠ¸: `C:\Users\ë°•ì •ìš°\Documents\RL-3DbinPacking\production_final_test_annotated.py`

- ë¹ ë¥¸ ì²´í¬
  - ê²°ê³¼ë¬¼ì€ `results/phase4_enhanced_...json/png`, `results/production_final_...json`ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
  - ëª¨ë¸ì€ `models/production_optimal_...` ë“±ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.

- ìš”ì•½
  - ì „ì²´ êµ¬ì¡°ì™€ ê°•í™”í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ê°„ê²°íˆ ì •ë¦¬í–ˆê³ , ë‘ ì—”íŠ¸ë¦¬ ìŠ¤í¬ë¦½íŠ¸ì˜ â€œì£¼ì„ í™•ì¥íŒâ€ì„ ì¶”ê°€ë¡œ ì œê³µí–ˆìŠµë‹ˆë‹¤.
  - ì‹¤í–‰ ì•ˆë‚´ì™€ ë¶„ì„/ì €ì¥ ì‚°ì¶œë¬¼ ê²½ë¡œê¹Œì§€ í¬í•¨í•´, ë…¼ë¬¸ ë¬¸ë§¥ê³¼ì˜ ì—°ê²°ì ë„ ëª…í™•íˆ í‘œê¸°í–ˆìŠµë‹ˆë‹¤.

---

**User**

@RL-3DbinPacking/ ì²¨ë¶€ì˜ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ì—¬ ì´ ì½”ë“œë² ì´ìŠ¤ ì „ì²´ì˜ êµ¬ì¡°ì™€ ë™ì‘ì„ ì•Œê¸° ì‰½ê²Œ ì„¤ëª…í•œ ë‹¤ìŒì—, ìµœìƒìœ„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ì¸ enhanced_optimization.pyì™€ production_final_test.pyì— ì£¼ì„ì„ ë‹¬ì•„ì„œ ê°ê° ë³„ë„ì˜ íŒŒì¼ë¡œ ë§Œë“¤ì–´ì¤˜(Think hard!) 

---

**Cursor**

ì•ì„œ ìš”ì²­í•˜ì‹  ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤! 3D Bin Packing ê°•í™”í•™ìŠµ ì½”ë“œë² ì´ìŠ¤ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³ , ë‘ ê°œì˜ ì£¼ì„ í™•ì¥ íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.

## ğŸ—ï¸ ì½”ë“œë² ì´ìŠ¤ ì „ì²´ êµ¬ì¡° ë° ë™ì‘ ê°œìš”

### ğŸ“‹ í•µì‹¬ ëª©ì 
- **3D Bin Packing ë¬¸ì œ**ë¥¼ **Maskable PPO ê°•í™”í•™ìŠµ**ìœ¼ë¡œ í•´ê²°
- ì»¨í…Œì´ë„ˆ ê³µê°„ í™œìš©ë¥  ìµœëŒ€í™”ë¥¼ ëª©í‘œë¡œ ë°•ìŠ¤ ë°°ì¹˜ ìµœì í™”
- ë¶ˆê°€ëŠ¥í•œ í–‰ë™ì„ ì‚¬ì „ ë§ˆìŠ¤í‚¹í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„± í–¥ìƒ

### ğŸ”„ ì „ì²´ ë™ì‘ íë¦„
```
ë°•ìŠ¤ ì‹œí€€ìŠ¤ ìƒì„± â†’ Gym í™˜ê²½ êµ¬ì„± â†’ í–‰ë™ ë§ˆìŠ¤í‚¹ â†’ ë³´ìƒ ì‰ì´í•‘ â†’ PPO í•™ìŠµ â†’ ë‹¤ì¤‘ í‰ê°€ â†’ ê²°ê³¼ ë¶„ì„
```

### ğŸ“ ì£¼ìš” ë””ë ‰í† ë¦¬ êµ¬ì¡°

**ìµœìƒìœ„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸:**
- `enhanced_optimization.py` - Phase 4 í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ìë™í™”
- `production_final_test.py` - ìµœì  ì„¤ì •ìœ¼ë¡œ ìµœì¢… ê²€ì¦

**í•µì‹¬ ëª¨ë“ˆ (src/):**
- `packing_env.py` - Gymnasium í™˜ê²½ (ìƒíƒœ: ë†’ì´ë§µ+ê°€ì‹œë°•ìŠ¤, í–‰ë™: ìœ„ì¹˜ì„ íƒ)
- `train_maskable_ppo.py` - í•™ìŠµ íŒŒì´í”„ë¼ì¸ + ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- `utils.py` - ë°•ìŠ¤ ì‹œí€€ìŠ¤ ìƒì„±ê¸° (ë¶„í•  ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜)
- `packing_kernel.py` - ì»¨í…Œì´ë„ˆ/ë°•ìŠ¤ ì—”ì§„ ë¡œì§

**ê²°ê³¼ë¬¼ ì €ì¥:**
- `results/` - JSON ê²°ê³¼, PNG ì°¨íŠ¸
- `models/` - í•™ìŠµëœ ëª¨ë¸ ì €ì¥
- `logs/` - í•™ìŠµ ë¡œê·¸, TensorBoard ë°ì´í„°

### ğŸ§  ê°•í™”í•™ìŠµ ì„¤ê³„

**ìƒíƒœ í‘œí˜„ (State):**
- `height_map`: ì»¨í…Œì´ë„ˆ ìƒë‹¨ë·° ë†’ì´ë§µ (2D ë°°ì—´)
- `visible_box_sizes`: í˜„ì¬ ê°€ì‹œ ë°•ìŠ¤ë“¤ì˜ í¬ê¸° (3D ë²¡í„°ë“¤)

**í–‰ë™ ê³µê°„ (Action):**
- ë‹¨ì¼ ì •ìˆ˜ ì¸ë±ìŠ¤: `[ê°€ì‹œë°•ìŠ¤ ì„ íƒ] Ã— [XY ì¢Œí‘œ]`
- ì˜ˆ: ì•¡ì…˜ 42 â†’ ë°•ìŠ¤ 1ë²ˆì„ (4,2) ìœ„ì¹˜ì— ë°°ì¹˜

**ë³´ìƒ ì„¤ê³„:**
- ê¸°ë³¸: ì»¨í…Œì´ë„ˆ í™œìš©ë¥  (ë°°ì¹˜ëœ ë°•ìŠ¤ ë¶€í”¼ / ì „ì²´ ë¶€í”¼)
- ê°•í™”: ì—°ì† ë°°ì¹˜ ë³´ë„ˆìŠ¤, ì„ê³„ê°’ ëŒíŒŒ ë³´ë„ˆìŠ¤, íš¨ìœ¨ì„± ë³´ìƒ

### ğŸ“Š ë…¼ë¬¸ê³¼ì˜ ì—°ê²°ì 

**ê³µí†µì  (ì²¨ë¶€ ë…¼ë¬¸ - Transformer ê¸°ë°˜ DRL):**
- ë†’ì´ë§µ ê¸°ë°˜ ìƒíƒœ í‘œí˜„
- ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ íƒìƒ‰ ê³µê°„ ì¶•ì†Œ
- í™œìš©ë¥  ì¤‘ì‹¬ ë³´ìƒ ì„¤ê³„
- ë‹¤ì¤‘ ì‹œë“œ í‰ê°€ë¥¼ í†µí•œ ì•ˆì •ì„± ê²€ì¦

**ì°¨ì´ì :**
- ë…¼ë¬¸: Transformer ê¸°ë°˜ ì •ì±… ë„¤íŠ¸ì›Œí¬
- ë³¸ ì½”ë“œ: MLP ê¸°ë°˜ MaskablePPO (ë” ë‹¨ìˆœí•˜ì§€ë§Œ íš¨ê³¼ì )

---

## ğŸ“ ìƒì„±ëœ ì£¼ì„ í™•ì¥ íŒŒì¼

### 1. `enhanced_optimization_annotated.py` (29KB)
**Phase 4 í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìŠ¤í¬ë¦½íŠ¸ ì£¼ì„íŒ**

**ì£¼ìš” ê¸°ëŠ¥:**
- 9ê°€ì§€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸íŠ¸ ìë™ ë¹„êµ
- ì•ˆì •ì„±/ì•„í‚¤í…ì²˜/ìµœì í™” ì¹´í…Œê³ ë¦¬ë³„ íƒìƒ‰
- ì¢…í•© ì ìˆ˜ ê¸°ì¤€ ìˆœìœ„ ë§¤ê¸°ê¸°
- ìë™ ì„±ëŠ¥ ë¶„ì„ ì°¨íŠ¸ ìƒì„±

**ì‚¬ìš©ë²•:**
```bash
# ì „ì²´ ì„¸íŠ¸ ì‹¤í—˜
python enhanced_optimization_annotated.py --focus all --timesteps 35000

# ì•ˆì •ì„± ì¤‘ì‹¬ íƒìƒ‰
python enhanced_optimization_annotated.py --focus stability

# ê²°ê³¼ ë¶„ì„ë§Œ
python enhanced_optimization_annotated.py --analyze results/phase4_enhanced_*.json
```

### 2. `production_final_test_annotated.py` (11KB)  
**í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì£¼ì„íŒ**

**ì£¼ìš” ê¸°ëŠ¥:**
- Phase 4 ê²°ê³¼ë¡œ í™•ì •ëœ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
- 50,000 ìŠ¤í… í•™ìŠµ â†’ 50 ì—í”¼ì†Œë“œ í‰ê°€
- ì¬í˜„ì„± ë° ì•ˆì •ì„± ê²€ì¦
- ëª©í‘œ ì ìˆ˜(20.591) ë‹¬ì„±ë„ ì¸¡ì •

**ì‚¬ìš©ë²•:**
```bash
# ì™„ì „ í…ŒìŠ¤íŠ¸
python production_final_test_annotated.py --timesteps 50000 --episodes 50

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python production_final_test_annotated.py --quick
```

## ğŸ¯ ì£¼ì„ ê°œì„  í¬ì¸íŠ¸

### ğŸ“– ìƒì„¸í•œ ì„¤ëª… ì¶”ê°€
- ê° í•¨ìˆ˜/í´ë˜ìŠ¤ì˜ ëª©ì ê³¼ ë™ì‘ ë°©ì‹
- ë…¼ë¬¸ê³¼ì˜ ì—°ê²°ì  ëª…ì‹œ
- í•˜ì´í¼íŒŒë¼ë¯¸í„°ë³„ ì˜í–¥ë„ ì„¤ëª…

### ğŸ”§ ì½”ë“œ ì•ˆì „ì„± ê°œì„ 
- ê°€ë³€ ê¸°ë³¸ ì¸ì â†’ `None` ì²˜ë¦¬
- UTF-8 ì¸ì½”ë”© ëª…ì‹œ
- ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”

### ğŸ’¡ ì‚¬ìš©ì ì¹œí™”ì„±
- ì‹¤í–‰ ë°©ë²•ê³¼ ì¶œë ¥ë¬¼ ìœ„ì¹˜ ì•ˆë‚´
- ì—ëŸ¬ ìƒí™©ë³„ ì¹œì ˆí•œ ë©”ì‹œì§€
- ì„±ëŠ¥ í•´ì„ ê°€ì´ë“œë¼ì¸

## ğŸ“ˆ ì‹¤í–‰ ê²°ê³¼ë¬¼

**ìë™ ìƒì„±ë˜ëŠ” íŒŒì¼ë“¤:**
- `results/phase4_enhanced_{focus}_{timestamp}.json` - ìƒì„¸ ê²°ê³¼
- `results/phase4_enhanced_{focus}_{timestamp}_analysis.png` - ì„±ëŠ¥ ì°¨íŠ¸
- `results/production_final_{timestamp}.json` - ìµœì¢… ê²€ì¦ ê²°ê³¼
- `models/production_optimal_{timestamp}` - ìµœì  ëª¨ë¸

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

1. **í™˜ê²½ ì„¤ì •**: ì €ì¥ì†Œ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ (src ê²½ë¡œ ì¸ì‹ì„ ìœ„í•´)
2. **íƒìƒ‰ ì‹¤í–‰**: `python enhanced_optimization_annotated.py`
3. **ìµœì¢… ê²€ì¦**: `python production_final_test_annotated.py`
4. **ê²°ê³¼ í™•ì¸**: `results/` í´ë”ì˜ JSON/PNG íŒŒì¼ë“¤

ë‘ ì£¼ì„ í™•ì¥ íŒŒì¼ì´ ì €ì¥ì†Œ ë£¨íŠ¸ì— ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìœ¼ë©°, ì›ë³¸ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ ì„¤ëª…ê³¼ ì‹¤í–‰ ê°€ì´ë“œë¥¼ í’ë¶€í•˜ê²Œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤! ğŸ‰

```plaintext
ë°•ìŠ¤ ì‹œí€€ìŠ¤ ìƒì„± â†’ Gym í™˜ê²½ êµ¬ì„± â†’ í–‰ë™ ë§ˆìŠ¤í‚¹ â†’ ë³´ìƒ ì‰ì´í•‘ â†’ PPO í•™ìŠµ â†’ ë‹¤ì¤‘ í‰ê°€ â†’ ê²°ê³¼ ë¶„ì„
```

```shellscript
# ì „ì²´ ì„¸íŠ¸ ì‹¤í—˜
python enhanced_optimization_annotated.py --focus all --timesteps 35000

# ì•ˆì •ì„± ì¤‘ì‹¬ íƒìƒ‰
python enhanced_optimization_annotated.py --focus stability

# ê²°ê³¼ ë¶„ì„ë§Œ
python enhanced_optimization_annotated.py --analyze results/phase4_enhanced_*.json
```

```shellscript
# ì™„ì „ í…ŒìŠ¤íŠ¸
python production_final_test_annotated.py --timesteps 50000 --episodes 50

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python production_final_test_annotated.py --quick
```

