#!/usr/bin/env python3
"""
ğŸ† ìµœì  íŒŒë¼ë¯¸í„° ìµœì¢… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ (ë¬¸ì œ í•´ê²° ë²„ì „)
ë‹¤ì–‘í•œ í‰ê°€ ë°©ë²•ê³¼ ì¶©ë¶„í•œ í•™ìŠµìœ¼ë¡œ ì‹¤ì œ ì„±ëŠ¥ ì¸¡ì •
"""

import os
import sys
import warnings
import datetime
import time
import json

os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

print("ğŸ† ìµœì  íŒŒë¼ë¯¸í„° ìµœì¢… ê²€ì¦")

OPTIMAL_PARAMS = {
    'learning_rate': 2.6777169756959113e-06,
    'n_steps': 128,     # ë” í¬ê²Œ
    'batch_size': 16,   # ë” í¬ê²Œ
    'n_epochs': 3,      # ìµœì ê°’ ìœ ì§€
    'clip_range': 0.17716239549317803,
    'ent_coef': 0.06742268917730829,
    'vf_coef': 0.4545305173856873,
    'gae_lambda': 0.9449228658070746
}

# ë¹„êµìš© ê¸°ë³¸ íŒŒë¼ë¯¸í„°
DEFAULT_PARAMS = {
    'learning_rate': 3e-4,
    'n_steps': 128,
    'batch_size': 16,
    'n_epochs': 3,
    'clip_range': 0.2,
    'ent_coef': 0.01,
    'vf_coef': 0.5,
    'gae_lambda': 0.95
}

def create_environment(container_size, num_boxes, seed):
    """ê°œì„ ëœ í™˜ê²½ ìƒì„±"""
    try:
        import gymnasium as gym
        from gymnasium.envs.registration import register
        from packing_env import PackingEnv
        from utils import boxes_generator
        
        if 'PackingEnv-v0' not in gym.envs.registry:
            register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')
        
        box_sizes = boxes_generator(container_size, num_boxes, seed)
        env = gym.make(
            "PackingEnv-v0",
            container_size=container_size,
            box_sizes=box_sizes,
            num_visible_boxes=min(3, num_boxes),
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
        )
        
        # ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì ìš©
        try:
            from train_maskable_ppo import ImprovedRewardWrapper
            env = ImprovedRewardWrapper(env)
        except:
            pass
        
        # ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš©
        from sb3_contrib.common.wrappers import ActionMasker
        
        def get_masks(env):
            try:
                from train_maskable_ppo import get_action_masks
                return get_action_masks(env)
            except:
                import numpy as np
                return np.ones(env.action_space.n, dtype=bool)
        
        env = ActionMasker(env, get_masks)
        return env
        
    except Exception as e:
        print(f"âŒ í™˜ê²½ ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def train_model(env, params, train_steps=2000):
    """ëª¨ë¸ í•™ìŠµ"""
    try:
        import torch
        torch.set_default_dtype(torch.float32)
        torch.set_num_threads(2)
        
        from sb3_contrib import MaskablePPO
        import torch.nn as nn
        
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=params['learning_rate'],
            n_steps=params['n_steps'],
            batch_size=params['batch_size'],
            n_epochs=params['n_epochs'],
            gamma=0.99,
            gae_lambda=params['gae_lambda'],
            clip_range=params['clip_range'],
            ent_coef=params['ent_coef'],
            vf_coef=params['vf_coef'],
            max_grad_norm=0.5,
            verbose=0,
            device='cpu',
            policy_kwargs=dict(
                net_arch=[64, 64],  # ì ë‹¹í•œ í¬ê¸°
                activation_fn=nn.ReLU,
            )
        )
        
        print(f"ğŸ“ í•™ìŠµ ì‹œì‘: {train_steps:,} ìŠ¤í…")
        start_time = time.time()
        
        model.learn(total_timesteps=train_steps, progress_bar=False)
        
        training_time = time.time() - start_time
        print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")
        
        return model, training_time
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í•™ìŠµ ì˜¤ë¥˜: {e}")
        return None, 0

def diverse_evaluation(model, container_size, num_boxes, n_episodes=10):
    """ë‹¤ì–‘í•œ ì‹œë“œì™€ ë°©ë²•ìœ¼ë¡œ í‰ê°€"""
    try:
        print(f"ğŸ” ë‹¤ì–‘í•œ í‰ê°€ ì‹œì‘ ({n_episodes} ì—í”¼ì†Œë“œ)")
        
        all_rewards = []
        all_utilizations = []
        success_count = 0
        
        for ep in range(n_episodes):
            # ë§¤ë²ˆ ë‹¤ë¥¸ ì‹œë“œë¡œ ìƒˆ í™˜ê²½ ìƒì„±
            seed = 42 + ep * 10
            eval_env = create_environment(container_size, num_boxes, seed)
            
            if eval_env is None:
                continue
            
            obs, _ = eval_env.reset(seed=seed)
            episode_reward = 0.0
            steps = 0
            max_steps = 20  # ë” ë§ì€ ìŠ¤í…
            
            # Stochastic í‰ê°€ (ë‹¤ì–‘ì„±ì„ ìœ„í•´)
            for step in range(max_steps):
                try:
                    action, _ = model.predict(obs, deterministic=False)  # Falseë¡œ ë³€ê²½!
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    steps += 1
                    
                    if terminated or truncated:
                        break
                        
                except Exception as e:
                    print(f"     ì—í”¼ì†Œë“œ {ep+1} ìŠ¤í… {step} ì˜¤ë¥˜: {e}")
                    break
            
            # í™œìš©ë¥  ê³„ì‚°
            utilization = 0.0
            placed_boxes = 0
            try:
                if hasattr(eval_env.unwrapped, 'container'):
                    placed_volume = sum(box.volume for box in eval_env.unwrapped.container.boxes 
                                      if box.position is not None)
                    container_volume = eval_env.unwrapped.container.volume
                    utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                    
                    placed_boxes = sum(1 for box in eval_env.unwrapped.container.boxes 
                                     if box.position is not None)
            except:
                pass
            
            all_rewards.append(episode_reward)
            all_utilizations.append(utilization)
            
            if utilization >= 0.4:  # 40% ì´ìƒì„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                success_count += 1
            
            print(f"   ì—í”¼ì†Œë“œ {ep+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={utilization:.1%}, ë°•ìŠ¤={placed_boxes}ê°œ")
            
            eval_env.close()
        
        if not all_rewards:
            return None
        
        import numpy as np
        results = {
            'mean_reward': np.mean(all_rewards),
            'std_reward': np.std(all_rewards),
            'mean_utilization': np.mean(all_utilizations),
            'std_utilization': np.std(all_utilizations),
            'success_rate': success_count / len(all_rewards),
            'combined_score': np.mean(all_rewards) * 0.3 + np.mean(all_utilizations) * 100 * 0.7,
            'episodes': len(all_rewards)
        }
        
        return results
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì˜¤ë¥˜: {e}")
        return None

def final_validation():
    """ìµœì¢… ê²€ì¦ ì‹¤í–‰"""
    try:
        print("\nğŸ¯ ì‹¤í—˜ ì„¤ì •:")
        container_size = [8, 8, 8]  # ì ë‹¹í•œ í¬ê¸°
        num_boxes = 6  # ì ë‹¹í•œ ê°œìˆ˜
        train_steps = 2000  # ì¶©ë¶„í•œ í•™ìŠµ
        
        print(f"   - ì»¨í…Œì´ë„ˆ: {container_size}")
        print(f"   - ë°•ìŠ¤ ê°œìˆ˜: {num_boxes}")
        print(f"   - í•™ìŠµ ìŠ¤í…: {train_steps:,}")
        
        # === 1. ìµœì  íŒŒë¼ë¯¸í„° ì‹¤í—˜ ===
        print(f"\nğŸ† === ìµœì  íŒŒë¼ë¯¸í„° ì‹¤í—˜ ===")
        
        optimal_env = create_environment(container_size, num_boxes, 42)
        if optimal_env is None:
            raise ValueError("ìµœì  í™˜ê²½ ìƒì„± ì‹¤íŒ¨")
        
        optimal_model, optimal_time = train_model(optimal_env, OPTIMAL_PARAMS, train_steps)
        if optimal_model is None:
            raise ValueError("ìµœì  ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
        
        optimal_results = diverse_evaluation(optimal_model, container_size, num_boxes)
        optimal_env.close()
        del optimal_model
        
        # === 2. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‹¤í—˜ ===
        print(f"\nğŸ“Š === ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‹¤í—˜ ===")
        
        default_env = create_environment(container_size, num_boxes, 42)
        if default_env is None:
            raise ValueError("ê¸°ë³¸ í™˜ê²½ ìƒì„± ì‹¤íŒ¨")
        
        default_model, default_time = train_model(default_env, DEFAULT_PARAMS, train_steps)
        if default_model is None:
            raise ValueError("ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
        
        default_results = diverse_evaluation(default_model, container_size, num_boxes)
        default_env.close()
        del default_model
        
        # === 3. ê²°ê³¼ ë¹„êµ ===
        if optimal_results and default_results:
            print(f"\nğŸ“ˆ === ìµœì¢… ì„±ëŠ¥ ë¹„êµ ===")
            
            metrics = [
                ('í‰ê·  ë³´ìƒ', 'mean_reward', '.4f'),
                ('ë³´ìƒ ì•ˆì •ì„±', 'std_reward', '.4f'),
                ('í‰ê·  í™œìš©ë¥ ', 'mean_utilization', '.1%'),
                ('í™œìš©ë¥  ì•ˆì •ì„±', 'std_utilization', '.1%'),
                ('ì„±ê³µë¥ ', 'success_rate', '.1%'),
                ('ì¢…í•© ì ìˆ˜', 'combined_score', '.4f')
            ]
            
            print(f"{'ì§€í‘œ':<15} {'ìµœì  íŒŒë¼ë¯¸í„°':<15} {'ê¸°ë³¸ íŒŒë¼ë¯¸í„°':<15} {'ê°œì„ ìœ¨':<10}")
            print("-" * 65)
            
            for name, key, fmt in metrics:
                opt_val = optimal_results[key]
                def_val = default_results[key]
                
                if def_val != 0:
                    improvement = f"{((opt_val - def_val) / def_val * 100):+.1f}%"
                else:
                    improvement = "N/A"
                
                print(f"{name:<15} {opt_val:{fmt}:<15} {def_val:{fmt}:<15} {improvement:<10}")
            
            # ì¢…í•© í‰ê°€
            combined_improvement = ((optimal_results['combined_score'] - default_results['combined_score']) / 
                                   default_results['combined_score'] * 100)
            
            print(f"\nğŸ¯ ì¢…í•© ê²°ê³¼:")
            if combined_improvement > 15:
                print(f"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°ê°€ {combined_improvement:.1f}% ë›°ì–´ë‚œ ì„±ëŠ¥!")
            elif combined_improvement > 5:
                print(f"âœ… ìµœì  íŒŒë¼ë¯¸í„°ê°€ {combined_improvement:.1f}% ê°œì„ ëœ ì„±ëŠ¥!")
            elif combined_improvement > 0:
                print(f"ğŸ”„ ìµœì  íŒŒë¼ë¯¸í„°ê°€ ì•½ê°„ ê°œì„ ë¨ (+{combined_improvement:.1f}%)")
            else:
                print(f"âš ï¸ ìµœì  íŒŒë¼ë¯¸í„° ì„±ëŠ¥ì´ {abs(combined_improvement):.1f}% ë‚®ìŒ")
            
            # ê²°ê³¼ ì €ì¥
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = f"results/final_validation_{timestamp}.json"
            
            import os
            os.makedirs('results', exist_ok=True)
            
            final_data = {
                'timestamp': timestamp,
                'experiment_config': {
                    'container_size': container_size,
                    'num_boxes': num_boxes,
                    'train_steps': train_steps
                },
                'optimal_params': OPTIMAL_PARAMS,
                'default_params': DEFAULT_PARAMS,
                'optimal_results': optimal_results,
                'default_results': default_results,
                'training_times': {
                    'optimal': optimal_time,
                    'default': default_time
                },
                'improvement': combined_improvement
            }
            
            with open(results_file, 'w') as f:
                json.dump(final_data, f, indent=2)
            
            print(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ìµœì¢… ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ ì‹œìŠ¤í…œ í™˜ê²½:")
    print(f"   Python: {sys.version}")
    print(f"   ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    success = final_validation()
    if success:
        print(f"\nğŸ‰ ìµœì  íŒŒë¼ë¯¸í„° ìµœì¢… ê²€ì¦ ì™„ë£Œ!")
        print(f"ğŸ’¡ Optuna ìµœì í™” ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nâŒ ìµœì¢… ê²€ì¦ ì‹¤íŒ¨")
