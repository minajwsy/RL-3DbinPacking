#!/usr/bin/env python3
"""
ğŸš€ í´ë¼ìš°ë“œ í™˜ê²½ìš© ì‹¤ì œ PPO í•™ìŠµ + Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
í„°ë¯¸ë„ í¬ë˜ì‹œ ë¬¸ì œ í•´ê²° í›„ ì‹¤ì œ í•™ìŠµ ìˆ˜í–‰
"""

import os
import sys
import warnings
import datetime
import time

# í´ë¼ìš°ë“œ í™˜ê²½ ìµœì í™” ì„¤ì •
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")

# ë©”ëª¨ë¦¬ ìµœì í™”
import gc
gc.collect()

print("ğŸš€ í´ë¼ìš°ë“œ í”„ë¡œë•ì…˜ 3D Bin Packing í•™ìŠµ ì‹œì‘")

def production_train_with_optuna():
    """ì‹¤ì œ PPO í•™ìŠµì„ í¬í•¨í•œ Optuna ìµœì í™”"""
    try:
        # í•„ìˆ˜ ëª¨ë“ˆ ìˆœì°¨ ë¡œë”©
        print("ğŸ“¦ ëª¨ë“ˆ ë¡œë”© ì¤‘...")
        
        import numpy as np
        import optuna
        import torch
        print("âœ… ê¸°ë³¸ ëª¨ë“ˆ ë¡œë“œ")
        
        # í™˜ê²½ ê²½ë¡œ ì„¤ì •
        sys.path.append('src')
        
        # ë¡œì»¬ ëª¨ë“ˆ
        from train_maskable_ppo import make_env
        from packing_kernel import Container, Box
        print("âœ… ë¡œì»¬ ëª¨ë“ˆ ë¡œë“œ")
        
        # ê°•í™”í•™ìŠµ ëª¨ë“ˆ
        from sb3_contrib import MaskablePPO
        from sb3_contrib.common.maskable.utils import get_action_masks
        from stable_baselines3.common.monitor import Monitor
        print("âœ… ê°•í™”í•™ìŠµ ëª¨ë“ˆ ë¡œë“œ")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        # Optuna ìŠ¤í„°ë”” ìƒì„±
        study = optuna.create_study(
            direction="maximize",
            sampler=optuna.samplers.TPESampler(seed=42, multivariate=True),
            pruner=optuna.pruners.MedianPruner(
                n_startup_trials=3,
                n_warmup_steps=500,
                interval_steps=200
            )
        )
        
        print("âœ… Optuna ìŠ¤í„°ë”” ìƒì„± ì™„ë£Œ")
        
        def objective(trial):
            """ì‹¤ì œ PPO í•™ìŠµì„ í¬í•¨í•œ ëª©ì  í•¨ìˆ˜"""
            try:
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ (ìµœì í™”ëœ ë²”ìœ„)
                learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True)
                n_steps = trial.suggest_categorical('n_steps', [512, 1024, 2048, 4096])
                batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512])
                n_epochs = trial.suggest_int('n_epochs', 3, 15)
                clip_range = trial.suggest_float('clip_range', 0.1, 0.4)
                ent_coef = trial.suggest_float('ent_coef', 1e-4, 1e-1, log=True)
                vf_coef = trial.suggest_float('vf_coef', 0.1, 1)
                gae_lambda = trial.suggest_float('gae_lambda', 0.9, 0.99)
                
                print(f"ğŸ”¬ Trial {trial.number}: lr={learning_rate:.6f}, steps={n_steps}, batch={batch_size}")
                
                # í™˜ê²½ ìƒì„± (ì ì§„ì ìœ¼ë¡œ í¬ê¸° ì¦ê°€)
                container_size = [10, 10, 10]  # ì‹¤ì œ í¬ê¸°
                num_boxes = 12  # ì ë‹¹í•œ ë³µì¡ì„±
                
                env = make_env(
                    container_size=container_size,
                    num_boxes=num_boxes,
                    num_visible_boxes=3,
                    seed=42 + trial.number,
                    render_mode=None,
                    random_boxes=False,
                    only_terminal_reward=False,
                    improved_reward_shaping=True,
                )()
                
                eval_env = make_env(
                    container_size=container_size,
                    num_boxes=num_boxes,
                    num_visible_boxes=3,
                    seed=43 + trial.number,
                    render_mode=None,
                    random_boxes=False,
                    only_terminal_reward=False,
                    improved_reward_shaping=True,
                )()
                
                # ëª¨ë‹ˆí„°ë§
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                env = Monitor(env, f"logs/prod_train_trial_{trial.number}_{timestamp}.csv")
                eval_env = Monitor(eval_env, f"logs/prod_eval_trial_{trial.number}_{timestamp}.csv")
                
                # PPO ëª¨ë¸ ìƒì„± (ìˆ˜ì •ë¨)
                model = MaskablePPO(
                    "MultiInputPolicy",
                    env,
                    learning_rate=learning_rate,
                    n_steps=n_steps,
                    batch_size=batch_size,
                    n_epochs=n_epochs,
                    clip_range=clip_range,
                    ent_coef=ent_coef,
                    vf_coef=vf_coef,
                    gae_lambda=gae_lambda,
                    gamma=0.99,
                    max_grad_norm=0.5,
                    verbose=0,
                    seed=42 + trial.number,
                    policy_kwargs=dict(
                        net_arch=[256, 256, 128],
                        activation_fn=torch.nn.ReLU,  # ìˆ˜ì •: ë¬¸ìì—´ ëŒ€ì‹  torch ê°ì²´
                        share_features_extractor=True,
                    )
                )
                
                # ì‹¤ì œ í•™ìŠµ ìˆ˜í–‰ (ì§§ì€ ì‹œê°„)
                train_timesteps = 3000  # í´ë¼ìš°ë“œ í™˜ê²½ì— ë§ì¶˜ ì ë‹¹í•œ ê¸¸ì´
                start_time = time.time()
                
                print(f"   ğŸ“ í•™ìŠµ ì‹œì‘: {train_timesteps} ìŠ¤í…")
                model.learn(total_timesteps=train_timesteps, progress_bar=False)
                
                training_time = time.time() - start_time
                print(f"   â±ï¸ í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")
                
                # ëª¨ë¸ í‰ê°€ (ìˆ˜ì •ëœ get_action_masks ì‚¬ìš©ë²•)
                total_rewards = []
                total_utilizations = []
                
                for ep in range(3):  # 3 ì—í”¼ì†Œë“œ í‰ê°€
                    obs, _ = eval_env.reset()
                    episode_reward = 0.0
                    done = False
                    step_count = 0
                    
                    while not done and step_count < 50:
                        # ìˆ˜ì •: get_action_masksë¥¼ í™˜ê²½ì—ì„œ ì§ì ‘ í˜¸ì¶œ
                        if hasattr(eval_env, 'action_masks'):
                            action_masks = eval_env.action_masks()
                        else:
                            # ê¸°ë³¸ ë§ˆìŠ¤í¬ (ëª¨ë“  ì•¡ì…˜ í—ˆìš©)
                            action_masks = np.ones(eval_env.action_space.n, dtype=bool)
                        
                        action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
                        obs, reward, terminated, truncated, info = eval_env.step(action)
                        episode_reward += reward
                        done = terminated or truncated
                        step_count += 1
                    
                    total_rewards.append(episode_reward)
                    
                    # í™œìš©ë¥  ê³„ì‚°
                    if hasattr(eval_env.unwrapped, 'container'):
                        placed_volume = sum(box.volume for box in eval_env.unwrapped.container.boxes 
                                            if box.position is not None)
                        container_volume = eval_env.unwrapped.container.volume
                        utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                        total_utilizations.append(utilization)
                    else:
                        total_utilizations.append(0.0)
                
                # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                mean_reward = np.mean(total_rewards)
                mean_utilization = np.mean(total_utilizations)
                
                # ë‹¤ì¤‘ ëª©ì  ìµœì í™”: ê³µê°„ í™œìš©ë¥  70% + í‰ê·  ë³´ìƒ 30%
                combined_score = mean_reward * 0.3 + mean_utilization * 100 * 0.7
                
                print(f"   ğŸ“Š í‰ê·  ë³´ìƒ: {mean_reward:.4f}")
                print(f"   ğŸ“¦ í‰ê·  í™œìš©ë¥ : {mean_utilization:.1%}")
                print(f"   ğŸ¯ ì¢…í•© ì ìˆ˜: {combined_score:.4f}")
                
                # í™˜ê²½ ì •ë¦¬
                env.close()
                eval_env.close()
                del model  # ë©”ëª¨ë¦¬ í•´ì œ
                gc.collect()
                
                return combined_score
                
            except Exception as e:
                print(f"âŒ Trial {trial.number} ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                return 0.0
        
        # ìµœì í™” ì‹¤í–‰
        n_trials = 10  # ì‹¤ì œ í•™ìŠµì„ ìœ„í•œ ì ë‹¹í•œ trial ìˆ˜
        print(f"ğŸš€ í”„ë¡œë•ì…˜ ìµœì í™” ì‹œì‘ ({n_trials} trials)")
        
        study.optimize(objective, n_trials=n_trials)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ† í”„ë¡œë•ì…˜ ìµœì í™” ì™„ë£Œ!")
        print(f"ìµœì  ê°’: {study.best_value:.4f}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°:")
        for key, value in study.best_params.items():
            if 'learning_rate' in key:
                print(f"  {key}: {value:.6f}")
            else:
                print(f"  {key}: {value}")
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"results/production_optuna_results_{timestamp}.txt"
        os.makedirs('results', exist_ok=True)
        
        with open(results_file, 'w') as f:
            f.write("=== í´ë¼ìš°ë“œ í”„ë¡œë•ì…˜ Optuna ìµœì í™” ê²°ê³¼ ===\n")
            f.write(f"ì‹¤í–‰ ì‹œê°„: {timestamp}\n")
            f.write(f"Trial ìˆ˜: {n_trials}\n")
            f.write(f"ìµœì  ê°’: {study.best_value:.4f}\n")
            f.write(f"ìµœì  íŒŒë¼ë¯¸í„°:\n")
            for key, value in study.best_params.items():
                f.write(f"  {key}: {value}\n")
            f.write(f"\nëª¨ë“  Trial ê²°ê³¼:\n")
            for trial in study.trials:
                f.write(f"Trial {trial.number}: {trial.value:.4f} - {trial.params}\n")
        
        print(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {results_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í”„ë¡œë•ì…˜ ìµœì í™” ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="í´ë¼ìš°ë“œ í”„ë¡œë•ì…˜ 3D Bin Packing")
    parser.add_argument("--optimize", action="store_true", help="í”„ë¡œë•ì…˜ Optuna ìµœì í™”")
    parser.add_argument("--trials", type=int, default=10, help="Trial ìˆ˜")
    
    try:
        args = parser.parse_args()
        
        if args.optimize:
            print(f"ğŸ”¬ í”„ë¡œë•ì…˜ Optuna ìµœì í™” ëª¨ë“œ ({args.trials} trials)")
            success = production_train_with_optuna()
            if success:
                print("âœ… í”„ë¡œë•ì…˜ ìµœì í™” ì„±ê³µ!")
            else:
                print("âŒ í”„ë¡œë•ì…˜ ìµœì í™” ì‹¤íŒ¨!")
        else:
            print("ğŸ’¡ ì‚¬ìš©ë²•:")
            print("  python production_train_cloud.py --optimize")
            print("  python production_train_cloud.py --optimize --trials 20")
            
    except Exception as e:
        print(f"âŒ ë©”ì¸ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main() 