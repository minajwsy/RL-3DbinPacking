#!/usr/bin/env python3
"""
ğŸš€ 999ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²° + GIF + ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„± + Optuna/W&B í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
í‰ê°€ ì½œë°±ì„ ì™„ì „íˆ ì¬ì‘ì„±í•˜ì—¬ ë¬´í•œ ëŒ€ê¸° ë¬¸ì œ 100% í•´ê²°
Optunaì™€ W&B sweepì„ ì´ìš©í•œ ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì§€ì›
"""

import os
import sys
import time
import datetime
import warnings
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from pathlib import Path
import torch  # torch import ì¶”ê°€
import json
from typing import Dict, Any, Optional, Tuple

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import optuna
    from optuna.integration import WeightsAndBiasesCallback
    OPTUNA_AVAILABLE = True
    print("âœ… Optuna ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optuna ì—†ìŒ - pip install optuna í•„ìš”")

try:
    import wandb
    WANDB_AVAILABLE = True
    print("âœ… W&B ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸ W&B ì—†ìŒ - pip install wandb í•„ìš”")

# ì„œë²„ í™˜ê²½ ëŒ€ì‘
matplotlib.use('Agg')
warnings.filterwarnings("ignore")

# í°íŠ¸ ì„¤ì • (í•œê¸€ í°íŠ¸ ë¬¸ì œ í•´ê²°)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 10
plt.rcParams['axes.unicode_minus'] = False

# ê²½ë¡œ ì„¤ì •
sys.path.append('src')
os.environ['PYTHONPATH'] = os.getcwd() + ':' + os.getcwd() + '/src'

from stable_baselines3 import PPO
from sb3_contrib import MaskablePPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_checker import check_env

# ë¡œì»¬ ëª¨ë“ˆ import
from packing_kernel import *
from train_maskable_ppo import make_env, get_action_masks

class UltimateSafeCallback(BaseCallback):
    """
    999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²°ì„ ìœ„í•œ ì•ˆì „í•œ ì½œë°±
    - ìµœì†Œí•œì˜ í‰ê°€ë§Œ ìˆ˜í–‰
    - íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±
    """
    def __init__(self, eval_env, eval_freq=2000, verbose=1):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.last_eval_time = 0
        
        # ì„±ëŠ¥ ì§€í‘œ ì €ì¥
        self.timesteps = []
        self.episode_rewards = []
        self.eval_rewards = []
        self.eval_timesteps = []
        self.success_rates = []
        self.utilization_rates = []
        
        # ì‹¤ì‹œê°„ í”Œë¡¯ ì„¤ì •
        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle('Real-time Training Performance Monitoring', fontsize=16)
        plt.ion()
        
        print(f"ğŸ›¡ï¸ ì•ˆì „í•œ ì½œë°± ì´ˆê¸°í™” ì™„ë£Œ (í‰ê°€ ì£¼ê¸°: {eval_freq})")
        
    def _on_training_start(self) -> None:
        """í•™ìŠµ ì‹œì‘"""
        print("ğŸš€ ì•ˆì „í•œ í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        self.start_time = time.time()
        self._setup_plots()
        
    def _on_step(self) -> bool:
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ - ì•ˆì „í•œ ì²˜ë¦¬"""
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²´í¬
        if self.locals.get('dones', [False])[0]:
            if 'episode' in self.locals.get('infos', [{}])[0]:
                episode_info = self.locals['infos'][0]['episode']
                episode_reward = episode_info['r']
                
                self.timesteps.append(self.num_timesteps)
                self.episode_rewards.append(episode_reward)
                
                # ì£¼ê¸°ì  ì¶œë ¥
                if len(self.episode_rewards) % 20 == 0:
                    recent_rewards = self.episode_rewards[-20:]
                    mean_reward = np.mean(recent_rewards)
                    elapsed = time.time() - self.start_time
                    
                    print(f"ğŸ“Š ìŠ¤í…: {self.num_timesteps:,} | "
                          f"ì—í”¼ì†Œë“œ: {len(self.episode_rewards)} | "
                          f"ìµœê·¼ í‰ê·  ë³´ìƒ: {mean_reward:.3f} | "
                          f"ê²½ê³¼: {elapsed:.1f}ì´ˆ")
        
        # ì•ˆì „í•œ í‰ê°€ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
        if self.num_timesteps - self.last_eval_time >= self.eval_freq:
            self._safe_evaluation()
            self._update_plots()
            self.last_eval_time = self.num_timesteps
            
        return True
    
    def _safe_evaluation(self):
        """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” ì•ˆì „í•œ í‰ê°€ -> í‰ê°€í•¨ìˆ˜ ê°œì„ """
        try:
            print(f"ğŸ” ì•ˆì „í•œ í‰ê°€ ì‹œì‘ (ìŠ¤í…: {self.num_timesteps:,})")
            
            eval_rewards = []
            success_count = 0
            max_episodes = 15  # 8 â†’ 15ë¡œ ì¦ê°€ (ë” ì •í™•í•œ í‰ê°€)
            
            for ep_idx in range(max_episodes):
                try:
                    # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                    eval_start = time.time()
                    timeout = 60  # 30 â†’ 60ì´ˆë¡œ ì¦ê°€
                    
                    obs, _ = self.eval_env.reset()
                    episode_reward = 0.0
                    step_count = 0
                    max_steps = 100  # 50 â†’ 100ìœ¼ë¡œ ì¦ê°€ (ì¶©ë¶„í•œ ì‹œê°„)
                    
                    while step_count < max_steps:
                        # íƒ€ì„ì•„ì›ƒ ì²´í¬
                        if time.time() - eval_start > timeout:
                            print(f"â° í‰ê°€ íƒ€ì„ì•„ì›ƒ (ì—í”¼ì†Œë“œ {ep_idx})")
                            break
                            
                        try:
                            action_masks = get_action_masks(self.eval_env)
                            action, _ = self.model.predict(obs, action_masks=action_masks, deterministic=True)
                            obs, reward, terminated, truncated, info = self.eval_env.step(action)
                            episode_reward += reward
                            step_count += 1
                            
                            if terminated or truncated:
                                break
                        except Exception as e:
                            print(f"âš ï¸ í‰ê°€ ìŠ¤í… ì˜¤ë¥˜: {e}")
                            break
                    
                    eval_rewards.append(episode_reward)
                    success_threshold = 5.0  # 3.0 â†’ 5.0ìœ¼ë¡œ ìƒí–¥ ì¡°ì •
                    if episode_reward >= success_threshold:
                        success_count += 1
                        
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì—í”¼ì†Œë“œ {ep_idx} ì˜¤ë¥˜: {e}")
                    eval_rewards.append(0.0)
            
            # ê²°ê³¼ ì €ì¥
            if eval_rewards:
                mean_eval_reward = np.mean(eval_rewards)
                success_rate = success_count / len(eval_rewards)
                utilization = max(0.0, mean_eval_reward)
                
                self.eval_rewards.append(mean_eval_reward)
                self.eval_timesteps.append(self.num_timesteps)
                self.success_rates.append(success_rate)
                self.utilization_rates.append(utilization)
                
                print(f"âœ… í‰ê°€ ì™„ë£Œ: ë³´ìƒ {mean_eval_reward:.3f}, "
                      f"ì„±ê³µë¥  {success_rate:.1%}, í™œìš©ë¥  {utilization:.1%}")
            else:
                print("âŒ í‰ê°€ ì‹¤íŒ¨ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                self.eval_rewards.append(0.0)
                self.eval_timesteps.append(self.num_timesteps)
                self.success_rates.append(0.0)
                self.utilization_rates.append(0.0)
                
        except Exception as e:
            print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œì—ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰
            self.eval_rewards.append(0.0)
            self.eval_timesteps.append(self.num_timesteps)
            self.success_rates.append(0.0)
            self.utilization_rates.append(0.0)
    
    def _setup_plots(self):
        """í”Œë¡¯ ì´ˆê¸° ì„¤ì •"""
        titles = [
            'Episode Rewards (Training)',
            'Evaluation Rewards (Periodic)',
            'Success Rate (%)',
            'Utilization Rate (%)'
        ]
        
        for i, ax in enumerate(self.axes.flat):
            ax.set_title(titles[i])
            ax.grid(True, alpha=0.3)
            ax.set_xlabel('Steps')
    
    def _update_plots(self):
        """ì‹¤ì‹œê°„ í”Œë¡¯ ì—…ë°ì´íŠ¸"""
        try:
            # 1. ì—í”¼ì†Œë“œ ë³´ìƒ
            if self.timesteps and self.episode_rewards:
                self.axes[0, 0].clear()
                self.axes[0, 0].plot(self.timesteps, self.episode_rewards, 'b-', alpha=0.6, linewidth=1)
                if len(self.episode_rewards) > 50:
                    # ì´ë™ í‰ê· 
                    window = min(50, len(self.episode_rewards) // 4)
                    moving_avg = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')
                    moving_steps = self.timesteps[window-1:]
                    self.axes[0, 0].plot(moving_steps, moving_avg, 'r-', linewidth=2, label=f'Moving Avg({window})')
                    self.axes[0, 0].legend()
                self.axes[0, 0].set_title('Episode Rewards (Training)')
                self.axes[0, 0].grid(True, alpha=0.3)
            
            # 2. í‰ê°€ ë³´ìƒ
            if self.eval_timesteps and self.eval_rewards:
                self.axes[0, 1].clear()
                self.axes[0, 1].plot(self.eval_timesteps, self.eval_rewards, 'g-o', linewidth=2, markersize=6)
                self.axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
                self.axes[0, 1].set_title('Evaluation Rewards (Periodic)')
                self.axes[0, 1].grid(True, alpha=0.3)
            
            # 3. ì„±ê³µë¥ 
            if self.eval_timesteps and self.success_rates:
                self.axes[1, 0].clear()
                success_pct = [rate * 100 for rate in self.success_rates]
                self.axes[1, 0].plot(self.eval_timesteps, success_pct, 'orange', linewidth=2, marker='s')
                self.axes[1, 0].axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Target(80%)')
                self.axes[1, 0].set_ylim(0, 100)
                self.axes[1, 0].set_title('Success Rate (%)')
                self.axes[1, 0].grid(True, alpha=0.3)
                self.axes[1, 0].legend()
            
            # 4. í™œìš©ë¥ 
            if self.eval_timesteps and self.utilization_rates:
                self.axes[1, 1].clear()
                util_pct = [rate * 100 for rate in self.utilization_rates]
                self.axes[1, 1].plot(self.eval_timesteps, util_pct, 'purple', linewidth=2, marker='^')
                # ë™ì  Yì¶• ì„¤ì • (ìµœëŒ€ê°’ì˜ 110%ê¹Œì§€)
                max_util = max(util_pct) if util_pct else 100
                self.axes[1, 1].set_ylim(0, max(100, max_util * 1.1))
                self.axes[1, 1].set_title('Utilization Rate (%)')
                self.axes[1, 1].grid(True, alpha=0.3)
            
            # í”Œë¡¯ ì €ì¥
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            plt.tight_layout()
            plt.savefig(f'results/realtime_performance_{timestamp}.png', dpi=150, bbox_inches='tight')
            
        except Exception as e:
            print(f"âš ï¸ í”Œë¡¯ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def _on_training_end(self) -> None:
        """í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì¢… ê·¸ë˜í”„ ì €ì¥"""
        print("ğŸ“Š ìµœì¢… ì„±ê³¼ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ìµœì¢… ì„±ê³¼ ëŒ€ì‹œë³´ë“œ
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Final Training Performance Dashboard', fontsize=20)
        
        try:
            # 1. í•™ìŠµ ê³¡ì„ 
            if self.timesteps and self.episode_rewards:
                axes[0, 0].plot(self.timesteps, self.episode_rewards, 'b-', alpha=0.4, linewidth=1, label='Episode Rewards')
                if len(self.episode_rewards) > 20:
                    window = min(50, len(self.episode_rewards) // 4)
                    moving_avg = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')
                    moving_steps = self.timesteps[window-1:]
                    axes[0, 0].plot(moving_steps, moving_avg, 'r-', linewidth=3, label=f'Moving Avg({window})')
                axes[0, 0].set_title('Learning Curve')
                axes[0, 0].set_xlabel('Steps')
                axes[0, 0].set_ylabel('Reward')
                axes[0, 0].legend()
                axes[0, 0].grid(True, alpha=0.3)
            
            # 2. í‰ê°€ ì„±ëŠ¥
            if self.eval_timesteps and self.eval_rewards:
                axes[0, 1].plot(self.eval_timesteps, self.eval_rewards, 'g-o', linewidth=3, markersize=8)
                axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
                axes[0, 1].set_title('Evaluation Performance')
                axes[0, 1].set_xlabel('Steps')
                axes[0, 1].set_ylabel('Evaluation Reward')
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. ì„±ê³µë¥  ì¶”ì´
            if self.eval_timesteps and self.success_rates:
                success_pct = [rate * 100 for rate in self.success_rates]
                axes[0, 2].plot(self.eval_timesteps, success_pct, 'orange', linewidth=3, marker='s', markersize=8)
                axes[0, 2].axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Target(80%)')
                axes[0, 2].set_ylim(0, 100)
                axes[0, 2].set_title('Success Rate Trend')
                axes[0, 2].set_xlabel('Steps')
                axes[0, 2].set_ylabel('Success Rate (%)')
                axes[0, 2].legend()
                axes[0, 2].grid(True, alpha=0.3)
            
            # 4. í™œìš©ë¥  ì¶”ì´
            if self.eval_timesteps and self.utilization_rates:
                util_pct = [rate * 100 for rate in self.utilization_rates]
                axes[1, 0].plot(self.eval_timesteps, util_pct, 'purple', linewidth=3, marker='^', markersize=8)
                # ë™ì  Yì¶• ì„¤ì • (ìµœëŒ€ê°’ì˜ 110%ê¹Œì§€)
                max_util = max(util_pct) if util_pct else 100
                axes[1, 0].set_ylim(0, max(100, max_util * 1.1))
                axes[1, 0].set_title('Utilization Rate Trend')
                axes[1, 0].set_xlabel('Steps')
                axes[1, 0].set_ylabel('Utilization Rate (%)')
                axes[1, 0].grid(True, alpha=0.3)
            
            # 5. ë³´ìƒ ë¶„í¬
            if self.episode_rewards:
                axes[1, 1].hist(self.episode_rewards, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
                axes[1, 1].axvline(np.mean(self.episode_rewards), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(self.episode_rewards):.3f}')
                axes[1, 1].set_title('Reward Distribution')
                axes[1, 1].set_xlabel('Reward')
                axes[1, 1].set_ylabel('Frequency')
                axes[1, 1].legend()
                axes[1, 1].grid(True, alpha=0.3)
            
            # 6. ì„±ê³¼ ìš”ì•½
            axes[1, 2].axis('off')
            if self.episode_rewards and self.eval_rewards:
                summary_text = f"""
Training Summary Statistics

Total Episodes: {len(self.episode_rewards):,}
Final Steps: {self.num_timesteps:,}
Training Time: {(time.time() - self.start_time):.1f}s

Training Performance:
â€¢ Mean Reward: {np.mean(self.episode_rewards):.3f}
â€¢ Max Reward: {np.max(self.episode_rewards):.3f}
â€¢ Min Reward: {np.min(self.episode_rewards):.3f}
â€¢ Std Dev: {np.std(self.episode_rewards):.3f}

Evaluation Performance:
â€¢ Final Eval Reward: {self.eval_rewards[-1] if self.eval_rewards else 0:.3f}
â€¢ Best Eval Reward: {np.max(self.eval_rewards) if self.eval_rewards else 0:.3f}
â€¢ Final Success Rate: {self.success_rates[-1]*100 if self.success_rates else 0:.1f}%
â€¢ Final Utilization: {self.utilization_rates[-1]*100 if self.utilization_rates else 0:.1f}%
"""
                axes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes, 
                               fontsize=12, verticalalignment='top', fontfamily='monospace',
                               bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
            
            plt.tight_layout()
            dashboard_path = f'results/ultimate_dashboard_{timestamp}.png'
            plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ìµœì¢… ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_path}")
            
            # ì„±ëŠ¥ ë°ì´í„° ì €ì¥
            performance_data = {
                'timesteps': self.timesteps,
                'episode_rewards': self.episode_rewards,
                'eval_timesteps': self.eval_timesteps,
                'eval_rewards': self.eval_rewards,
                'success_rates': self.success_rates,
                'utilization_rates': self.utilization_rates
            }
            np.save(f'results/performance_data_{timestamp}.npy', performance_data)
            
        except Exception as e:
            print(f"âš ï¸ ìµœì¢… ê·¸ë˜í”„ ìƒì„± ì˜¤ë¥˜: {e}")
        
        plt.close('all')

class AdaptiveCurriculumCallback(BaseCallback):
    """
    MathWorks/YouTube ì‚¬ë¡€ ê¸°ë°˜ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ
    - ì„±ëŠ¥ ê¸°ë°˜ ë™ì  ë‚œì´ë„ ì¡°ì •
    - ë‹¤ì¤‘ ì§€í‘œ í‰ê°€ (ë³´ìƒ, ì„±ê³µë¥ , ì•ˆì •ì„±)
    - ìë™ ë°±íŠ¸ë˜í‚¹ (ì„±ëŠ¥ ì €í•˜ì‹œ ì´ì „ ë‹¨ê³„ë¡œ)
    """
    
    def __init__(
        self,
        container_size,
        initial_boxes,
        target_boxes,
        num_visible_boxes,
        success_threshold=0.45,  # MathWorks ê¶Œì¥: ë‚®ì€ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘
        curriculum_steps=7,     # ë” ë§ì€ ë‹¨ê³„
        patience=10,            # ë” ê¸´ ì¸ë‚´ì‹¬
        stability_window=50,   # ì•ˆì •ì„± ì¸¡ì • ìœˆë„ìš°
        verbose=1,
    ):
        super().__init__(verbose)
        self.container_size = container_size
        self.initial_boxes = initial_boxes
        self.target_boxes = target_boxes
        self.num_visible_boxes = num_visible_boxes
        self.success_threshold = success_threshold
        self.curriculum_steps = curriculum_steps
        self.patience = patience
        self.stability_window = stability_window
        self.verbose = verbose
        
        # ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ ì„¤ì •
        self.current_boxes = initial_boxes
        self.box_increments = []
        if target_boxes > initial_boxes:
            step_size = max(1, (target_boxes - initial_boxes) // curriculum_steps)
            for i in range(curriculum_steps):
                next_boxes = initial_boxes + (i + 1) * step_size
                if next_boxes > target_boxes:
                    next_boxes = target_boxes
                self.box_increments.append(next_boxes)
            # ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” í•­ìƒ target_boxes
            if self.box_increments[-1] != target_boxes:
                self.box_increments.append(target_boxes)
        
        # ì„±ê³¼ ì¶”ì  ë³€ìˆ˜ (ë‹¤ì¤‘ ì§€í‘œ)
        self.evaluation_count = 0
        self.consecutive_successes = 0
        self.curriculum_level = 0
        self.last_success_rate = 0.0
        self.recent_rewards = []
        self.recent_episode_lengths = []
        self.recent_utilizations = []
        
        # MathWorks ê¸°ë°˜ ì ì‘ì  íŒŒë¼ë¯¸í„°
        self.performance_history = []  # ê° ë ˆë²¨ë³„ ì„±ëŠ¥ ê¸°ë¡
        self.stability_scores = []     # ì•ˆì •ì„± ì ìˆ˜
        self.backtrack_count = 0       # ë°±íŠ¸ë˜í‚¹ íšŸìˆ˜
        self.max_backtrack = 3         # ìµœëŒ€ ë°±íŠ¸ë˜í‚¹ í—ˆìš©
        
        # ì ì‘ì  ì„ê³„ê°’ (ì„±ëŠ¥ì— ë”°ë¼ ë™ì  ì¡°ì •)
        self.adaptive_threshold = success_threshold
        self.threshold_decay = 0.95    # ì„ê³„ê°’ ê°ì†Œìœ¨
        
        if self.verbose >= 1:
            print(f"ğŸ“ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì´ˆê¸°í™”:")
            print(f"   - ì‹œì‘ ë°•ìŠ¤ ìˆ˜: {self.initial_boxes}")
            print(f"   - ëª©í‘œ ë°•ìŠ¤ ìˆ˜: {self.target_boxes}")
            print(f"   - ë‹¨ê³„ë³„ ì¦ê°€: {self.box_increments}")
            print(f"   - ì´ˆê¸° ì„±ê³µ ì„ê³„ê°’: {self.success_threshold}")
            print(f"   - ì•ˆì •ì„± ìœˆë„ìš°: {self.stability_window}")
    
    def _on_step(self) -> bool:
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ - ë‹¤ì¤‘ ì§€í‘œ ìˆ˜ì§‘"""
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²´í¬
        if self.locals.get('dones', [False])[0]:
            if 'episode' in self.locals.get('infos', [{}])[0]:
                episode_info = self.locals['infos'][0]['episode']
                episode_reward = episode_info['r']
                episode_length = episode_info['l']
                
                # í™œìš©ë¥  ê³„ì‚° (ë³´ìƒì„ í™œìš©ë¥ ë¡œ ê°„ì£¼)
                episode_utilization = max(0.0, episode_reward / 10.0)  # ì •ê·œí™”
                
                # ìµœê·¼ ì„±ê³¼ ê¸°ë¡ (ì ì‘ì  ìœˆë„ìš°)
                self.recent_rewards.append(episode_reward)
                self.recent_episode_lengths.append(episode_length)
                self.recent_utilizations.append(episode_utilization)
                
                # ìœˆë„ìš° í¬ê¸° ìœ ì§€
                if len(self.recent_rewards) > self.stability_window:
                    self.recent_rewards.pop(0)
                    self.recent_episode_lengths.pop(0)
                    self.recent_utilizations.pop(0)
                
                # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ í‰ê°€
                if len(self.recent_rewards) >= min(15, self.stability_window // 2):
                    self._evaluate_adaptive_curriculum()
        
        return True
    
    def _evaluate_adaptive_curriculum(self):
        """ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰ ìƒí™© í‰ê°€ (MathWorks ê¸°ë²•)"""
        try:
            # 1. ë‹¤ì¤‘ ì„±ê³¼ ì§€í‘œ ê³„ì‚°
            rewards = self.recent_rewards
            lengths = self.recent_episode_lengths
            utilizations = self.recent_utilizations
            
            # ì„±ê³µë¥  (ë³´ìƒ ê¸°ë°˜)
            success_count = sum(1 for r in rewards if r > self.adaptive_threshold)
            success_rate = success_count / len(rewards)
            
            # ì•ˆì •ì„± ì ìˆ˜ (ë³€ë™ì„± ê¸°ë°˜)
            import numpy as np
            reward_stability = 1.0 / (1.0 + np.std(rewards))
            length_stability = 1.0 / (1.0 + np.std(lengths))
            stability_score = (reward_stability + length_stability) / 2.0
            
            # í™œìš©ë¥  ê°œì„ ë„
            if len(utilizations) >= 10:
                recent_util = np.mean(utilizations[-5:])
                prev_util = np.mean(utilizations[-10:-5])
                util_improvement = recent_util - prev_util
            else:
                util_improvement = 0.0
            
            # ì¢…í•© ì„±ê³¼ ì ìˆ˜ (MathWorks ê°€ì¤‘ í‰ê· )
            performance_score = (
                success_rate * 0.3 +          # ì„±ê³µë¥  30%
                stability_score * 0.7 +       # ì•ˆì •ì„± 70%
                max(0, util_improvement) * 0.0 # ê°œì„ ë„ 0%
            )
            
            # í˜„ì¬ ì„±ê³¼ ê¸°ë¡
            self.last_success_rate = success_rate
            self.stability_scores.append(stability_score)
            self.performance_history.append(performance_score)
            self.evaluation_count += 1
            
            if self.verbose >= 2:
                print(f"ğŸ“Š ì ì‘ì  í‰ê°€ (ë ˆë²¨ {self.curriculum_level}):")
                print(f"   - ì„±ê³µë¥ : {success_rate:.1%}")
                print(f"   - ì•ˆì •ì„±: {stability_score:.3f}")
                print(f"   - í™œìš©ë¥  ê°œì„ : {util_improvement:.3f}")
                print(f"   - ì¢…í•© ì ìˆ˜: {performance_score:.3f}")
            
            # 2. ì ì‘ì  ë‚œì´ë„ ì¡°ì • ê²°ì •
            if performance_score >= 0.6:  # MathWorks ê¶Œì¥ ê¸°ì¤€
                self.consecutive_successes += 1
                
                # ì—°ì† ì„±ê³µ + ì•ˆì •ì„± í™•ì¸
                if (self.consecutive_successes >= self.patience and 
                    stability_score >= 0.7):  # ì•ˆì •ì„± ê¸°ì¤€ ì¶”ê°€
                    self._adaptive_increase_difficulty()
                    
            elif performance_score < 0.3:  # ì„±ëŠ¥ ì €í•˜ ê°ì§€
                self._consider_backtrack()
            else:
                self.consecutive_successes = 0
                # ì ì‘ì  ì„ê³„ê°’ ì¡°ì •
                if len(self.performance_history) >= 5:
                    recent_performance = np.mean(self.performance_history[-5:])
                    if recent_performance < 0.4:
                        self.adaptive_threshold *= self.threshold_decay
                        if self.verbose >= 1:
                            print(f"ğŸ“‰ ì ì‘ì  ì„ê³„ê°’ ì¡°ì •: {self.adaptive_threshold:.3f}")
                
        except Exception as e:
            if self.verbose >= 1:
                print(f"âš ï¸ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í‰ê°€ ì˜¤ë¥˜: {e}")
    
    def _adaptive_increase_difficulty(self):
        """ì ì‘ì  ë‚œì´ë„ ì¦ê°€ (MathWorks ë°©ì‹)"""
        if self.curriculum_level < len(self.box_increments):
            new_boxes = self.box_increments[self.curriculum_level]
            
            if self.verbose >= 1:
                print(f"\nğŸ¯ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼: ë‚œì´ë„ ì¦ê°€!")
                print(f"   - ì´ì „ ë°•ìŠ¤ ìˆ˜: {self.current_boxes}")
                print(f"   - ìƒˆë¡œìš´ ë°•ìŠ¤ ìˆ˜: {new_boxes}")
                print(f"   - í˜„ì¬ ì„±ê³µë¥ : {self.last_success_rate:.1%}")
                print(f"   - ì•ˆì •ì„± ì ìˆ˜: {self.stability_scores[-1]:.3f}")
                print(f"   - ì—°ì† ì„±ê³µ: {self.consecutive_successes}")
                print(f"   - ì ì‘ì  ì„ê³„ê°’: {self.adaptive_threshold:.3f}")
            
            self.current_boxes = new_boxes
            self.curriculum_level += 1
            self.consecutive_successes = 0
            
            # ìƒˆë¡œìš´ ë‚œì´ë„ì—ì„œ ì¸¡ì • ì´ˆê¸°í™”
            self.recent_rewards = []
            self.recent_episode_lengths = []
            self.recent_utilizations = []
            
            # ì„ê³„ê°’ ì•½ê°„ ì¦ê°€ (ìƒˆë¡œìš´ ë‚œì´ë„ì— ë§ì¶°)
            self.adaptive_threshold = min(
                self.adaptive_threshold * 1.1, 
                self.success_threshold * 1.5
            )
    
    def _consider_backtrack(self):
        """ì„±ëŠ¥ ì €í•˜ì‹œ ë°±íŠ¸ë˜í‚¹ ê³ ë ¤ (YouTube ì‚¬ë¡€)"""
        if (self.curriculum_level > 0 and 
            self.backtrack_count < self.max_backtrack and
            len(self.performance_history) >= 10):
            
            # ìµœê·¼ ì„±ëŠ¥ì´ ì´ì „ ë ˆë²¨ë³´ë‹¤ í˜„ì €íˆ ë‚®ì€ì§€ í™•ì¸
            recent_performance = np.mean(self.performance_history[-5:])
            if len(self.performance_history) >= 15:
                prev_performance = np.mean(self.performance_history[-15:-10])
                if recent_performance < prev_performance * 0.7:  # 30% ì´ìƒ ì €í•˜
                    
                    if self.verbose >= 1:
                        print(f"\nâ¬‡ï¸ ì„±ëŠ¥ ì €í•˜ ê°ì§€: ë°±íŠ¸ë˜í‚¹ ì‹¤í–‰")
                        print(f"   - í˜„ì¬ ì„±ëŠ¥: {recent_performance:.3f}")
                        print(f"   - ì´ì „ ì„±ëŠ¥: {prev_performance:.3f}")
                        print(f"   - ë°±íŠ¸ë˜í‚¹ íšŸìˆ˜: {self.backtrack_count + 1}")
                    
                    # ì´ì „ ë ˆë²¨ë¡œ ë³µê·€
                    self.curriculum_level = max(0, self.curriculum_level - 1)
                    if self.curriculum_level == 0:
                        self.current_boxes = self.initial_boxes
                    else:
                        self.current_boxes = self.box_increments[self.curriculum_level - 1]
                    
                    self.backtrack_count += 1
                    self.consecutive_successes = 0
                    
                    # ì„ê³„ê°’ ë‚®ì¶°ì„œ ë” ì‰½ê²Œ ë§Œë“¤ê¸°
                    self.adaptive_threshold *= 0.9
    
    def get_adaptive_difficulty_info(self):
        """ì ì‘ì  ë‚œì´ë„ ì •ë³´ ë°˜í™˜"""
        return {
            "current_boxes": self.current_boxes,
            "curriculum_level": self.curriculum_level,
            "max_level": len(self.box_increments),
            "success_rate": self.last_success_rate,
            "adaptive_threshold": self.adaptive_threshold,
            "stability_score": self.stability_scores[-1] if self.stability_scores else 0.0,
            "performance_score": self.performance_history[-1] if self.performance_history else 0.0,
            "backtrack_count": self.backtrack_count,
            "consecutive_successes": self.consecutive_successes,
            "progress_percentage": (self.curriculum_level / len(self.box_increments)) * 100 if self.box_increments else 0,
        }

def create_ultimate_gif(model, env, timestamp):
    """í”„ë¦¬ë¯¸ì—„ í’ˆì§ˆ GIF ìƒì„± - ê¸°ì¡´ ê³ í’ˆì§ˆ GIFë“¤ê³¼ ë™ì¼í•œ ìˆ˜ì¤€"""
    print("ğŸ¬ í”„ë¦¬ë¯¸ì—„ í’ˆì§ˆ GIF ìƒì„± ì¤‘...")
    
    try:
        import matplotlib.pyplot as plt
        from matplotlib.patches import Rectangle
        from mpl_toolkits.mplot3d import Axes3D
        from mpl_toolkits.mplot3d.art3d import Poly3DCollection
        from PIL import Image
        import io
        import numpy as np
        
        # í™˜ê²½ ìƒíƒœ í™•ì¸
        if env is None:
            print("âŒ í™˜ê²½ì´ Noneì…ë‹ˆë‹¤")
            return None
            
        # GIF ì „ìš© ìƒˆë¡œìš´ í™˜ê²½ ìƒì„± (ì•ˆì „í•œ ë°©ë²•)
        print("ğŸ”§ GIF ì „ìš© í™˜ê²½ ìƒì„± ì¤‘...")
        try:
            # ì›ë³¸ í™˜ê²½ê³¼ ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ìƒˆ í™˜ê²½ ìƒì„±
            gif_env = make_env(
                container_size=[10, 10, 10],
                num_boxes=16,  # ê¸°ë³¸ê°’ ì‚¬ìš©
                num_visible_boxes=3,
                seed=42,
                render_mode=None,
                random_boxes=False,
                only_terminal_reward=False,
                improved_reward_shaping=True,
            )()
        except Exception as e:
            print(f"âŒ GIF í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
        
        frames = []
        
        try:
            obs, _ = gif_env.reset()
            print(f"âœ… GIF í™˜ê²½ ë¦¬ì…‹ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ GIF í™˜ê²½ ë¦¬ì…‹ ì‹¤íŒ¨: {e}")
            gif_env.close()
            return None
        
        # matplotlib ì„¤ì • (ê³ í’ˆì§ˆ)
        plt.ioff()  # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ ë¹„í™œì„±í™”
        plt.style.use('default')
        fig = plt.figure(figsize=(16, 12), facecolor='white')  # ë” í° í•´ìƒë„
        ax = fig.add_subplot(111, projection='3d')
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì„¤ì • (ë” ì˜ˆìœ ìƒ‰ìƒë“¤)
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', 
                  '#FF9FF3', '#54A0FF', '#5F27CD', '#00D2D3', '#FF9F43',
                  '#10AC84', '#EE5A24', '#0084FF', '#D63031', '#74B9FF',
                  '#A29BFE', '#6C5CE7', '#FD79A8', '#FDCB6E', '#E17055']
        
        print(f"ğŸ¬ í”„ë ˆì„ ìƒì„± ì‹œì‘ (ìµœëŒ€ 50 í”„ë ˆì„)")
        total_reward = 0
        
        for step in range(50):  # ë” ë§ì€ í”„ë ˆì„
            try:
                # í˜„ì¬ ìƒíƒœ ì‹œê°í™”
                ax.clear()
                
                # ì¶• ë²”ìœ„ ë° ë¼ë²¨ ì„¤ì •
                ax.set_xlim(0, 10)
                ax.set_ylim(0, 10)
                ax.set_zlim(0, 10)
                ax.set_xlabel('X-axis (Width)', fontsize=12, fontweight='bold')
                ax.set_ylabel('Y-axis (Depth)', fontsize=12, fontweight='bold')
                ax.set_zlabel('Z-axis (Height)', fontsize=12, fontweight='bold')
                
                # ì»¨í…Œì´ë„ˆ ì™¸ê³½ì„  ê·¸ë¦¬ê¸° (ë” ëª…í™•í•˜ê²Œ)
                container_edges = [
                    # ë°”ë‹¥ë©´
                    [(0,0,0), (10,0,0), (10,10,0), (0,10,0)],
                    # ìœ—ë©´
                    [(0,0,10), (10,0,10), (10,10,10), (0,10,10)],
                    # ì¸¡ë©´ë“¤
                    [(0,0,0), (0,0,10), (0,10,10), (0,10,0)],
                    [(10,0,0), (10,0,10), (10,10,10), (10,10,0)],
                    [(0,0,0), (10,0,0), (10,0,10), (0,0,10)],
                    [(0,10,0), (10,10,0), (10,10,10), (0,10,10)]
                ]
                
                # ì»¨í…Œì´ë„ˆ ì™¸ê³½ì„  ê·¸ë¦¬ê¸°
                for face in container_edges:
                    if face == container_edges[0]:  # ë°”ë‹¥ë©´ë§Œ ì±„ìš°ê¸°
                        poly = Poly3DCollection([face], alpha=0.1, facecolor='lightgray', edgecolor='black')
                        ax.add_collection3d(poly)
                    else:
                        poly = Poly3DCollection([face], alpha=0.02, facecolor='lightblue', edgecolor='gray')
                        ax.add_collection3d(poly)
                
                # ë°•ìŠ¤ë“¤ ê·¸ë¦¬ê¸° (í™˜ê²½ì—ì„œ ì •ë³´ ì¶”ì¶œ)
                box_count = 0
                utilization = 0
                
                try:
                    if hasattr(gif_env, 'unwrapped') and hasattr(gif_env.unwrapped, 'container'):
                        container = gif_env.unwrapped.container
                        for box in container.boxes:
                            if hasattr(box, 'position') and box.position is not None:
                                x, y, z = box.position
                                w, h, d = box.size
                                
                                # ë°•ìŠ¤ ìƒ‰ìƒ ì„ íƒ
                                color = colors[box_count % len(colors)]
                                
                                # 3D ë°•ìŠ¤ ê·¸ë¦¬ê¸° (6ë©´ ëª¨ë‘)
                                r = [0, w]
                                s = [0, h] 
                                t = [0, d]
                                
                                # ë°•ìŠ¤ì˜ 8ê°œ ê¼­ì§“ì  ê³„ì‚°
                                xx, yy, zz = np.meshgrid(r, s, t)
                                vertices = []
                                for i in range(2):
                                    for j in range(2):
                                        for k in range(2):
                                            vertices.append([x + xx[i,j,k], y + yy[i,j,k], z + zz[i,j,k]])
                                
                                # ë°•ìŠ¤ì˜ 6ê°œ ë©´ ì •ì˜
                                faces = [
                                    [vertices[0], vertices[1], vertices[3], vertices[2]],  # ì•ë©´
                                    [vertices[4], vertices[5], vertices[7], vertices[6]],  # ë’·ë©´
                                    [vertices[0], vertices[1], vertices[5], vertices[4]],  # ì•„ë˜ë©´
                                    [vertices[2], vertices[3], vertices[7], vertices[6]],  # ìœ—ë©´
                                    [vertices[0], vertices[2], vertices[6], vertices[4]],  # ì™¼ìª½ë©´
                                    [vertices[1], vertices[3], vertices[7], vertices[5]]   # ì˜¤ë¥¸ìª½ë©´
                                ]
                                
                                # ë©´ ì¶”ê°€
                                face_collection = Poly3DCollection(faces, alpha=0.8, facecolor=color, edgecolor='black', linewidth=1)
                                ax.add_collection3d(face_collection)
                                
                                # ë°•ìŠ¤ ë¼ë²¨ ì¶”ê°€
                                ax.text(x + w/2, y + h/2, z + d/2, f'{box_count+1}', 
                                       fontsize=10, fontweight='bold', ha='center', va='center')
                                
                                box_count += 1
                                utilization += w * h * d
                        
                        # í™œìš©ë¥  ê³„ì‚° (ì»¨í…Œì´ë„ˆ ë¶€í”¼: 10*10*10 = 1000)
                        utilization_percent = (utilization / 1000) * 100
                        
                except Exception as box_e:
                    print(f"âš ï¸ ë°•ìŠ¤ ë Œë”ë§ ì˜¤ë¥˜ (ìŠ¤í… {step}): {box_e}")
                
                # ì œëª© ë° ì •ë³´ í‘œì‹œ
                ax.set_title(f'Real-time Training Performance Monitoring\n'
                           f'Step: {step+1}/50 | Placed Boxes: {box_count} | Utilization: {utilization_percent:.1f}%', 
                           fontsize=14, fontweight='bold', pad=20)
                
                # ì¹´ë©”ë¼ ê°ë„ ì„¤ì • (ë” ì¢‹ì€ ì‹œì•¼ê°)
                ax.view_init(elev=25, azim=45 + step * 2)  # íšŒì „ íš¨ê³¼
                
                # ê·¸ë¦¬ë“œ ì„¤ì •
                ax.grid(True, alpha=0.3)
                ax.set_facecolor('white')
                
                # í”„ë ˆì„ ì €ì¥ (ê³ í’ˆì§ˆ)
                try:
                    buf = io.BytesIO()
                    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight', 
                              facecolor='white', edgecolor='none', pad_inches=0.1)
                    buf.seek(0)
                    frame = Image.open(buf).copy()
                    frames.append(frame)
                    buf.close()
                    
                    if step % 10 == 0:
                        print(f"  Frame {step + 1}/50 completed (Boxes: {box_count})")
                        
                except Exception as save_e:
                    print(f"âš ï¸ í”„ë ˆì„ ì €ì¥ ì˜¤ë¥˜ (ìŠ¤í… {step}): {save_e}")
                    continue
                
                # ë‹¤ìŒ ì•¡ì…˜ ìˆ˜í–‰ (ì•ˆì „í•œ ë°©ë²•)
                try:
                    action_masks = get_action_masks(gif_env)
                    action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
                    obs, reward, terminated, truncated, info = gif_env.step(action)
                    total_reward += reward
                    
                    if terminated or truncated:
                        print(f"  Episode ended (Step {step + 1}, Total Reward: {total_reward:.2f})")
                        # ë§ˆì§€ë§‰ í”„ë ˆì„ ëª‡ ê°œ ë” ì¶”ê°€ (ê²°ê³¼ í™•ì¸ìš©)
                        for _ in range(3):
                            frames.append(frame.copy())
                        break
                        
                except Exception as step_e:
                    print(f"âš ï¸ ì•¡ì…˜ ìˆ˜í–‰ ì˜¤ë¥˜ (ìŠ¤í… {step}): {step_e}")
                    break
                    
            except Exception as frame_e:
                print(f"âš ï¸ í”„ë ˆì„ {step} ìƒì„± ì¤‘ ì˜¤ë¥˜: {frame_e}")
                break
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        plt.close(fig)
        gif_env.close()
        
        # GIF ì €ì¥ (ê³ í’ˆì§ˆ)
        if len(frames) >= 5:  # ìµœì†Œ 5 í”„ë ˆì„ ì´ìƒ
            try:
                gif_path = f'gifs/ultimate_demo_{timestamp}.gif'
                os.makedirs('gifs', exist_ok=True)
                
                # í”„ë ˆì„ í¬ê¸° í†µì¼
                if frames:
                    base_size = frames[0].size
                    normalized_frames = []
                    for frame in frames:
                        if frame.size != base_size:
                            frame = frame.resize(base_size, Image.LANCZOS)
                        normalized_frames.append(frame)
                    
                    # ê³ í’ˆì§ˆ GIF ì €ì¥
                    normalized_frames[0].save(
                        gif_path,
                        save_all=True,
                        append_images=normalized_frames[1:],
                        duration=600,  # 0.6ì´ˆ ê°„ê²© (ë” ë¶€ë“œëŸ½ê²Œ)
                        loop=0,
                        optimize=True
                    )
                    
                    # íŒŒì¼ í¬ê¸° í™•ì¸
                    file_size = os.path.getsize(gif_path)
                    print(f"ğŸ¬ í”„ë¦¬ë¯¸ì—„ GIF ì €ì¥ ì™„ë£Œ: {gif_path}")
                    print(f"  ğŸ“Š í”„ë ˆì„ ìˆ˜: {len(normalized_frames)}")
                    print(f"  ğŸ“ íŒŒì¼ í¬ê¸°: {file_size / 1024:.1f} KB")
                    print(f"  ğŸ¯ ìµœì¢… ë³´ìƒ: {total_reward:.2f}")
                    print(f"  ğŸ“¦ ë°°ì¹˜ëœ ë°•ìŠ¤: {box_count}ê°œ")
                    
                    return gif_path
                
            except Exception as save_e:
                print(f"âŒ GIF íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {save_e}")
                return None
        else:
            print(f"âŒ ì¶©ë¶„í•œ í”„ë ˆì„ ì—†ìŒ ({len(frames)}ê°œ)")
            return None
            
    except Exception as e:
        print(f"âŒ GIF ìƒì„± ì „ì²´ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def ultimate_train(
    timesteps=15000,
    eval_freq=2000,
    container_size=[10, 10, 10],
    num_boxes=16,
    create_gif=True,
    curriculum_learning=True,
    initial_boxes=None,
    success_threshold=0.45,  # MathWorks: ë‚®ì€ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘
    curriculum_steps=7,      # ë” ë§ì€ ë‹¨ê³„
    patience=10              # ë” ê¸´ ì¸ë‚´ì‹¬
):
    """999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²°ëœ í•™ìŠµ í•¨ìˆ˜ (MathWorks ê¸°ë°˜ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì§€ì›)"""
    
    # MathWorks ê¶Œì¥: ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ ì„¤ì •
    if curriculum_learning and initial_boxes is None:
        # ì‹œì‘ì ì„ ëª©í‘œì˜ 60%ë¡œ ì„¤ì • (MathWorks ê¶Œì¥)
        initial_boxes = max(6, int(num_boxes * 0.6))  
    elif not curriculum_learning:
        initial_boxes = num_boxes
    
    current_boxes = initial_boxes
    
    print("ğŸš€ MathWorks ê¸°ë°˜ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì‹œì‘")
    print(f"ğŸ“‹ ì„¤ì •: {timesteps:,} ìŠ¤í…, í‰ê°€ ì£¼ê¸° {eval_freq:,}")
    if curriculum_learning:
        print(f"ğŸ“ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼: {initial_boxes}ê°œ â†’ {num_boxes}ê°œ ë°•ìŠ¤")
        print(f"   - ì„±ê³µ ì„ê³„ê°’: {success_threshold}")
        print(f"   - ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„: {curriculum_steps}")
        print(f"   - ì¸ë‚´ì‹¬: {patience}")
    else:
        print(f"ğŸ“¦ ê³ ì • ë°•ìŠ¤ ìˆ˜: {num_boxes}ê°œ")
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('results', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    os.makedirs('gifs', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    # í™˜ê²½ ìƒì„± (ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ê³ ë ¤)
    print("ğŸ—ï¸ í™˜ê²½ ìƒì„± ì¤‘...")
    env = make_env(
        container_size=container_size,
        num_boxes=current_boxes,  # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì‹œ ì´ˆê¸° ë°•ìŠ¤ ìˆ˜
        num_visible_boxes=3,
        seed=42,
        render_mode=None,
        random_boxes=False,
        only_terminal_reward=False,
        improved_reward_shaping=True,  # í•­ìƒ ê°œì„ ëœ ë³´ìƒ ì‚¬ìš©
    )()
    
    # í‰ê°€ìš© í™˜ê²½
    eval_env = make_env(
        container_size=container_size,
        num_boxes=current_boxes,  # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì‹œ ì´ˆê¸° ë°•ìŠ¤ ìˆ˜
        num_visible_boxes=3,
        seed=43,
        render_mode=None,
        random_boxes=False,
        only_terminal_reward=False,
        improved_reward_shaping=True,
    )()
    
    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    env = Monitor(env, f"logs/ultimate_train_{timestamp}.csv")
    eval_env = Monitor(eval_env, f"logs/ultimate_eval_{timestamp}.csv")
    
    print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    print(f"  - ì»¨í…Œì´ë„ˆ: {container_size}")
    print(f"  - í˜„ì¬ ë°•ìŠ¤ ìˆ˜: {current_boxes}")
    if curriculum_learning:
        print(f"  - ìµœì¢… ëª©í‘œ ë°•ìŠ¤ ìˆ˜: {num_boxes}")
    print(f"  - ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤: {env.action_space}")
    print(f"  - ê´€ì°° ìŠ¤í˜ì´ìŠ¤: {env.observation_space}")
    
    # ì•ˆì „í•œ ì½œë°± ì„¤ì • (ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ í¬í•¨)
    print("ğŸ›¡ï¸ ì•ˆì „í•œ ì½œë°± ì„¤ì • ì¤‘...")
    
    # í‰ê°€ ì£¼ê¸°ê°€ ì¶©ë¶„íˆ í° ê²½ìš°ì—ë§Œ ì½œë°± ì‚¬ìš©
    if eval_freq >= 2000:
        safe_callback = UltimateSafeCallback(eval_env, eval_freq=eval_freq)
        
        # ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì½œë°± ì¶”ê°€
        callbacks = [safe_callback]
        
        if curriculum_learning:
            curriculum_callback = AdaptiveCurriculumCallback(
                container_size=container_size,
                initial_boxes=initial_boxes,
                target_boxes=num_boxes,
                num_visible_boxes=3,
                success_threshold=success_threshold,
                curriculum_steps=curriculum_steps,
                patience=patience,
                stability_window=50,  # ì•ˆì •ì„± ìœˆë„ìš°
                verbose=1
            )
            callbacks.append(curriculum_callback)
            print(f"ğŸ“ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì½œë°± ì¶”ê°€")
        
        # ì²´í¬í¬ì¸íŠ¸ ì½œë°± (ì•ˆì „í•œ ì„¤ì •)
        checkpoint_callback = CheckpointCallback(
            save_freq=max(eval_freq, 3000),
            save_path="models/checkpoints",
            name_prefix=f"adaptive_model_{timestamp}",
            verbose=1
        )
        callbacks.append(checkpoint_callback)
        
        print(f"âœ… ì•ˆì „í•œ ì½œë°± í™œì„±í™” (í‰ê°€ ì£¼ê¸°: {eval_freq})")
        if curriculum_learning:
            print(f"   - ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ í™œì„±í™”")
            print(f"   - ì„±ê³µ ì„ê³„ê°’: {success_threshold}")
            print(f"   - ì•ˆì •ì„± ìœˆë„ìš°: 50")
    else:
        callbacks = None
        print("âš ï¸ ì½œë°± ë¹„í™œì„±í™” (í‰ê°€ ì£¼ê¸°ê°€ ë„ˆë¬´ ì§§ìŒ)")
        if curriculum_learning:
            print("âš ï¸ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµë„ ë¹„í™œì„±í™”ë¨")
    
    # MathWorks ê¶Œì¥ ìµœì í™”ëœ ëª¨ë¸ ìƒì„±
    print("ğŸ¤– MathWorks ê¸°ë°˜ ìµœì í™”ëœ ëª¨ë¸ ìƒì„± ì¤‘...")
    
    # TensorBoard ë¡œê·¸ ì„¤ì • (ì„ íƒì )
    tensorboard_log = None
    try:
        import tensorboard
        tensorboard_log = "logs/tensorboard"
        print("âœ… TensorBoard ë¡œê¹… í™œì„±í™”")
    except ImportError:
        print("âš ï¸ TensorBoard ì—†ìŒ - ë¡œê¹… ë¹„í™œì„±í™”")
    
    # MathWorks ê¸°ë°˜ ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    model = MaskablePPO(
        "MultiInputPolicy",
        env,
        # MathWorks ê¶Œì¥ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        learning_rate=lambda progress: max(1e-5, 3e-4 * (1 - progress * 0.9)),  
        n_steps=2048,        # ë” ë§ì€ ìŠ¤í…ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
        batch_size=256,      # ì ë‹¹í•œ ë°°ì¹˜ í¬ê¸°
        n_epochs=10,         # ì ë‹¹í•œ ì—í¬í¬
        gamma=0.99,          # í‘œì¤€ ê°ê°€ìœ¨
        gae_lambda=0.95,     # í‘œì¤€ GAE
        clip_range=0.2,      # í‘œì¤€ í´ë¦½ ë²”ìœ„
        clip_range_vf=None,  # VF í´ë¦½ ë¹„í™œì„±í™”
        ent_coef=0.01,       # ì ë‹¹í•œ ì—”íŠ¸ë¡œí”¼
        vf_coef=0.5,         # í‘œì¤€ ê°€ì¹˜ í•¨ìˆ˜ ê³„ìˆ˜
        max_grad_norm=0.5,   # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        verbose=1,
        tensorboard_log=tensorboard_log,
        seed=42,
        # MathWorks ê¶Œì¥ ì •ì±… kwargs
        policy_kwargs=dict(
            net_arch=[256, 256, 128],  # ì ë‹¹í•œ ë„¤íŠ¸ì›Œí¬ í¬ê¸°
            activation_fn=torch.nn.ReLU,
            share_features_extractor=True,
        )
    )
    
    print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ (MathWorks ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°)")
    print(f"  - í•™ìŠµë¥ : ì ì‘ì  ìŠ¤ì¼€ì¤„ë§ (3e-4 â†’ 3e-5)")
    print(f"  - n_steps: 2048")
    print(f"  - batch_size: 256")
    print(f"  - n_epochs: 10")
    print(f"  - ë„¤íŠ¸ì›Œí¬: [256, 256, 128]")
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í…")
    print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    
    try:
        # progress_bar ì„¤ì • (ì˜ì¡´ì„± í™•ì¸)
        use_progress_bar = False
        try:
            import tqdm
            import rich
            use_progress_bar = True
            print("âœ… ì§„í–‰ë¥  í‘œì‹œ í™œì„±í™”")
        except ImportError:
            print("âš ï¸ tqdm/rich ì—†ìŒ - ì§„í–‰ë¥  í‘œì‹œ ë¹„í™œì„±í™”")
        
        model.learn(
            total_timesteps=timesteps,
            callback=callbacks,
            progress_bar=use_progress_bar,
            tb_log_name=f"adaptive_ppo_{timestamp}",
        )
        
        training_time = time.time() - start_time
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {training_time:.2f}ì´ˆ")
        
        # ëª¨ë¸ ì €ì¥
        model_path = f"models/adaptive_ppo_{timestamp}"
        model.save(model_path)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")
        
        # ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ ê²°ê³¼ ì¶œë ¥
        if curriculum_learning and callbacks:
            for callback in callbacks:
                if isinstance(callback, AdaptiveCurriculumCallback):
                    difficulty_info = callback.get_adaptive_difficulty_info()
                    print(f"\nğŸ“ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ê²°ê³¼:")
                    print(f"   - ìµœì¢… ë°•ìŠ¤ ìˆ˜: {difficulty_info['current_boxes']}")
                    print(f"   - ì§„í–‰ë„: {difficulty_info['curriculum_level']}/{difficulty_info['max_level']}")
                    print(f"   - ìµœì¢… ì„±ê³µë¥ : {difficulty_info['success_rate']:.1%}")
                    print(f"   - ì•ˆì •ì„± ì ìˆ˜: {difficulty_info['stability_score']:.3f}")
                    print(f"   - ì„±ê³¼ ì ìˆ˜: {difficulty_info['performance_score']:.3f}")
                    print(f"   - ë°±íŠ¸ë˜í‚¹ íšŸìˆ˜: {difficulty_info['backtrack_count']}")
                    print(f"   - ì ì‘ì  ì„ê³„ê°’: {difficulty_info['adaptive_threshold']:.3f}")
                    break
        
        # ìµœì¢… í‰ê°€ (ì•ˆì „í•œ ë°©ì‹)
        print("\nğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...")
        final_rewards = []
        
        for ep in range(3):
            obs, _ = eval_env.reset()
            episode_reward = 0.0
            step_count = 0
            
            while step_count < 50:
                try:
                    action_masks = get_action_masks(eval_env)
                    action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    step_count += 1
                    
                    if terminated or truncated:
                        break
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    break
            
            final_rewards.append(episode_reward)
        
        final_reward = np.mean(final_rewards) if final_rewards else 0.0
        print(f"ğŸ“ˆ ìµœì¢… í‰ê°€ ë³´ìƒ: {final_reward:.4f}")
        
        # GIF ìƒì„±
        gif_path = None
        if create_gif:
            print("\nğŸ¬ GIF ìƒì„± ì¤‘...")
                gif_path = create_ultimate_gif(model, eval_env, timestamp)
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'timestamp': timestamp,
            'total_timesteps': timesteps,
            'training_time': training_time,
            'final_reward': final_reward,
            'container_size': container_size,
            'num_boxes': num_boxes,
            'model_path': model_path,
            'eval_freq': eval_freq,
            'callbacks_used': callbacks is not None,
            'curriculum_learning': curriculum_learning,
            'gif_path': gif_path,
        }
        
        # ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ ì •ë³´ ì¶”ê°€
        if curriculum_learning and callbacks:
            for callback in callbacks:
                if isinstance(callback, AdaptiveCurriculumCallback):
                    results['curriculum_info'] = callback.get_adaptive_difficulty_info()
                    break
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        results_file = f'results/adaptive_results_{timestamp}.txt'
        with open(results_file, 'w') as f:
            f.write("=== MathWorks ê¸°ë°˜ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ê²°ê³¼ ===\n")
            for key, value in results.items():
                if key != 'curriculum_info':
                    f.write(f"{key}: {value}\n")
            
            if 'curriculum_info' in results:
                f.write("\n=== ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ ìƒì„¸ ì •ë³´ ===\n")
                for key, value in results['curriculum_info'].items():
                    f.write(f"{key}: {value}\n")
        
        print(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {results_file}")
        
        return model, results
        
            except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# ===== í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í•¨ìˆ˜ë“¤ =====

def calculate_utilization_rate(env) -> float:
    """í™˜ê²½ì—ì„œ ì‹¤ì œ ê³µê°„ í™œìš©ë¥  ê³„ì‚°"""
    try:
        if hasattr(env, 'unwrapped') and hasattr(env.unwrapped, 'container'):
            container = env.unwrapped.container
            placed_volume = 0
            
            for box in container.boxes:
                if hasattr(box, 'position') and box.position is not None:
                    if hasattr(box, 'size'):
                        w, h, d = box.size
                        placed_volume += w * h * d
                    elif hasattr(box, 'volume'):
                        placed_volume += box.volume
            
            container_volume = container.size[0] * container.size[1] * container.size[2]
            return placed_volume / container_volume if container_volume > 0 else 0.0
        
        return 0.0
    except Exception as e:
        print(f"âš ï¸ í™œìš©ë¥  ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 0.0

def evaluate_model_performance(model, eval_env, n_episodes: int = 5) -> Tuple[float, float]:
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€: í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒê³¼ í™œìš©ë¥  ë°˜í™˜"""
    episode_rewards = []
    utilization_rates = []
    
    for ep in range(n_episodes):
        try:
            obs, _ = eval_env.reset()
            episode_reward = 0.0
            step_count = 0
            max_steps = 100
            
            while step_count < max_steps:
                try:
                    action_masks = get_action_masks(eval_env)
                    action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    step_count += 1
                    
                    if terminated or truncated:
                        break
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ìŠ¤í… ì˜¤ë¥˜: {e}")
                    break
            
            episode_rewards.append(episode_reward)
            
            # í™œìš©ë¥  ê³„ì‚°
            utilization = calculate_utilization_rate(eval_env)
            utilization_rates.append(utilization)
            
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ ì—í”¼ì†Œë“œ {ep} ì˜¤ë¥˜: {e}")
            episode_rewards.append(0.0)
            utilization_rates.append(0.0)
    
    mean_reward = np.mean(episode_rewards) if episode_rewards else 0.0
    mean_utilization = np.mean(utilization_rates) if utilization_rates else 0.0
    
    return mean_reward, mean_utilization

def create_hyperparameter_config(trial: 'optuna.Trial') -> Dict[str, Any]:
    """Optuna trialì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ìƒì„±"""
    hyperparams = {
        'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True),
        'n_steps': trial.suggest_categorical('n_steps', [1024, 2048, 4096]),
        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),
        'n_epochs': trial.suggest_int('n_epochs', 3, 15),
        'clip_range': trial.suggest_float('clip_range', 0.1, 0.4),
        'ent_coef': trial.suggest_float('ent_coef', 1e-4, 1e-1, log=True),
        'vf_coef': trial.suggest_float('vf_coef', 0.1, 1.0),
        'gae_lambda': trial.suggest_float('gae_lambda', 0.9, 0.99),
    }
    
    return hyperparams

def optuna_objective(trial: 'optuna.Trial', 
                    base_config: Dict[str, Any]) -> float:
    """Optuna ìµœì í™” ëª©ì  í•¨ìˆ˜"""
    
    print(f"\nğŸ”¬ Trial {trial.number} ì‹œì‘")
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒì„±
    hyperparams = create_hyperparameter_config(trial)
    
    print(f"ğŸ“‹ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for key, value in hyperparams.items():
        print(f"   - {key}: {value}")
    
    # W&B ë¡œê¹… ì„¤ì • (ì„ íƒì )
    run_name = f"trial_{trial.number}_{datetime.datetime.now().strftime('%H%M%S')}"
    
    if WANDB_AVAILABLE and base_config.get('use_wandb', False):
        wandb.init(
            project=base_config.get('wandb_project', 'ppo-3d-binpacking'),
            name=run_name,
            config=hyperparams,
            group="optuna-optimization",
            reinit=True
        )
    
    try:
        # í™˜ê²½ ìƒì„±
        container_size = base_config.get('container_size', [10, 10, 10])
        num_boxes = base_config.get('num_boxes', 16)
        
        env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=42 + trial.number,  # ê° trialë§ˆë‹¤ ë‹¤ë¥¸ ì‹œë“œ
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        
        eval_env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=43 + trial.number,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        
        # ëª¨ë‹ˆí„°ë§ ì„¤ì •
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        env = Monitor(env, f"logs/optuna_train_trial_{trial.number}_{timestamp}.csv")
        eval_env = Monitor(eval_env, f"logs/optuna_eval_trial_{trial.number}_{timestamp}.csv")
        
        # ëª¨ë¸ ìƒì„± (í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©)
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=hyperparams['learning_rate'],
            n_steps=hyperparams['n_steps'],
            batch_size=hyperparams['batch_size'],
            n_epochs=hyperparams['n_epochs'],
            gamma=0.99,
            gae_lambda=hyperparams['gae_lambda'],
            clip_range=hyperparams['clip_range'],
            clip_range_vf=None,
            ent_coef=hyperparams['ent_coef'],
            vf_coef=hyperparams['vf_coef'],
            max_grad_norm=0.5,
            verbose=0,  # ì¡°ìš©íˆ ì‹¤í–‰
            seed=42 + trial.number,
            policy_kwargs=dict(
                net_arch=[256, 256, 128],
                activation_fn=torch.nn.ReLU,
                share_features_extractor=True,
            )
        )
        
        # í•™ìŠµ (ì§§ì€ ì‹œê°„ìœ¼ë¡œ ì„¤ì •)
        timesteps = base_config.get('trial_timesteps', 5000)  # Trialìš© ì§§ì€ í•™ìŠµ
        
        # Pruningì„ ìœ„í•œ ì¤‘ê°„ í‰ê°€ ì½œë°±
        class OptunaPruningCallback(BaseCallback):
            def __init__(self, trial, eval_env, eval_freq=1000):
                super().__init__()
                self.trial = trial
                self.eval_env = eval_env
                self.eval_freq = eval_freq
                self.last_eval = 0
                
            def _on_step(self) -> bool:
                if self.num_timesteps - self.last_eval >= self.eval_freq:
                    # ì¤‘ê°„ í‰ê°€
                    mean_reward, mean_utilization = evaluate_model_performance(
                        self.model, self.eval_env, n_episodes=3
                    )
                    
                    # ë‹¤ì¤‘ ëª©ì  ìµœì í™”: ê°€ì¤‘ í•©ì‚°
                    # ë³´ìƒ * 0.3 + í™œìš©ë¥  * 0.7 (ê³µê°„ íš¨ìœ¨ì„± ìš°ì„ )
                    combined_score = mean_reward * 0.3 + mean_utilization * 100 * 0.7
                    
                    # Optunaì— ì¤‘ê°„ ê²°ê³¼ ë³´ê³ 
                    self.trial.report(combined_score, self.num_timesteps)
                    
                    # Pruning ì²´í¬
                    if self.trial.should_prune():
                        print(f"ğŸ”ª Trial {self.trial.number} pruned at step {self.num_timesteps}")
                        raise optuna.TrialPruned()
                    
                    self.last_eval = self.num_timesteps
                    
                return True
        
        # Pruning ì½œë°± ì„¤ì •
        pruning_callback = OptunaPruningCallback(trial, eval_env)
        
        print(f"ğŸš€ í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í…")
        start_time = time.time()
        
        model.learn(
            total_timesteps=timesteps,
            callback=pruning_callback,
            progress_bar=False
        )
        
        training_time = time.time() - start_time
        
        # ìµœì¢… í‰ê°€
        print(f"ğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...")
        mean_reward, mean_utilization = evaluate_model_performance(
            model, eval_env, n_episodes=5
        )
        
        # ë‹¤ì¤‘ ëª©ì  ìµœì í™”: ê°€ì¤‘ í•©ì‚°
        # ë³´ìƒ * 0.3 + í™œìš©ë¥  * 0.7 (ê³µê°„ íš¨ìœ¨ì„± ìš°ì„ )
        combined_score = mean_reward * 0.3 + mean_utilization * 100 * 0.7
        
        print(f"âœ… Trial {trial.number} ì™„ë£Œ:")
        print(f"   - í‰ê·  ë³´ìƒ: {mean_reward:.4f}")
        print(f"   - í‰ê·  í™œìš©ë¥ : {mean_utilization:.1%}")
        print(f"   - ì¢…í•© ì ìˆ˜: {combined_score:.4f}")
        print(f"   - í•™ìŠµ ì‹œê°„: {training_time:.1f}ì´ˆ")
        
        # W&B ë¡œê¹…
        if WANDB_AVAILABLE and base_config.get('use_wandb', False):
            wandb.log({
                "mean_episode_reward": mean_reward,
                "mean_utilization_rate": mean_utilization,
                "combined_score": combined_score,
                "training_time": training_time,
                **hyperparams
            })
            wandb.finish()
        
        # í™˜ê²½ ì •ë¦¬
        env.close()
        eval_env.close()
        
        return combined_score
        
    except optuna.TrialPruned:
        print(f"ğŸ”ª Trial {trial.number} was pruned")
        if WANDB_AVAILABLE and base_config.get('use_wandb', False):
            wandb.finish()
        raise
        
    except Exception as e:
        print(f"âŒ Trial {trial.number} ì˜¤ë¥˜: {e}")
        if WANDB_AVAILABLE and base_config.get('use_wandb', False):
            wandb.finish()
        
        # ì˜¤ë¥˜ ì‹œ ë‚®ì€ ì ìˆ˜ ë°˜í™˜ (ìµœì í™”ê°€ ê³„ì†ë˜ë„ë¡)
        return -1000.0

def run_optuna_optimization(
    n_trials: int = 50,
    container_size: list = [10, 10, 10],
    num_boxes: int = 16,
    trial_timesteps: int = 5000,
    use_wandb: bool = False,
    wandb_project: str = "ppo-3d-binpacking-optuna",
    study_name: str = None
) -> Dict[str, Any]:
    """Optunaë¥¼ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰"""
    
    if not OPTUNA_AVAILABLE:
        raise ImportError("Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install optuna")
    
    print("ğŸ”¬ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
    print(f"ğŸ“‹ ì„¤ì •:")
    print(f"   - ì‹œí–‰ íšŸìˆ˜: {n_trials}")
    print(f"   - ì»¨í…Œì´ë„ˆ í¬ê¸°: {container_size}")
    print(f"   - ë°•ìŠ¤ ê°œìˆ˜: {num_boxes}")
    print(f"   - Trial í•™ìŠµ ìŠ¤í…: {trial_timesteps:,}")
    print(f"   - W&B ì‚¬ìš©: {use_wandb and WANDB_AVAILABLE}")
    
    # ê¸°ë³¸ ì„¤ì •
    base_config = {
        'container_size': container_size,
        'num_boxes': num_boxes,
        'trial_timesteps': trial_timesteps,
        'use_wandb': use_wandb and WANDB_AVAILABLE,
        'wandb_project': wandb_project
    }
    
    # Study ìƒì„±
    study_name = study_name or f"ppo_optimization_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # W&B ì½œë°± ì„¤ì •
    callbacks = []
    if use_wandb and WANDB_AVAILABLE:
        try:
            wandb_callback = WeightsAndBiasesCallback(
                metric_name="combined_score",
                wandb_kwargs={
                    "project": wandb_project,
                    "group": "optuna-study"
                }
            )
            callbacks.append(wandb_callback)
            print("âœ… W&B ì½œë°± ì¶”ê°€ë¨")
        except Exception as e:
            print(f"âš ï¸ W&B ì½œë°± ì„¤ì • ì˜¤ë¥˜: {e}")
    
    # Study ìƒì„± (TPE + MedianPruner)
    study = optuna.create_study(
        study_name=study_name,
        direction="maximize",  # combined_score ìµœëŒ€í™”
        sampler=optuna.samplers.TPESampler(seed=42, multivariate=True),
        pruner=optuna.pruners.MedianPruner(
            n_startup_trials=5,
            n_warmup_steps=1000,
            interval_steps=500
        )
    )
    
    print(f"ğŸ“Š Study ìƒì„± ì™„ë£Œ: {study_name}")
    
    # ìµœì í™” ì‹¤í–‰
    try:
        start_time = time.time()
        
        study.optimize(
            lambda trial: optuna_objective(trial, base_config),
            n_trials=n_trials,
            callbacks=callbacks
        )
        
        total_time = time.time() - start_time
        
        print(f"\nğŸ‰ ìµœì í™” ì™„ë£Œ!")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥: {study.best_value:.4f}")
        print(f"ğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        
        for key, value in study.best_params.items():
            print(f"   - {key}: {value}")
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        results = {
            "study_name": study_name,
            "best_value": study.best_value,
            "best_params": study.best_params,
            "n_trials": len(study.trials),
            "optimization_time": total_time,
            "timestamp": timestamp,
            "config": base_config
        }
        
        results_file = f"results/optuna_results_{timestamp}.json"
        os.makedirs('results', exist_ok=True)
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # ì‹œê°í™” ìƒì„±
        try:
            print("ğŸ“Š ìµœì í™” ê²°ê³¼ ì‹œê°í™” ì¤‘...")
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('Optuna Optimization Results', fontsize=16)
            
            # 1. ìµœì í™” íˆìŠ¤í† ë¦¬
            optuna.visualization.matplotlib.plot_optimization_history(study, ax=axes[0, 0])
            axes[0, 0].set_title('Optimization History')
            
            # 2. íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„
            try:
                optuna.visualization.matplotlib.plot_param_importances(study, ax=axes[0, 1])
                axes[0, 1].set_title('Parameter Importances')
            except Exception:
                axes[0, 1].text(0.5, 0.5, 'Not enough trials\nfor importance analysis', 
                               ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('Parameter Importances')
            
            # 3. ë³‘ë ¬ ì¢Œí‘œ í”Œë¡¯ (ìƒìœ„ trialsë§Œ)
            try:
                if len(study.trials) >= 10:
                    optuna.visualization.matplotlib.plot_parallel_coordinate(
                        study, params=['learning_rate', 'n_epochs', 'clip_range'], ax=axes[1, 0]
                    )
                else:
                    axes[1, 0].text(0.5, 0.5, 'Not enough trials\nfor parallel coordinate', 
                                   ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Parallel Coordinate Plot')
            except Exception:
                axes[1, 0].text(0.5, 0.5, 'Parallel coordinate\nplot failed', 
                               ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Parallel Coordinate Plot')
            
            # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìŠ¬ë¼ì´ìŠ¤ í”Œë¡¯
            try:
                optuna.visualization.matplotlib.plot_slice(
                    study, params=['learning_rate', 'clip_range'], ax=axes[1, 1]
                )
                axes[1, 1].set_title('Hyperparameter Slice Plot')
            except Exception:
                axes[1, 1].text(0.5, 0.5, 'Slice plot\nnot available', 
                               ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('Hyperparameter Slice Plot')
            
            plt.tight_layout()
            
            viz_file = f"results/optuna_visualization_{timestamp}.png"
            plt.savefig(viz_file, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {viz_file}")
            
        except Exception as e:
            print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")
        
        return results
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ìµœì í™” ì¤‘ë‹¨ë¨")
        return {"status": "interrupted", "n_completed_trials": len(study.trials)}
    
    except Exception as e:
        print(f"\nâŒ ìµœì í™” ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {"status": "error", "error": str(e)}

def create_wandb_sweep_config() -> Dict[str, Any]:
    """W&B Sweep ì„¤ì • ìƒì„±"""
    sweep_config = {
        "method": "bayes",  # bayes, grid, random
        "metric": {
            "goal": "maximize",
            "name": "combined_score"
        },
        "parameters": {
            "learning_rate": {
                "distribution": "log_uniform_values",
                "min": 1e-6,
                "max": 1e-3
            },
            "n_steps": {
                "values": [1024, 2048, 4096]
            },
            "batch_size": {
                "values": [64, 128, 256]
            },
            "n_epochs": {
                "distribution": "int_uniform",
                "min": 3,
                "max": 15
            },
            "clip_range": {
                "distribution": "uniform",
                "min": 0.1,
                "max": 0.4
            },
            "ent_coef": {
                "distribution": "log_uniform_values",
                "min": 1e-4,
                "max": 1e-1
            },
            "vf_coef": {
                "distribution": "uniform",
                "min": 0.1,
                "max": 1.0
            },
            "gae_lambda": {
                "distribution": "uniform",
                "min": 0.9,
                "max": 0.99
            }
        }
    }
    return sweep_config

def wandb_sweep_train():
    """W&B Sweepì—ì„œ ì‹¤í–‰ë˜ëŠ” í•™ìŠµ í•¨ìˆ˜"""
    if not WANDB_AVAILABLE:
        raise ImportError("W&Bê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install wandb")
    
    # W&B run ì´ˆê¸°í™”
    with wandb.init() as run:
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        config = wandb.config
        
        # í™˜ê²½ ìƒì„±
        container_size = [10, 10, 10]
        num_boxes = 16
        
        env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=42,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        
        eval_env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=43,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        
        # ëª¨ë‹ˆí„°ë§ ì„¤ì •
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        env = Monitor(env, f"logs/wandb_train_{run.id}_{timestamp}.csv")
        eval_env = Monitor(eval_env, f"logs/wandb_eval_{run.id}_{timestamp}.csv")
        
        # ëª¨ë¸ ìƒì„±
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=config.learning_rate,
            n_steps=config.n_steps,
            batch_size=config.batch_size,
            n_epochs=config.n_epochs,
            gamma=0.99,
            gae_lambda=config.gae_lambda,
            clip_range=config.clip_range,
            ent_coef=config.ent_coef,
            vf_coef=config.vf_coef,
            max_grad_norm=0.5,
            verbose=0,
            policy_kwargs=dict(
                net_arch=[256, 256, 128],
                activation_fn=torch.nn.ReLU,
                share_features_extractor=True,
            )
        )
        
        # í•™ìŠµ
        timesteps = 8000
        model.learn(total_timesteps=timesteps, progress_bar=False)
        
        # í‰ê°€
        mean_reward, mean_utilization = evaluate_model_performance(
            model, eval_env, n_episodes=5
        )
        
        # ë‹¤ì¤‘ ëª©ì  ìµœì í™” ì ìˆ˜ ê³„ì‚°
        combined_score = mean_reward * 0.3 + mean_utilization * 100 * 0.7
        
        # W&B ë¡œê¹…
        wandb.log({
            "mean_episode_reward": mean_reward,
            "mean_utilization_rate": mean_utilization,
            "combined_score": combined_score,
            "learning_rate": config.learning_rate,
            "n_steps": config.n_steps,
            "batch_size": config.batch_size,
            "n_epochs": config.n_epochs,
            "clip_range": config.clip_range,
            "ent_coef": config.ent_coef,
            "vf_coef": config.vf_coef,
            "gae_lambda": config.gae_lambda
        })
        
        # í™˜ê²½ ì •ë¦¬
        env.close()
        eval_env.close()

def run_wandb_sweep(
    n_trials: int = 50,
    wandb_project: str = "ppo-3d-binpacking-sweep"
) -> Dict[str, Any]:
    """W&B Sweep ì‹¤í–‰"""
    
    if not WANDB_AVAILABLE:
        raise ImportError("W&Bê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install wandb")
    
    print("ğŸŒŠ W&B Sweep í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
    
    # Sweep ì„¤ì •
    sweep_config = create_wandb_sweep_config()
    
    # Sweep ìƒì„±
    sweep_id = wandb.sweep(sweep_config, project=wandb_project)
    
    print(f"ğŸ“Š Sweep ìƒì„± ì™„ë£Œ: {sweep_id}")
    print(f"ğŸ”— W&B ëŒ€ì‹œë³´ë“œ: https://wandb.ai/{wandb.api.default_entity}/{wandb_project}/sweeps/{sweep_id}")
    
    # Agent ì‹¤í–‰
    wandb.agent(sweep_id, wandb_sweep_train, count=n_trials)
    
    return {
        "status": "completed",
        "sweep_id": sweep_id,
        "n_trials": n_trials,
        "project": wandb_project
    }

def run_hyperparameter_optimization(
    method: str = "optuna",
    n_trials: int = 50,
    container_size: list = [10, 10, 10],
    num_boxes: int = 16,
    trial_timesteps: int = 8000,
    use_wandb: bool = False,
    wandb_project: str = "ppo-3d-binpacking-optimization"
) -> Dict[str, Any]:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í†µí•© í•¨ìˆ˜"""
    
    print(f"ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ - ë°©ë²•: {method}")
    
    results = {}
    
    if method == "optuna":
        if not OPTUNA_AVAILABLE:
            raise ImportError("Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install optuna")
        
        results = run_optuna_optimization(
            n_trials=n_trials,
            container_size=container_size,
            num_boxes=num_boxes,
            trial_timesteps=trial_timesteps,
            use_wandb=use_wandb,
            wandb_project=wandb_project
        )
        
    elif method == "wandb":
        if not WANDB_AVAILABLE:
            raise ImportError("W&Bê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install wandb")
        
        results = run_wandb_sweep(
            n_trials=n_trials,
            wandb_project=wandb_project
        )
        
    elif method == "both":
        if not OPTUNA_AVAILABLE or not WANDB_AVAILABLE:
            missing = []
            if not OPTUNA_AVAILABLE:
                missing.append("optuna")
            if not WANDB_AVAILABLE:
                missing.append("wandb")
            raise ImportError(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install {' '.join(missing)}")
        
        print("ğŸ“Š 1ë‹¨ê³„: Optuna ìµœì í™”")
        optuna_results = run_optuna_optimization(
            n_trials=n_trials//2,
            container_size=container_size,
            num_boxes=num_boxes,
            trial_timesteps=trial_timesteps,
            use_wandb=use_wandb,
            wandb_project=f"{wandb_project}-optuna"
        )
        
        print("ğŸŒŠ 2ë‹¨ê³„: W&B Sweep ìµœì í™”")
        wandb_results = run_wandb_sweep(
            n_trials=n_trials//2,
            wandb_project=f"{wandb_project}-sweep"
        )
        
        results = {
            "method": "both",
            "optuna_results": optuna_results,
            "wandb_results": wandb_results
        }
        
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìµœì í™” ë°©ë²•: {method}. 'optuna', 'wandb', 'both' ì¤‘ ì„ íƒí•˜ì„¸ìš”.")
    
    print(f"âœ… {method} ìµœì í™” ì™„ë£Œ!")
    return results

def train_with_best_params(results_file: str, 
                          timesteps: int = 50000,
                          create_gif: bool = True) -> Tuple[Any, Dict]:
    """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ"""
    
    print(f"ğŸ† ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ ì‹œì‘")
    
    # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
    with open(results_file, 'r') as f:
        optuna_results = json.load(f)
    
    best_params = optuna_results['best_params']
    config = optuna_results['config']
    
    print(f"ğŸ“‹ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for key, value in best_params.items():
        print(f"   - {key}: {value}")
    
    # ultimate_train í•¨ìˆ˜ í˜¸ì¶œ (ìµœì  íŒŒë¼ë¯¸í„° ì ìš©)
    # ultimate_train í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•¨
    
    # ì„ì‹œë¡œ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
    model, results = ultimate_train(
        timesteps=timesteps,
        eval_freq=2000,
        container_size=config['container_size'],
        num_boxes=config['num_boxes'],
        create_gif=create_gif,
        curriculum_learning=False  # ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë¹„í™œì„±í™”
    )
    
    if results:
        results['optuna_optimization'] = optuna_results
        print(f"ğŸ‰ ìµœì¢… í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ë³´ìƒ: {results['final_reward']:.4f}")
    
    return model, results

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="MathWorks ê¸°ë°˜ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ + 999 ìŠ¤í… ë¬¸ì œ í•´ê²° + Optuna/W&B í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    parser.add_argument("--timesteps", type=int, default=15000, help="ì´ í•™ìŠµ ìŠ¤í… ìˆ˜")
    parser.add_argument("--eval-freq", type=int, default=2000, help="í‰ê°€ ì£¼ê¸°")
    parser.add_argument("--container-size", nargs=3, type=int, default=[10, 10, 10], help="ì»¨í…Œì´ë„ˆ í¬ê¸°")
    parser.add_argument("--num-boxes", type=int, default=16, help="ëª©í‘œ ë°•ìŠ¤ ê°œìˆ˜")
    parser.add_argument("--no-gif", action="store_true", help="GIF ìƒì„± ì•ˆí•¨")
    
    # MathWorks ê¸°ë°˜ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì˜µì…˜
    parser.add_argument("--curriculum-learning", action="store_true", default=True, 
                        help="ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ í™œì„±í™” (ê¸°ë³¸ê°’: True)")
    parser.add_argument("--no-curriculum", action="store_true", 
                        help="ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ë¹„í™œì„±í™”")
    parser.add_argument("--initial-boxes", type=int, default=None, 
                        help="ì‹œì‘ ë°•ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: ëª©í‘œì˜ 60%)")
    parser.add_argument("--success-threshold", type=float, default=0.45, 
                        help="ì„±ê³µ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.45, MathWorks ê¶Œì¥)")
    parser.add_argument("--curriculum-steps", type=int, default=7, 
                        help="ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ ìˆ˜ (ê¸°ë³¸ê°’: 7)")
    parser.add_argument("--patience", type=int, default=10, 
                        help="ë‚œì´ë„ ì¦ê°€ ëŒ€ê¸° íšŸìˆ˜ (ê¸°ë³¸ê°’: 10)")
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜µì…˜
    hyperopt_group = parser.add_argument_group('í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜µì…˜')
    hyperopt_group.add_argument("--optimize", action="store_true", 
                               help="í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰")
    hyperopt_group.add_argument("--optimization-method", type=str, 
                               choices=["optuna", "wandb", "both"], 
                               default="optuna",
                               help="ìµœì í™” ë°©ë²• (ê¸°ë³¸ê°’: optuna)")
    hyperopt_group.add_argument("--n-trials", type=int, default=50,
                               help="ìµœì í™” ì‹œí–‰ íšŸìˆ˜ (ê¸°ë³¸ê°’: 50)")
    hyperopt_group.add_argument("--trial-timesteps", type=int, default=8000,
                               help="ê° trialì˜ í•™ìŠµ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 8000)")
    hyperopt_group.add_argument("--use-wandb", action="store_true",
                               help="W&B ë¡œê¹… í™œì„±í™”")
    hyperopt_group.add_argument("--wandb-project", type=str, 
                               default="ppo-3d-binpacking-optimization",
                               help="W&B í”„ë¡œì íŠ¸ ì´ë¦„")
    hyperopt_group.add_argument("--train-with-best", type=str, default=None,
                               help="ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ (Optuna ê²°ê³¼ íŒŒì¼ ê²½ë¡œ)")
    
    args = parser.parse_args()
    
    # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì„¤ì •
    curriculum_learning = args.curriculum_learning and not args.no_curriculum
    
    print("ğŸš€ MathWorks ê¸°ë°˜ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ + 999 ìŠ¤í… ë¬¸ì œ í•´ê²° + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    print("=" * 100)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰
    if args.optimize:
        print("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª¨ë“œ")
        print(f"   - ë°©ë²•: {args.optimization_method}")
        print(f"   - ì‹œí–‰ íšŸìˆ˜: {args.n_trials}")
        print(f"   - Trial ìŠ¤í…: {args.trial_timesteps:,}")
        print(f"   - W&B ì‚¬ìš©: {args.use_wandb}")
        print(f"   - W&B í”„ë¡œì íŠ¸: {args.wandb_project}")
        
        try:
            optimization_results = run_hyperparameter_optimization(
                method=args.optimization_method,
                n_trials=args.n_trials,
                container_size=args.container_size,
                num_boxes=args.num_boxes,
                use_wandb=args.use_wandb,
                wandb_project=args.wandb_project
            )
            
            print("\nğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì„±ê³µ!")
            print("ğŸ” ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œ ìµœì¢… í•™ìŠµì„ ì›í•œë‹¤ë©´:")
            
            if "optuna" in optimization_results and "best_params" in optimization_results["optuna"]:
                timestamp = optimization_results["optuna"]["timestamp"]
                results_file = f"results/optuna_results_{timestamp}.json"
                print(f"   python src/ultimate_train_fix.py --train-with-best {results_file}")
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ìµœì í™” ì¤‘ë‹¨ë¨")
        except Exception as e:
            print(f"\nâŒ ìµœì í™” ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ
    elif args.train_with_best:
        print("ğŸ† ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ ëª¨ë“œ")
        print(f"   - ê²°ê³¼ íŒŒì¼: {args.train_with_best}")
        print(f"   - í•™ìŠµ ìŠ¤í…: {args.timesteps:,}")
        
        try:
            model, results = train_with_best_params(
                results_file=args.train_with_best,
                timesteps=args.timesteps,
                create_gif=not args.no_gif
            )
            
            if results:
                print("\nğŸ‰ ìµœì¢… í•™ìŠµ ì„±ê³µ!")
                print(f"ğŸ“Š ìµœì¢… ë³´ìƒ: {results['final_reward']:.4f}")
                print(f"ğŸ’¾ ëª¨ë¸ ê²½ë¡œ: {results['model_path']}")
            
        except Exception as e:
            print(f"\nâŒ ìµœì¢… í•™ìŠµ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    # ì¼ë°˜ í•™ìŠµ ëª¨ë“œ
    else:
        if curriculum_learning:
            print("ğŸ“ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ëª¨ë“œ í™œì„±í™” (MathWorks ê¸°ë°˜)")
            print(f"   - ì„±ê³µ ì„ê³„ê°’: {args.success_threshold} (ë‚®ì€ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘)")
            print(f"   - ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„: {args.curriculum_steps} (ë” ë§ì€ ë‹¨ê³„)")
            print(f"   - ì¸ë‚´ì‹¬: {args.patience} (ë” ê¸´ ëŒ€ê¸°)")
            print(f"   - ì‹œì‘ ë°•ìŠ¤ ìˆ˜: {args.initial_boxes or f'ëª©í‘œì˜ 60% ({int(args.num_boxes * 0.6)}ê°œ)'}")
            print(f"   âœ¨ íŠ¹ì§•: ë‹¤ì¤‘ ì§€í‘œ í‰ê°€, ì ì‘ì  ì„ê³„ê°’ ì¡°ì •, ë°±íŠ¸ë˜í‚¹")
        else:
            print("ğŸ“¦ ê³ ì • ë‚œì´ë„ ëª¨ë“œ")
        
        print(f"ğŸ“‹ í•™ìŠµ ì„¤ì •:")
        print(f"   - ì´ ìŠ¤í…: {args.timesteps:,}")
        print(f"   - í‰ê°€ ì£¼ê¸°: {args.eval_freq:,}")
        print(f"   - ì»¨í…Œì´ë„ˆ í¬ê¸°: {args.container_size}")
        print(f"   - ëª©í‘œ ë°•ìŠ¤ ìˆ˜: {args.num_boxes}")
        print(f"   - GIF ìƒì„±: {'ë¹„í™œì„±í™”' if args.no_gif else 'í™œì„±í™”'}")
        
        try:
    model, results = ultimate_train(
        timesteps=args.timesteps,
        eval_freq=args.eval_freq,
                container_size=args.container_size,
        num_boxes=args.num_boxes,
                create_gif=not args.no_gif,
                curriculum_learning=curriculum_learning,
                initial_boxes=args.initial_boxes,
                success_threshold=args.success_threshold,
                curriculum_steps=args.curriculum_steps,
                patience=args.patience
    )
    
    if results:
        print("\nğŸ‰ í•™ìŠµ ì„±ê³µ!")
        print(f"ğŸ“Š ìµœì¢… ë³´ìƒ: {results['final_reward']:.4f}")
        print(f"â±ï¸ ì†Œìš” ì‹œê°„: {results['training_time']:.2f}ì´ˆ")
        print(f"ğŸ’¾ ëª¨ë¸ ê²½ë¡œ: {results['model_path']}")
                
                # ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ê²°ê³¼ ì¶œë ¥
                if curriculum_learning and 'curriculum_info' in results:
                    curriculum_info = results['curriculum_info']
                    print(f"\nğŸ“ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ìƒì„¸ ê²°ê³¼:")
                    print(f"   - ìµœì¢… ë°•ìŠ¤ ìˆ˜: {curriculum_info['current_boxes']}")
                    print(f"   - ì§„í–‰ë„: {curriculum_info['curriculum_level']}/{curriculum_info['max_level']}")
                    print(f"   - ìµœì¢… ì„±ê³µë¥ : {curriculum_info['success_rate']:.1%}")
                    print(f"   - ì•ˆì •ì„± ì ìˆ˜: {curriculum_info['stability_score']:.3f}")
                    print(f"   - ì„±ê³¼ ì ìˆ˜: {curriculum_info['performance_score']:.3f}")
                    print(f"   - ë°±íŠ¸ë˜í‚¹ íšŸìˆ˜: {curriculum_info['backtrack_count']}")
                    print(f"   - ì ì‘ì  ì„ê³„ê°’: {curriculum_info['adaptive_threshold']:.3f}")
                    
                    # ì„±ê³¼ ë“±ê¸‰ íŒì •
                    performance_score = curriculum_info['performance_score']
                    if performance_score >= 0.8:
                        grade = "ğŸ† Sê¸‰ (íƒì›”í•¨)"
                    elif performance_score >= 0.7:
                        grade = "ğŸ¥‡ Aê¸‰ (ìš°ìˆ˜í•¨)"
                    elif performance_score >= 0.6:
                        grade = "ğŸ¥ˆ Bê¸‰ (ë³´í†µ)"
                    elif performance_score >= 0.5:
                        grade = "ğŸ¥‰ Cê¸‰ (ê°œì„  í•„ìš”)"
                    else:
                        grade = "ğŸ“ˆ Dê¸‰ (ì¶”ê°€ í•™ìŠµ í•„ìš”)"
                    
                    print(f"   - ì„±ê³¼ ë“±ê¸‰: {grade}")
                    
                    # ì»¤ë¦¬í˜ëŸ¼ ì™„ë£Œ ì—¬ë¶€
                    progress = curriculum_info['progress_percentage']
                    if progress >= 100:
                        print(f"   âœ… ì»¤ë¦¬í˜ëŸ¼ ì™„ë£Œ: ëª©í‘œ ë‹¬ì„±!")
                    elif progress >= 80:
                        print(f"   ğŸ”¥ ì»¤ë¦¬í˜ëŸ¼ ê±°ì˜ ì™„ë£Œ: {progress:.1f}%")
                    elif progress >= 50:
                        print(f"   ğŸ’ª ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰ ì¤‘: {progress:.1f}%")
                    else:
                        print(f"   ğŸŒ± ì»¤ë¦¬í˜ëŸ¼ ì´ˆê¸° ë‹¨ê³„: {progress:.1f}%")
                        
    else:
        print("\nâŒ í•™ìŠµ ì‹¤íŒ¨") 
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        except Exception as e:
            print(f"\nâŒ ì „ì²´ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    print("\nğŸ¯ ê°œì„  ì‚¬í•­ ìš”ì•½:")
    print("âœ¨ MathWorks ê¸°ë°˜ ì ì‘ì  ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ")
    print("âœ¨ ì ì‘ì  ì„ê³„ê°’ ì¡°ì • (ì„±ëŠ¥ì— ë”°ë¼ ë™ì  ë³€í™”)")
    print("âœ¨ ë‹¤ì¤‘ ì§€í‘œ í‰ê°€ (ì„±ê³µë¥  + ì•ˆì •ì„± + ê°œì„ ë„)")
    print("âœ¨ ë°±íŠ¸ë˜í‚¹ ê¸°ëŠ¥ (ì„±ëŠ¥ ì €í•˜ ì‹œ ì´ì „ ë‹¨ê³„ë¡œ)")
    print("âœ¨ ì•ˆì •ì„± ì¤‘ì‹¬ ë‚œì´ë„ ì¦ê°€ (ì¶©ë¶„í•œ ì•ˆì •ì„± í™•ë³´ í›„ ì§„í–‰)")
    print("âœ¨ ì ì‘ì  í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (í•™ìŠµ ì§„í–‰ì— ë”°ë¼ ê°ì†Œ)")
    print("âœ¨ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (TPE + Pruning)")
    print("âœ¨ W&B Sweep ë² ì´ì§€ì•ˆ ìµœì í™”")
    print("âœ¨ ë‹¤ì¤‘ ëª©ì  ìµœì í™” (ë³´ìƒ + í™œìš©ë¥ )")
    print("âœ¨ ì¢…í•© ì„±ê³¼ í‰ê°€ ì‹œìŠ¤í…œ (S~D ë“±ê¸‰)")
    
    if OPTUNA_AVAILABLE or WANDB_AVAILABLE:
        print("\nğŸ”¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‚¬ìš©ë²•:")
        if OPTUNA_AVAILABLE:
            print("   # Optunaë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
            print("   python src/ultimate_train_fix.py --optimize --optimization-method optuna --n-trials 30")
        if WANDB_AVAILABLE:
            print("   # W&B Sweepìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
            print("   python src/ultimate_train_fix.py --optimize --optimization-method wandb --use-wandb --n-trials 30")
        if OPTUNA_AVAILABLE and WANDB_AVAILABLE:
            print("   # ë‘ ë°©ë²• ëª¨ë‘ ì‚¬ìš©")
            print("   python src/ultimate_train_fix.py --optimize --optimization-method both --use-wandb --n-trials 30")
    else:
        print("\nğŸ“¦ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:")
        print("   pip install optuna wandb  # ë‘˜ ë‹¤ ì„¤ì¹˜ ê¶Œì¥")
        print("   pip install optuna        # Optunaë§Œ ì„¤ì¹˜")
        print("   pip install wandb         # W&Bë§Œ ì„¤ì¹˜")

# ì‹¤ì œ ê³µê°„ í™œìš©ë¥  ê³„ì‚° ë¡œì§ ì¶”ê°€
def calculate_real_utilization(env):
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í™œìš©ë¥  ê³„ì‚° í•¨ìˆ˜"""
    if hasattr(env.unwrapped, 'container'):
        placed_volume = sum(box.volume for box in env.unwrapped.container.boxes 
                          if box.position is not None)
        container_volume = env.unwrapped.container.volume
        return placed_volume / container_volume
    return 0.0