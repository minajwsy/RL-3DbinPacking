#!/usr/bin/env python3
"""
ğŸš€ 999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²° + GIF + ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±
í‰ê°€ ì½œë°±ì„ ì™„ì „íˆ ì¬ì‘ì„±í•˜ì—¬ ë¬´í•œ ëŒ€ê¸° ë¬¸ì œ 100% í•´ê²°
"""

import os
import sys
import time
import datetime
import warnings
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from pathlib import Path

# ì„œë²„ í™˜ê²½ ëŒ€ì‘
matplotlib.use('Agg')
warnings.filterwarnings("ignore")

# í°íŠ¸ ì„¤ì • (í•œê¸€ í°íŠ¸ ë¬¸ì œ í•´ê²°)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 10
plt.rcParams['axes.unicode_minus'] = False

# ê²½ë¡œ ì„¤ì •
sys.path.append('src')
os.environ['PYTHONPATH'] = os.getcwd() + ':' + os.getcwd() + '/src'

from stable_baselines3 import PPO
from sb3_contrib import MaskablePPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_checker import check_env

# ë¡œì»¬ ëª¨ë“ˆ import
from packing_kernel import *
from train_maskable_ppo import make_env, get_action_masks

class UltimateSafeCallback(BaseCallback):
    """
    999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²°ì„ ìœ„í•œ ì•ˆì „í•œ ì½œë°±
    - ìµœì†Œí•œì˜ í‰ê°€ë§Œ ìˆ˜í–‰
    - íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±
    """
    def __init__(self, eval_env, eval_freq=2000, verbose=1):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.last_eval_time = 0
        
        # ì„±ëŠ¥ ì§€í‘œ ì €ì¥
        self.timesteps = []
        self.episode_rewards = []
        self.eval_rewards = []
        self.eval_timesteps = []
        self.success_rates = []
        self.utilization_rates = []
        
        # ì‹¤ì‹œê°„ í”Œë¡¯ ì„¤ì •
        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle('Real-time Training Performance Monitoring', fontsize=16)
        plt.ion()
        
        print(f"ğŸ›¡ï¸ ì•ˆì „í•œ ì½œë°± ì´ˆê¸°í™” ì™„ë£Œ (í‰ê°€ ì£¼ê¸°: {eval_freq})")
        
    def _on_training_start(self) -> None:
        """í•™ìŠµ ì‹œì‘"""
        print("ğŸš€ ì•ˆì „í•œ í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        self.start_time = time.time()
        self._setup_plots()
        
    def _on_step(self) -> bool:
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ - ì•ˆì „í•œ ì²˜ë¦¬"""
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²´í¬
        if self.locals.get('dones', [False])[0]:
            if 'episode' in self.locals.get('infos', [{}])[0]:
                episode_info = self.locals['infos'][0]['episode']
                episode_reward = episode_info['r']
                
                self.timesteps.append(self.num_timesteps)
                self.episode_rewards.append(episode_reward)
                
                # ì£¼ê¸°ì  ì¶œë ¥
                if len(self.episode_rewards) % 20 == 0:
                    recent_rewards = self.episode_rewards[-20:]
                    mean_reward = np.mean(recent_rewards)
                    elapsed = time.time() - self.start_time
                    
                    print(f"ğŸ“Š ìŠ¤í…: {self.num_timesteps:,} | "
                          f"ì—í”¼ì†Œë“œ: {len(self.episode_rewards)} | "
                          f"ìµœê·¼ í‰ê·  ë³´ìƒ: {mean_reward:.3f} | "
                          f"ê²½ê³¼: {elapsed:.1f}ì´ˆ")
        
        # ì•ˆì „í•œ í‰ê°€ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
        if self.num_timesteps - self.last_eval_time >= self.eval_freq:
            self._safe_evaluation()
            self._update_plots()
            self.last_eval_time = self.num_timesteps
            
        return True
    
    def _safe_evaluation(self):
        """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” ì•ˆì „í•œ í‰ê°€ -> í‰ê°€í•¨ìˆ˜ ê°œì„ """
        try:
            print(f"ğŸ” ì•ˆì „í•œ í‰ê°€ ì‹œì‘ (ìŠ¤í…: {self.num_timesteps:,})")
            
            eval_rewards = []
            success_count = 0
            max_episodes = 15  # 8 â†’ 15ë¡œ ì¦ê°€ (ë” ì •í™•í•œ í‰ê°€)
            
            for ep_idx in range(max_episodes):
                try:
                    # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                    eval_start = time.time()
                    timeout = 60  # 30 â†’ 60ì´ˆë¡œ ì¦ê°€
                    
                    obs, _ = self.eval_env.reset()
                    episode_reward = 0.0
                    step_count = 0
                    max_steps = 100  # 50 â†’ 100ìœ¼ë¡œ ì¦ê°€ (ì¶©ë¶„í•œ ì‹œê°„)
                    
                    while step_count < max_steps:
                        # íƒ€ì„ì•„ì›ƒ ì²´í¬
                        if time.time() - eval_start > timeout:
                            print(f"â° í‰ê°€ íƒ€ì„ì•„ì›ƒ (ì—í”¼ì†Œë“œ {ep_idx})")
                            break
                            
                        try:
                            action_masks = get_action_masks(self.eval_env)
                            action, _ = self.model.predict(obs, action_masks=action_masks, deterministic=True)
                            obs, reward, terminated, truncated, info = self.eval_env.step(action)
                            episode_reward += reward
                            step_count += 1
                            
                            if terminated or truncated:
                                break
                        except Exception as e:
                            print(f"âš ï¸ í‰ê°€ ìŠ¤í… ì˜¤ë¥˜: {e}")
                            break
                    
                    eval_rewards.append(episode_reward)
                    success_threshold = 5.0  # 3.0 â†’ 5.0ìœ¼ë¡œ ìƒí–¥ ì¡°ì •
                    if episode_reward >= success_threshold:
                        success_count += 1
                        
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì—í”¼ì†Œë“œ {ep_idx} ì˜¤ë¥˜: {e}")
                    eval_rewards.append(0.0)
            
            # ê²°ê³¼ ì €ì¥
            if eval_rewards:
                mean_eval_reward = np.mean(eval_rewards)
                success_rate = success_count / len(eval_rewards)
                utilization = max(0.0, mean_eval_reward)
                
                self.eval_rewards.append(mean_eval_reward)
                self.eval_timesteps.append(self.num_timesteps)
                self.success_rates.append(success_rate)
                self.utilization_rates.append(utilization)
                
                print(f"âœ… í‰ê°€ ì™„ë£Œ: ë³´ìƒ {mean_eval_reward:.3f}, "
                      f"ì„±ê³µë¥  {success_rate:.1%}, í™œìš©ë¥  {utilization:.1%}")
            else:
                print("âŒ í‰ê°€ ì‹¤íŒ¨ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                self.eval_rewards.append(0.0)
                self.eval_timesteps.append(self.num_timesteps)
                self.success_rates.append(0.0)
                self.utilization_rates.append(0.0)
                
        except Exception as e:
            print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œì—ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰
            self.eval_rewards.append(0.0)
            self.eval_timesteps.append(self.num_timesteps)
            self.success_rates.append(0.0)
            self.utilization_rates.append(0.0)
    
    def _setup_plots(self):
        """í”Œë¡¯ ì´ˆê¸° ì„¤ì •"""
        titles = [
            'Episode Rewards (Training)',
            'Evaluation Rewards (Periodic)',
            'Success Rate (%)',
            'Utilization Rate (%)'
        ]
        
        for i, ax in enumerate(self.axes.flat):
            ax.set_title(titles[i])
            ax.grid(True, alpha=0.3)
            ax.set_xlabel('Steps')
    
    def _update_plots(self):
        """ì‹¤ì‹œê°„ í”Œë¡¯ ì—…ë°ì´íŠ¸"""
        try:
            # 1. ì—í”¼ì†Œë“œ ë³´ìƒ
            if self.timesteps and self.episode_rewards:
                self.axes[0, 0].clear()
                self.axes[0, 0].plot(self.timesteps, self.episode_rewards, 'b-', alpha=0.6, linewidth=1)
                if len(self.episode_rewards) > 50:
                    # ì´ë™ í‰ê· 
                    window = min(50, len(self.episode_rewards) // 4)
                    moving_avg = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')
                    moving_steps = self.timesteps[window-1:]
                    self.axes[0, 0].plot(moving_steps, moving_avg, 'r-', linewidth=2, label=f'Moving Avg({window})')
                    self.axes[0, 0].legend()
                self.axes[0, 0].set_title('Episode Rewards (Training)')
                self.axes[0, 0].grid(True, alpha=0.3)
            
            # 2. í‰ê°€ ë³´ìƒ
            if self.eval_timesteps and self.eval_rewards:
                self.axes[0, 1].clear()
                self.axes[0, 1].plot(self.eval_timesteps, self.eval_rewards, 'g-o', linewidth=2, markersize=6)
                self.axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
                self.axes[0, 1].set_title('Evaluation Rewards (Periodic)')
                self.axes[0, 1].grid(True, alpha=0.3)
            
            # 3. ì„±ê³µë¥ 
            if self.eval_timesteps and self.success_rates:
                self.axes[1, 0].clear()
                success_pct = [rate * 100 for rate in self.success_rates]
                self.axes[1, 0].plot(self.eval_timesteps, success_pct, 'orange', linewidth=2, marker='s')
                self.axes[1, 0].axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Target(80%)')
                self.axes[1, 0].set_ylim(0, 100)
                self.axes[1, 0].set_title('Success Rate (%)')
                self.axes[1, 0].grid(True, alpha=0.3)
                self.axes[1, 0].legend()
            
            # 4. í™œìš©ë¥ 
            if self.eval_timesteps and self.utilization_rates:
                self.axes[1, 1].clear()
                util_pct = [rate * 100 for rate in self.utilization_rates]
                self.axes[1, 1].plot(self.eval_timesteps, util_pct, 'purple', linewidth=2, marker='^')
                # ë™ì  Yì¶• ì„¤ì • (ìµœëŒ€ê°’ì˜ 110%ê¹Œì§€)
                max_util = max(util_pct) if util_pct else 100
                self.axes[1, 1].set_ylim(0, max(100, max_util * 1.1))
                self.axes[1, 1].set_title('Utilization Rate (%)')
                self.axes[1, 1].grid(True, alpha=0.3)
            
            # í”Œë¡¯ ì €ì¥
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            plt.tight_layout()
            plt.savefig(f'results/realtime_performance_{timestamp}.png', dpi=150, bbox_inches='tight')
            
        except Exception as e:
            print(f"âš ï¸ í”Œë¡¯ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def _on_training_end(self) -> None:
        """í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì¢… ê·¸ë˜í”„ ì €ì¥"""
        print("ğŸ“Š ìµœì¢… ì„±ê³¼ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ìµœì¢… ì„±ê³¼ ëŒ€ì‹œë³´ë“œ
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Final Training Performance Dashboard', fontsize=20)
        
        try:
            # 1. í•™ìŠµ ê³¡ì„ 
            if self.timesteps and self.episode_rewards:
                axes[0, 0].plot(self.timesteps, self.episode_rewards, 'b-', alpha=0.4, linewidth=1, label='Episode Rewards')
                if len(self.episode_rewards) > 20:
                    window = min(50, len(self.episode_rewards) // 4)
                    moving_avg = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')
                    moving_steps = self.timesteps[window-1:]
                    axes[0, 0].plot(moving_steps, moving_avg, 'r-', linewidth=3, label=f'Moving Avg({window})')
                axes[0, 0].set_title('Learning Curve')
                axes[0, 0].set_xlabel('Steps')
                axes[0, 0].set_ylabel('Reward')
                axes[0, 0].legend()
                axes[0, 0].grid(True, alpha=0.3)
            
            # 2. í‰ê°€ ì„±ëŠ¥
            if self.eval_timesteps and self.eval_rewards:
                axes[0, 1].plot(self.eval_timesteps, self.eval_rewards, 'g-o', linewidth=3, markersize=8)
                axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
                axes[0, 1].set_title('Evaluation Performance')
                axes[0, 1].set_xlabel('Steps')
                axes[0, 1].set_ylabel('Evaluation Reward')
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. ì„±ê³µë¥  ì¶”ì´
            if self.eval_timesteps and self.success_rates:
                success_pct = [rate * 100 for rate in self.success_rates]
                axes[0, 2].plot(self.eval_timesteps, success_pct, 'orange', linewidth=3, marker='s', markersize=8)
                axes[0, 2].axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Target(80%)')
                axes[0, 2].set_ylim(0, 100)
                axes[0, 2].set_title('Success Rate Trend')
                axes[0, 2].set_xlabel('Steps')
                axes[0, 2].set_ylabel('Success Rate (%)')
                axes[0, 2].legend()
                axes[0, 2].grid(True, alpha=0.3)
            
            # 4. í™œìš©ë¥  ì¶”ì´
            if self.eval_timesteps and self.utilization_rates:
                util_pct = [rate * 100 for rate in self.utilization_rates]
                axes[1, 0].plot(self.eval_timesteps, util_pct, 'purple', linewidth=3, marker='^', markersize=8)
                # ë™ì  Yì¶• ì„¤ì • (ìµœëŒ€ê°’ì˜ 110%ê¹Œì§€)
                max_util = max(util_pct) if util_pct else 100
                axes[1, 0].set_ylim(0, max(100, max_util * 1.1))
                axes[1, 0].set_title('Utilization Rate Trend')
                axes[1, 0].set_xlabel('Steps')
                axes[1, 0].set_ylabel('Utilization Rate (%)')
                axes[1, 0].grid(True, alpha=0.3)
            
            # 5. ë³´ìƒ ë¶„í¬
            if self.episode_rewards:
                axes[1, 1].hist(self.episode_rewards, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
                axes[1, 1].axvline(np.mean(self.episode_rewards), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(self.episode_rewards):.3f}')
                axes[1, 1].set_title('Reward Distribution')
                axes[1, 1].set_xlabel('Reward')
                axes[1, 1].set_ylabel('Frequency')
                axes[1, 1].legend()
                axes[1, 1].grid(True, alpha=0.3)
            
            # 6. ì„±ê³¼ ìš”ì•½
            axes[1, 2].axis('off')
            if self.episode_rewards and self.eval_rewards:
                summary_text = f"""
Training Summary Statistics

Total Episodes: {len(self.episode_rewards):,}
Final Steps: {self.num_timesteps:,}
Training Time: {(time.time() - self.start_time):.1f}s

Training Performance:
â€¢ Mean Reward: {np.mean(self.episode_rewards):.3f}
â€¢ Max Reward: {np.max(self.episode_rewards):.3f}
â€¢ Min Reward: {np.min(self.episode_rewards):.3f}
â€¢ Std Dev: {np.std(self.episode_rewards):.3f}

Evaluation Performance:
â€¢ Final Eval Reward: {self.eval_rewards[-1] if self.eval_rewards else 0:.3f}
â€¢ Best Eval Reward: {np.max(self.eval_rewards) if self.eval_rewards else 0:.3f}
â€¢ Final Success Rate: {self.success_rates[-1]*100 if self.success_rates else 0:.1f}%
â€¢ Final Utilization: {self.utilization_rates[-1]*100 if self.utilization_rates else 0:.1f}%
"""
                axes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes, 
                               fontsize=12, verticalalignment='top', fontfamily='monospace',
                               bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
            
            plt.tight_layout()
            dashboard_path = f'results/ultimate_dashboard_{timestamp}.png'
            plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ìµœì¢… ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_path}")
            
            # ì„±ëŠ¥ ë°ì´í„° ì €ì¥
            performance_data = {
                'timesteps': self.timesteps,
                'episode_rewards': self.episode_rewards,
                'eval_timesteps': self.eval_timesteps,
                'eval_rewards': self.eval_rewards,
                'success_rates': self.success_rates,
                'utilization_rates': self.utilization_rates
            }
            np.save(f'results/performance_data_{timestamp}.npy', performance_data)
            
        except Exception as e:
            print(f"âš ï¸ ìµœì¢… ê·¸ë˜í”„ ìƒì„± ì˜¤ë¥˜: {e}")
        
        plt.close('all')

class UltimateCurriculumCallback(BaseCallback):
    """
    999 ìŠ¤í… ë¬¸ì œ ì—†ëŠ” ì•ˆì „í•œ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì½œë°±
    ì„±ê³µë¥ ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ë°•ìŠ¤ ê°œìˆ˜(ë‚œì´ë„)ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        container_size,
        initial_boxes,
        target_boxes,
        num_visible_boxes,
        success_threshold=0.6,
        curriculum_steps=5,
        patience=5,
        verbose=1,
    ):
        super().__init__(verbose)
        self.container_size = container_size
        self.initial_boxes = initial_boxes
        self.target_boxes = target_boxes
        self.num_visible_boxes = num_visible_boxes
        self.success_threshold = success_threshold
        self.curriculum_steps = curriculum_steps
        self.patience = patience
        self.verbose = verbose
        
        # ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ ì„¤ì •
        self.current_boxes = initial_boxes
        self.box_increments = []
        if target_boxes > initial_boxes:
            step_size = max(1, (target_boxes - initial_boxes) // curriculum_steps)
            for i in range(curriculum_steps):
                next_boxes = initial_boxes + (i + 1) * step_size
                if next_boxes > target_boxes:
                    next_boxes = target_boxes
                self.box_increments.append(next_boxes)
            # ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” í•­ìƒ target_boxes
            if self.box_increments[-1] != target_boxes:
                self.box_increments.append(target_boxes)
        
        # ì„±ê³¼ ì¶”ì  ë³€ìˆ˜
        self.evaluation_count = 0
        self.consecutive_successes = 0
        self.curriculum_level = 0
        self.last_success_rate = 0.0
        self.recent_rewards = []
        
        # ì•ˆì „í•œ ì¸¡ì •ì„ ìœ„í•œ ë³€ìˆ˜
        self.measurement_window = 20  # ì¸¡ì • ìœˆë„ìš° í¬ê¸°
        self.min_episodes = 10        # ìµœì†Œ ì—í”¼ì†Œë“œ ìˆ˜
        
        if self.verbose >= 1:
            print(f"ğŸ“ ì•ˆì „í•œ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì´ˆê¸°í™”:")
            print(f"   - ì‹œì‘ ë°•ìŠ¤ ìˆ˜: {self.initial_boxes}")
            print(f"   - ëª©í‘œ ë°•ìŠ¤ ìˆ˜: {self.target_boxes}")
            print(f"   - ë‹¨ê³„ë³„ ì¦ê°€: {self.box_increments}")
            print(f"   - ì„±ê³µ ì„ê³„ê°’: {self.success_threshold}")
            print(f"   - ì¸¡ì • ìœˆë„ìš°: {self.measurement_window}")
    
    def _on_step(self) -> bool:
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ - ì•ˆì „í•œ ì²˜ë¦¬"""
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²´í¬
        if self.locals.get('dones', [False])[0]:
            if 'episode' in self.locals.get('infos', [{}])[0]:
                episode_info = self.locals['infos'][0]['episode']
                episode_reward = episode_info['r']
                
                # ìµœê·¼ ë³´ìƒ ê¸°ë¡ (ì•ˆì „í•œ ë°©ì‹)
                self.recent_rewards.append(episode_reward)
                
                # ìœˆë„ìš° í¬ê¸° ìœ ì§€
                if len(self.recent_rewards) > self.measurement_window:
                    self.recent_rewards.pop(0)
                
                # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ í‰ê°€
                if len(self.recent_rewards) >= self.min_episodes:
                    self._evaluate_curriculum_progress()
        
        return True
    
    def _evaluate_curriculum_progress(self):
        """ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰ ìƒí™© í‰ê°€ (ì•ˆì „í•œ ë°©ì‹)"""
        try:
            # ì„±ê³µë¥  ê³„ì‚° (ë³´ìƒ > 0.5ì¸ ê²½ìš° ì„±ê³µìœ¼ë¡œ ê°„ì£¼)
            success_count = sum(1 for r in self.recent_rewards if r > 0.5)
            success_rate = success_count / len(self.recent_rewards)
            
            self.last_success_rate = success_rate
            self.evaluation_count += 1
            
            # ì„±ê³µë¥ ì´ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ë‚œì´ë„ ì¦ê°€ ê³ ë ¤
            if success_rate >= self.success_threshold:
                self.consecutive_successes += 1
                
                # ì—°ì† ì„±ê³µ íšŸìˆ˜ê°€ patienceë¥¼ ë„˜ìœ¼ë©´ ë‚œì´ë„ ì¦ê°€
                if self.consecutive_successes >= self.patience:
                    self._increase_difficulty()
            else:
                self.consecutive_successes = 0
                
        except Exception as e:
            if self.verbose >= 1:
                print(f"âš ï¸ ì»¤ë¦¬í˜ëŸ¼ í‰ê°€ ì˜¤ë¥˜: {e}")
    
    def _increase_difficulty(self):
        """ë‚œì´ë„ ì¦ê°€ (ë°•ìŠ¤ ê°œìˆ˜ ì¦ê°€) - ì•ˆì „í•œ ë°©ì‹"""
        if self.curriculum_level < len(self.box_increments):
            new_boxes = self.box_increments[self.curriculum_level]
            
            if self.verbose >= 1:
                print(f"\nğŸ¯ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: ë‚œì´ë„ ì¦ê°€!")
                print(f"   - ì´ì „ ë°•ìŠ¤ ìˆ˜: {self.current_boxes}")
                print(f"   - ìƒˆë¡œìš´ ë°•ìŠ¤ ìˆ˜: {new_boxes}")
                print(f"   - í˜„ì¬ ì„±ê³µë¥ : {self.last_success_rate:.1%}")
                print(f"   - ì—°ì† ì„±ê³µ íšŸìˆ˜: {self.consecutive_successes}")
                print(f"   - ì§„í–‰ë„: {self.curriculum_level + 1}/{len(self.box_increments)}")
            
            self.current_boxes = new_boxes
            self.curriculum_level += 1
            self.consecutive_successes = 0
            
            # ìƒˆë¡œìš´ ë‚œì´ë„ì—ì„œ ì¸¡ì • ì´ˆê¸°í™”
            self.recent_rewards = []
            
            # í™˜ê²½ ì—…ë°ì´íŠ¸ëŠ” ì•ˆì „ì„±ì„ ìœ„í•´ ë¡œê·¸ë§Œ ì¶œë ¥
            if self.verbose >= 1:
                print(f"   - ë‹¤ìŒ í•™ìŠµ ì„¸ì…˜ì—ì„œ {new_boxes}ê°œ ë°•ìŠ¤ë¡œ í•™ìŠµ ê¶Œì¥")
                print(f"   - í˜„ì¬ ì„¸ì…˜ì€ ì•ˆì •ì„±ì„ ìœ„í•´ ê³„ì† ì§„í–‰")
    
    def get_current_difficulty(self):
        """í˜„ì¬ ë‚œì´ë„ ì •ë³´ ë°˜í™˜"""
        return {
            "current_boxes": self.current_boxes,
            "curriculum_level": self.curriculum_level,
            "max_level": len(self.box_increments),
            "success_rate": self.last_success_rate,
            "consecutive_successes": self.consecutive_successes,
            "progress_percentage": (self.curriculum_level / len(self.box_increments)) * 100 if self.box_increments else 0,
            "recommended_boxes": self.current_boxes
        }
    
    def is_curriculum_complete(self):
        """ì»¤ë¦¬í˜ëŸ¼ ì™„ë£Œ ì—¬ë¶€ í™•ì¸"""
        return self.curriculum_level >= len(self.box_increments)

def create_ultimate_gif(model, env, timestamp):
    """í”„ë¦¬ë¯¸ì—„ í’ˆì§ˆ GIF ìƒì„± - ê¸°ì¡´ ê³ í’ˆì§ˆ GIFë“¤ê³¼ ë™ì¼í•œ ìˆ˜ì¤€"""
    print("ğŸ¬ í”„ë¦¬ë¯¸ì—„ í’ˆì§ˆ GIF ìƒì„± ì¤‘...")
    
    try:
        import matplotlib.pyplot as plt
        from matplotlib.patches import Rectangle
        from mpl_toolkits.mplot3d import Axes3D
        from mpl_toolkits.mplot3d.art3d import Poly3DCollection
        from PIL import Image
        import io
        import numpy as np
        
        # í™˜ê²½ ìƒíƒœ í™•ì¸
        if env is None:
            print("âŒ í™˜ê²½ì´ Noneì…ë‹ˆë‹¤")
            return None
            
        # GIF ì „ìš© ìƒˆë¡œìš´ í™˜ê²½ ìƒì„± (ì•ˆì „í•œ ë°©ë²•)
        print("ğŸ”§ GIF ì „ìš© í™˜ê²½ ìƒì„± ì¤‘...")
        try:
            # ì›ë³¸ í™˜ê²½ê³¼ ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ìƒˆ í™˜ê²½ ìƒì„±
            gif_env = make_env(
                container_size=[10, 10, 10],
                num_boxes=16,  # ê¸°ë³¸ê°’ ì‚¬ìš©
                num_visible_boxes=3,
                seed=42,
                render_mode=None,
                random_boxes=False,
                only_terminal_reward=False,
                improved_reward_shaping=True,
            )()
        except Exception as e:
            print(f"âŒ GIF í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
        
        frames = []
        
        try:
            obs, _ = gif_env.reset()
            print(f"âœ… GIF í™˜ê²½ ë¦¬ì…‹ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ GIF í™˜ê²½ ë¦¬ì…‹ ì‹¤íŒ¨: {e}")
            gif_env.close()
            return None
        
        # matplotlib ì„¤ì • (ê³ í’ˆì§ˆ)
        plt.ioff()  # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ ë¹„í™œì„±í™”
        plt.style.use('default')
        fig = plt.figure(figsize=(16, 12), facecolor='white')  # ë” í° í•´ìƒë„
        ax = fig.add_subplot(111, projection='3d')
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì„¤ì • (ë” ì˜ˆìœ ìƒ‰ìƒë“¤)
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', 
                  '#FF9FF3', '#54A0FF', '#5F27CD', '#00D2D3', '#FF9F43',
                  '#10AC84', '#EE5A24', '#0084FF', '#D63031', '#74B9FF',
                  '#A29BFE', '#6C5CE7', '#FD79A8', '#FDCB6E', '#E17055']
        
        print(f"ğŸ¬ í”„ë ˆì„ ìƒì„± ì‹œì‘ (ìµœëŒ€ 50 í”„ë ˆì„)")
        total_reward = 0
        
        for step in range(50):  # ë” ë§ì€ í”„ë ˆì„
            try:
                # í˜„ì¬ ìƒíƒœ ì‹œê°í™”
                ax.clear()
                
                # ì¶• ë²”ìœ„ ë° ë¼ë²¨ ì„¤ì •
                ax.set_xlim(0, 10)
                ax.set_ylim(0, 10)
                ax.set_zlim(0, 10)
                ax.set_xlabel('X-axis (Width)', fontsize=12, fontweight='bold')
                ax.set_ylabel('Y-axis (Depth)', fontsize=12, fontweight='bold')
                ax.set_zlabel('Z-axis (Height)', fontsize=12, fontweight='bold')
                
                # ì»¨í…Œì´ë„ˆ ì™¸ê³½ì„  ê·¸ë¦¬ê¸° (ë” ëª…í™•í•˜ê²Œ)
                container_edges = [
                    # ë°”ë‹¥ë©´
                    [(0,0,0), (10,0,0), (10,10,0), (0,10,0)],
                    # ìœ—ë©´
                    [(0,0,10), (10,0,10), (10,10,10), (0,10,10)],
                    # ì¸¡ë©´ë“¤
                    [(0,0,0), (0,0,10), (0,10,10), (0,10,0)],
                    [(10,0,0), (10,0,10), (10,10,10), (10,10,0)],
                    [(0,0,0), (10,0,0), (10,0,10), (0,0,10)],
                    [(0,10,0), (10,10,0), (10,10,10), (0,10,10)]
                ]
                
                # ì»¨í…Œì´ë„ˆ ì™¸ê³½ì„  ê·¸ë¦¬ê¸°
                for face in container_edges:
                    if face == container_edges[0]:  # ë°”ë‹¥ë©´ë§Œ ì±„ìš°ê¸°
                        poly = Poly3DCollection([face], alpha=0.1, facecolor='lightgray', edgecolor='black')
                        ax.add_collection3d(poly)
                    else:
                        poly = Poly3DCollection([face], alpha=0.02, facecolor='lightblue', edgecolor='gray')
                        ax.add_collection3d(poly)
                
                # ë°•ìŠ¤ë“¤ ê·¸ë¦¬ê¸° (í™˜ê²½ì—ì„œ ì •ë³´ ì¶”ì¶œ)
                box_count = 0
                utilization = 0
                
                try:
                    if hasattr(gif_env, 'unwrapped') and hasattr(gif_env.unwrapped, 'container'):
                        container = gif_env.unwrapped.container
                        for box in container.boxes:
                            if hasattr(box, 'position') and box.position is not None:
                                x, y, z = box.position
                                w, h, d = box.size
                                
                                # ë°•ìŠ¤ ìƒ‰ìƒ ì„ íƒ
                                color = colors[box_count % len(colors)]
                                
                                # 3D ë°•ìŠ¤ ê·¸ë¦¬ê¸° (6ë©´ ëª¨ë‘)
                                r = [0, w]
                                s = [0, h] 
                                t = [0, d]
                                
                                # ë°•ìŠ¤ì˜ 8ê°œ ê¼­ì§“ì  ê³„ì‚°
                                xx, yy, zz = np.meshgrid(r, s, t)
                                vertices = []
                                for i in range(2):
                                    for j in range(2):
                                        for k in range(2):
                                            vertices.append([x + xx[i,j,k], y + yy[i,j,k], z + zz[i,j,k]])
                                
                                # ë°•ìŠ¤ì˜ 6ê°œ ë©´ ì •ì˜
                                faces = [
                                    [vertices[0], vertices[1], vertices[3], vertices[2]],  # ì•ë©´
                                    [vertices[4], vertices[5], vertices[7], vertices[6]],  # ë’·ë©´
                                    [vertices[0], vertices[1], vertices[5], vertices[4]],  # ì•„ë˜ë©´
                                    [vertices[2], vertices[3], vertices[7], vertices[6]],  # ìœ—ë©´
                                    [vertices[0], vertices[2], vertices[6], vertices[4]],  # ì™¼ìª½ë©´
                                    [vertices[1], vertices[3], vertices[7], vertices[5]]   # ì˜¤ë¥¸ìª½ë©´
                                ]
                                
                                # ë©´ ì¶”ê°€
                                face_collection = Poly3DCollection(faces, alpha=0.8, facecolor=color, edgecolor='black', linewidth=1)
                                ax.add_collection3d(face_collection)
                                
                                # ë°•ìŠ¤ ë¼ë²¨ ì¶”ê°€
                                ax.text(x + w/2, y + h/2, z + d/2, f'{box_count+1}', 
                                       fontsize=10, fontweight='bold', ha='center', va='center')
                                
                                box_count += 1
                                utilization += w * h * d
                        
                        # í™œìš©ë¥  ê³„ì‚° (ì»¨í…Œì´ë„ˆ ë¶€í”¼: 10*10*10 = 1000)
                        utilization_percent = (utilization / 1000) * 100
                        
                except Exception as box_e:
                    print(f"âš ï¸ ë°•ìŠ¤ ë Œë”ë§ ì˜¤ë¥˜ (ìŠ¤í… {step}): {box_e}")
                
                # ì œëª© ë° ì •ë³´ í‘œì‹œ
                ax.set_title(f'Real-time Training Performance Monitoring\n'
                           f'Step: {step+1}/50 | Placed Boxes: {box_count} | Utilization: {utilization_percent:.1f}%', 
                           fontsize=14, fontweight='bold', pad=20)
                
                # ì¹´ë©”ë¼ ê°ë„ ì„¤ì • (ë” ì¢‹ì€ ì‹œì•¼ê°)
                ax.view_init(elev=25, azim=45 + step * 2)  # íšŒì „ íš¨ê³¼
                
                # ê·¸ë¦¬ë“œ ì„¤ì •
                ax.grid(True, alpha=0.3)
                ax.set_facecolor('white')
                
                # í”„ë ˆì„ ì €ì¥ (ê³ í’ˆì§ˆ)
                try:
                    buf = io.BytesIO()
                    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight', 
                              facecolor='white', edgecolor='none', pad_inches=0.1)
                    buf.seek(0)
                    frame = Image.open(buf).copy()
                    frames.append(frame)
                    buf.close()
                    
                    if step % 10 == 0:
                        print(f"  Frame {step + 1}/50 completed (Boxes: {box_count})")
                        
                except Exception as save_e:
                    print(f"âš ï¸ í”„ë ˆì„ ì €ì¥ ì˜¤ë¥˜ (ìŠ¤í… {step}): {save_e}")
                    continue
                
                # ë‹¤ìŒ ì•¡ì…˜ ìˆ˜í–‰ (ì•ˆì „í•œ ë°©ë²•)
                try:
                    action_masks = get_action_masks(gif_env)
                    action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
                    obs, reward, terminated, truncated, info = gif_env.step(action)
                    total_reward += reward
                    
                    if terminated or truncated:
                        print(f"  Episode ended (Step {step + 1}, Total Reward: {total_reward:.2f})")
                        # ë§ˆì§€ë§‰ í”„ë ˆì„ ëª‡ ê°œ ë” ì¶”ê°€ (ê²°ê³¼ í™•ì¸ìš©)
                        for _ in range(3):
                            frames.append(frame.copy())
                        break
                        
                except Exception as step_e:
                    print(f"âš ï¸ ì•¡ì…˜ ìˆ˜í–‰ ì˜¤ë¥˜ (ìŠ¤í… {step}): {step_e}")
                    break
                    
            except Exception as frame_e:
                print(f"âš ï¸ í”„ë ˆì„ {step} ìƒì„± ì¤‘ ì˜¤ë¥˜: {frame_e}")
                break
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        plt.close(fig)
        gif_env.close()
        
        # GIF ì €ì¥ (ê³ í’ˆì§ˆ)
        if len(frames) >= 5:  # ìµœì†Œ 5 í”„ë ˆì„ ì´ìƒ
            try:
                gif_path = f'gifs/ultimate_demo_{timestamp}.gif'
                os.makedirs('gifs', exist_ok=True)
                
                # í”„ë ˆì„ í¬ê¸° í†µì¼
                if frames:
                    base_size = frames[0].size
                    normalized_frames = []
                    for frame in frames:
                        if frame.size != base_size:
                            frame = frame.resize(base_size, Image.LANCZOS)
                        normalized_frames.append(frame)
                    
                    # ê³ í’ˆì§ˆ GIF ì €ì¥
                    normalized_frames[0].save(
                        gif_path,
                        save_all=True,
                        append_images=normalized_frames[1:],
                        duration=600,  # 0.6ì´ˆ ê°„ê²© (ë” ë¶€ë“œëŸ½ê²Œ)
                        loop=0,
                        optimize=True
                    )
                    
                    # íŒŒì¼ í¬ê¸° í™•ì¸
                    file_size = os.path.getsize(gif_path)
                    print(f"ğŸ¬ í”„ë¦¬ë¯¸ì—„ GIF ì €ì¥ ì™„ë£Œ: {gif_path}")
                    print(f"  ğŸ“Š í”„ë ˆì„ ìˆ˜: {len(normalized_frames)}")
                    print(f"  ğŸ“ íŒŒì¼ í¬ê¸°: {file_size / 1024:.1f} KB")
                    print(f"  ğŸ¯ ìµœì¢… ë³´ìƒ: {total_reward:.2f}")
                    print(f"  ğŸ“¦ ë°°ì¹˜ëœ ë°•ìŠ¤: {box_count}ê°œ")
                    
                    return gif_path
                
            except Exception as save_e:
                print(f"âŒ GIF íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {save_e}")
                return None
        else:
            print(f"âŒ ì¶©ë¶„í•œ í”„ë ˆì„ ì—†ìŒ ({len(frames)}ê°œ)")
            return None
            
    except Exception as e:
        print(f"âŒ GIF ìƒì„± ì „ì²´ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def ultimate_train(
    timesteps=5000,
    eval_freq=2000,
    container_size=[10, 10, 10],
    num_boxes=16,
    create_gif=True,
    curriculum_learning=True,
    initial_boxes=None,
    success_threshold=0.6,
    curriculum_steps=5,
    patience=5
):
    """999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²°ëœ í•™ìŠµ í•¨ìˆ˜ (ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì§€ì›)"""
    
    # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì„¤ì •
    if curriculum_learning and initial_boxes is None:
        initial_boxes = max(8, num_boxes // 2)  # ì‹œì‘ ë°•ìŠ¤ ìˆ˜ (ëª©í‘œì˜ ì ˆë°˜)
    elif not curriculum_learning:
        initial_boxes = num_boxes
    
    current_boxes = initial_boxes
    
    print("ğŸš€ 999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²° í•™ìŠµ ì‹œì‘")
    print(f"ğŸ“‹ ì„¤ì •: {timesteps:,} ìŠ¤í…, í‰ê°€ ì£¼ê¸° {eval_freq:,}")
    if curriculum_learning:
        print(f"ğŸ“ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: {initial_boxes}ê°œ â†’ {num_boxes}ê°œ ë°•ìŠ¤")
    else:
        print(f"ğŸ“¦ ê³ ì • ë°•ìŠ¤ ìˆ˜: {num_boxes}ê°œ")
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('results', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    os.makedirs('gifs', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    # í™˜ê²½ ìƒì„± (ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ê³ ë ¤)
    print("ğŸ—ï¸ í™˜ê²½ ìƒì„± ì¤‘...")
    env = make_env(
        container_size=container_size,
        num_boxes=current_boxes,  # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì‹œ ì´ˆê¸° ë°•ìŠ¤ ìˆ˜
        num_visible_boxes=3,
        seed=42,
        render_mode=None,
        random_boxes=False,
        only_terminal_reward=False,
        improved_reward_shaping=True,
    )()
    
    # í‰ê°€ìš© í™˜ê²½
    eval_env = make_env(
        container_size=container_size,
        num_boxes=current_boxes,  # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì‹œ ì´ˆê¸° ë°•ìŠ¤ ìˆ˜
        num_visible_boxes=3,
        seed=43,
        render_mode=None,
        random_boxes=False,
        only_terminal_reward=False,
        improved_reward_shaping=True,
    )()
    
    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    env = Monitor(env, f"logs/ultimate_train_{timestamp}.csv")
    eval_env = Monitor(eval_env, f"logs/ultimate_eval_{timestamp}.csv")
    
    print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    print(f"  - ì»¨í…Œì´ë„ˆ: {container_size}")
    print(f"  - í˜„ì¬ ë°•ìŠ¤ ìˆ˜: {current_boxes}")
    if curriculum_learning:
        print(f"  - ìµœì¢… ëª©í‘œ ë°•ìŠ¤ ìˆ˜: {num_boxes}")
    print(f"  - ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤: {env.action_space}")
    print(f"  - ê´€ì°° ìŠ¤í˜ì´ìŠ¤: {env.observation_space}")
    
    # ì•ˆì „í•œ ì½œë°± ì„¤ì • (ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ í¬í•¨)
    print("ğŸ›¡ï¸ ì•ˆì „í•œ ì½œë°± ì„¤ì • ì¤‘...")
    
    # í‰ê°€ ì£¼ê¸°ê°€ ì¶©ë¶„íˆ í° ê²½ìš°ì—ë§Œ ì½œë°± ì‚¬ìš©
    if eval_freq >= 2000:
        safe_callback = UltimateSafeCallback(eval_env, eval_freq=eval_freq)
        
        # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì½œë°± ì¶”ê°€
        callbacks = [safe_callback]
        
        if curriculum_learning:
            curriculum_callback = UltimateCurriculumCallback(
                container_size=container_size,
                initial_boxes=initial_boxes,
                target_boxes=num_boxes,
                num_visible_boxes=3,
                success_threshold=success_threshold,
                curriculum_steps=curriculum_steps,
                patience=patience,
                verbose=1
            )
            callbacks.append(curriculum_callback)
            print(f"ğŸ“ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì½œë°± ì¶”ê°€")
        
        # ì²´í¬í¬ì¸íŠ¸ ì½œë°± (ì•ˆì „í•œ ì„¤ì •)
        checkpoint_callback = CheckpointCallback(
            save_freq=max(eval_freq, 3000),
            save_path="models/checkpoints",
            name_prefix=f"ultimate_model_{timestamp}",
            verbose=1
        )
        callbacks.append(checkpoint_callback)
        
        print(f"âœ… ì•ˆì „í•œ ì½œë°± í™œì„±í™” (í‰ê°€ ì£¼ê¸°: {eval_freq})")
        if curriculum_learning:
            print(f"   - ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ í™œì„±í™”")
            print(f"   - ì„±ê³µ ì„ê³„ê°’: {success_threshold}")
            print(f"   - ì¸ë‚´ì‹¬: {patience}")
    else:
        callbacks = None
        print("âš ï¸ ì½œë°± ë¹„í™œì„±í™” (í‰ê°€ ì£¼ê¸°ê°€ ë„ˆë¬´ ì§§ìŒ)")
        if curriculum_learning:
            print("âš ï¸ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµë„ ë¹„í™œì„±í™”ë¨")
    
    # ìµœì í™”ëœ ëª¨ë¸ ìƒì„±
    print("ğŸ¤– ìµœì í™”ëœ ëª¨ë¸ ìƒì„± ì¤‘...")
    
    # TensorBoard ë¡œê·¸ ì„¤ì • (ì„ íƒì )
    tensorboard_log = None
    try:
        import tensorboard
        tensorboard_log = "logs/tensorboard"
        print("âœ… TensorBoard ë¡œê¹… í™œì„±í™”")
    except ImportError:
        print("âš ï¸ TensorBoard ì—†ìŒ - ë¡œê¹… ë¹„í™œì„±í™”")
    
    model = MaskablePPO(
        "MultiInputPolicy",
        env,
        ent_coef=0.05, 
        vf_coef=0.5,
        learning_rate=5e-4,  # 4e-4 / 9e-4  
        n_steps=1024,        
        batch_size=512,  # 256      
        n_epochs=15,  # 10
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,        
        max_grad_norm=0.5,
        verbose=1,
        tensorboard_log=tensorboard_log,
        seed=42,
    )
    
    print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í…")
    print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    
    try:
        # progress_bar ì„¤ì • (ì˜ì¡´ì„± í™•ì¸)
        use_progress_bar = False
        try:
            import tqdm
            import rich
            use_progress_bar = True
            print("âœ… ì§„í–‰ë¥  í‘œì‹œ í™œì„±í™”")
        except ImportError:
            print("âš ï¸ tqdm/rich ì—†ìŒ - ì§„í–‰ë¥  í‘œì‹œ ë¹„í™œì„±í™”")
        
        model.learn(
            total_timesteps=timesteps,
            callback=callbacks,
            progress_bar=use_progress_bar,
            tb_log_name=f"ultimate_ppo_{timestamp}",
        )
        
        training_time = time.time() - start_time
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {training_time:.2f}ì´ˆ")
        
        # ëª¨ë¸ ì €ì¥
        model_path = f"models/ultimate_ppo_{timestamp}"
        model.save(model_path)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")
        
        # ìµœì¢… í‰ê°€ (ì•ˆì „í•œ ë°©ì‹)
        print("\nğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...")
        final_rewards = []
        
        for ep in range(3):
            obs, _ = eval_env.reset()
            episode_reward = 0.0
            step_count = 0
            
            while step_count < 50:
                try:
                    action_masks = get_action_masks(eval_env)
                    action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    step_count += 1
                    
                    if terminated or truncated:
                        break
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì˜¤ë¥˜: {e}")
                    break
            
            final_rewards.append(episode_reward)
            print(f"  ì—í”¼ì†Œë“œ {ep + 1}: {episode_reward:.4f}")
        
        mean_reward = np.mean(final_rewards) if final_rewards else 0.0
        print(f"ğŸ“Š ìµœì¢… í‰ê·  ë³´ìƒ: {mean_reward:.4f}")
        
        # GIF ìƒì„± (í™˜ê²½ ë‹«ê¸° ì „ì— ìˆ˜í–‰)
        gif_path = None
        if create_gif:
            print("\nğŸ¬ GIF ìƒì„± ì¤‘...")
            try:
                gif_path = create_ultimate_gif(model, eval_env, timestamp)
                if gif_path:
                    print(f"âœ… GIF ìƒì„± ì™„ë£Œ: {gif_path}")
                else:
                    print("âš ï¸ GIF ìƒì„± ì‹¤íŒ¨ - í•™ìŠµì€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨")
            except Exception as e:
                print(f"âš ï¸ GIF ìƒì„± ì˜¤ë¥˜: {e} - í•™ìŠµì€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨")
        
        # í™˜ê²½ ì •ë¦¬ (GIF ìƒì„± í›„)
        env.close()
        eval_env.close()
        
        # ê²°ê³¼ ìš”ì•½
        results = {
            "timestamp": timestamp,
            "total_timesteps": timesteps,
            "training_time": training_time,
            "final_reward": mean_reward,
            "container_size": container_size,
            "num_boxes": num_boxes,
            "model_path": model_path,
            "eval_freq": eval_freq,
            "callbacks_used": callbacks is not None,
            "gif_path": gif_path if gif_path else "GIF ìƒì„± ì‹¤íŒ¨"
        }
        
        # ê²°ê³¼ ì €ì¥
        results_path = f"results/ultimate_results_{timestamp}.txt"
        with open(results_path, "w", encoding='utf-8') as f:
            f.write("=== 999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²° í•™ìŠµ ê²°ê³¼ ===\n")
            for key, value in results.items():
                f.write(f"{key}: {value}\n")
        
        print(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {results_path}")
        
        return model, results
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ í•™ìŠµ ì¤‘ë‹¨ë¨")
        model.save(f"models/interrupted_ultimate_{timestamp}")
        return model, None
    
    except Exception as e:
        print(f"\nâŒí•™ìŠµ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        model.save(f"models/error_ultimate_{timestamp}")
        return None, None

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²° + ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì§€ì›")
    parser.add_argument("--timesteps", type=int, default=5000, help="ì´ í•™ìŠµ ìŠ¤í… ìˆ˜")
    parser.add_argument("--eval-freq", type=int, default=2000, help="í‰ê°€ ì£¼ê¸°")
    parser.add_argument("--container-size", nargs=3, type=int, default=[10, 10, 10], help="ì»¨í…Œì´ë„ˆ í¬ê¸°")
    parser.add_argument("--num-boxes", type=int, default=16, help="ëª©í‘œ ë°•ìŠ¤ ê°œìˆ˜")
    parser.add_argument("--no-gif", action="store_true", help="GIF ìƒì„± ì•ˆí•¨")
    
    # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì˜µì…˜
    parser.add_argument("--curriculum-learning", action="store_true", default=True, 
                        help="ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ í™œì„±í™” (ê¸°ë³¸ê°’: True)")
    parser.add_argument("--no-curriculum", action="store_true", 
                        help="ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ë¹„í™œì„±í™”")
    parser.add_argument("--initial-boxes", type=int, default=None, 
                        help="ì‹œì‘ ë°•ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: ëª©í‘œì˜ ì ˆë°˜)")
    parser.add_argument("--success-threshold", type=float, default=0.6, 
                        help="ì„±ê³µ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.6)")
    parser.add_argument("--curriculum-steps", type=int, default=5, 
                        help="ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ ìˆ˜ (ê¸°ë³¸ê°’: 5)")
    parser.add_argument("--patience", type=int, default=5, 
                        help="ë‚œì´ë„ ì¦ê°€ ëŒ€ê¸° íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)")
    
    args = parser.parse_args()
    
    # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì„¤ì •
    curriculum_learning = args.curriculum_learning and not args.no_curriculum
    
    print("ğŸš€ 999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²° + ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    
    if curriculum_learning:
        print("ğŸ“ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ëª¨ë“œ í™œì„±í™”")
        print(f"   - ì„±ê³µ ì„ê³„ê°’: {args.success_threshold}")
        print(f"   - ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„: {args.curriculum_steps}")
        print(f"   - ì¸ë‚´ì‹¬: {args.patience}")
    else:
        print("ğŸ“¦ ê³ ì • ë‚œì´ë„ ëª¨ë“œ")
    
    try:
        model, results = ultimate_train(
            timesteps=args.timesteps,
            eval_freq=args.eval_freq,
            container_size=args.container_size,
            num_boxes=args.num_boxes,
            create_gif=not args.no_gif,
            curriculum_learning=curriculum_learning,
            initial_boxes=args.initial_boxes,
            success_threshold=args.success_threshold,
            curriculum_steps=args.curriculum_steps,
            patience=args.patience
        )
        
        if results:
            print("\nğŸ‰ í•™ìŠµ ì„±ê³µ!")
            print(f"ğŸ“Š ìµœì¢… ë³´ìƒ: {results['final_reward']:.4f}")
            print(f"â±ï¸ ì†Œìš” ì‹œê°„: {results['training_time']:.2f}ì´ˆ")
            print(f"ğŸ’¾ ëª¨ë¸ ê²½ë¡œ: {results['model_path']}")
            
            # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ê²°ê³¼ ì¶œë ¥
            if curriculum_learning and 'curriculum_info' in results:
                curriculum_info = results['curriculum_info']
                print(f"\nğŸ“ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ê²°ê³¼:")
                print(f"   - ìµœì¢… ë°•ìŠ¤ ìˆ˜: {curriculum_info['current_boxes']}")
                print(f"   - ì§„í–‰ë„: {curriculum_info['curriculum_level']}/{curriculum_info['max_level']}")
                print(f"   - ìµœì¢… ì„±ê³µë¥ : {curriculum_info['success_rate']:.1%}")
                print(f"   - ì™„ë£Œ ì—¬ë¶€: {'âœ… ì™„ë£Œ' if curriculum_info['progress_percentage'] >= 100 else 'â³ ì§„í–‰ ì¤‘'}")
        else:
            print("\nâŒ í•™ìŠµ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì „ì²´ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

# ì‹¤ì œ ê³µê°„ í™œìš©ë¥  ê³„ì‚° ë¡œì§ ì¶”ê°€
def calculate_real_utilization(env):
    if hasattr(env.unwrapped, 'container'):
        placed_volume = sum(box.volume for box in env.unwrapped.container.boxes 
                          if box.position is not None)
        container_volume = env.unwrapped.container.volume
        return placed_volume / container_volume
    return 0.0