#!/usr/bin/env python3
"""
999 ìŠ¤í… ë¬¸ì œ ì™„ì „ í•´ê²°: ì½œë°± ì—†ëŠ” ìˆœìˆ˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
í‰ê°€ ì½œë°±ì„ ì™„ì „íˆ ì œê±°í•˜ì—¬ í•™ìŠµ ì¤‘ë‹¨ ë¬¸ì œ ë°©ì§€
"""

import os
import sys
import time
import datetime
import warnings

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore")

# matplotlib ë°±ì—”ë“œ ì„¤ì • (ì„œë²„ í™˜ê²½ ëŒ€ì‘)
import matplotlib
matplotlib.use('Agg')

# ê²½ë¡œ ì„¤ì •
sys.path.append('src')
os.environ['PYTHONPATH'] = os.getcwd() + ':' + os.getcwd() + '/src'

from stable_baselines3 import PPO
from sb3_contrib import MaskablePPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_checker import check_env

# ë¡œì»¬ ëª¨ë“ˆ import
from packing_kernel import *
from train_maskable_ppo import make_env, get_action_masks

class SimpleProgressCallback:
    """ê°„ë‹¨í•œ ì§„í–‰ìƒí™© ì¶œë ¥ í´ë˜ìŠ¤ (ì½œë°± ì—†ìŒ)"""
    def __init__(self, total_timesteps, print_freq=1000):
        self.total_timesteps = total_timesteps
        self.print_freq = print_freq
        self.start_time = time.time()
        
    def print_progress(self, current_step):
        if current_step % self.print_freq == 0 or current_step == self.total_timesteps:
            elapsed = time.time() - self.start_time
            progress = current_step / self.total_timesteps * 100
            eta = elapsed / current_step * (self.total_timesteps - current_step) if current_step > 0 else 0
            
            print(f"ì§„í–‰: {current_step:,}/{self.total_timesteps:,} ({progress:.1f}%) | "
                  f"ê²½ê³¼: {elapsed:.1f}s | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta:.1f}s")

def no_callback_train(timesteps=3000, container_size=[10, 10, 10], num_boxes=16):
    """ì½œë°± ì—†ëŠ” ìˆœìˆ˜ í•™ìŠµ í•¨ìˆ˜"""
    
    print(f"=== ì½œë°± ì—†ëŠ” ìˆœìˆ˜ í•™ìŠµ ì‹œì‘ ===")
    print(f"ëª©í‘œ ìŠ¤í…: {timesteps:,}")
    print(f"ì»¨í…Œì´ë„ˆ í¬ê¸°: {container_size}")
    print(f"ë°•ìŠ¤ ê°œìˆ˜: {num_boxes}")
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í™˜ê²½ ìƒì„± (ê°„ë‹¨í•œ ì„¤ì •)
    env = make_env(
        container_size=container_size,
        num_boxes=num_boxes,
        num_visible_boxes=3,
        seed=42,
        render_mode=None,
        random_boxes=False,
        only_terminal_reward=False,
        improved_reward_shaping=True,
    )()
    
    # ëª¨ë‹ˆí„°ë§ ì„¤ì • (ë¡œê·¸ë§Œ ê¸°ë¡)
    env = Monitor(env, f"logs/no_callback_train_{timestamp}.csv")
    
    print("í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    print(f"ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤: {env.action_space}")
    print(f"ê´€ì°° ìŠ¤í˜ì´ìŠ¤: {env.observation_space}")
    
    # ê°„ë‹¨í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë¹ ë¥¸ í•™ìŠµìš©)
    model = MaskablePPO(
        "MultiInputPolicy",
        env,
        learning_rate=3e-4,
        n_steps=256,  # ë” ì‘ì€ ìŠ¤í…ìœ¼ë¡œ ë¹ ë¥¸ ì—…ë°ì´íŠ¸
        batch_size=32,  # ì‘ì€ ë°°ì¹˜ í¬ê¸°
        n_epochs=3,  # ì ì€ ì—í¬í¬
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=0,  # ìµœì†Œ ì¶œë ¥
        tensorboard_log=None  # í…ì„œë³´ë“œ ë¹„í™œì„±í™”
    )
    
    print("\n=== ìˆœìˆ˜ í•™ìŠµ ì‹œì‘ (ì½œë°± ì—†ìŒ) ===")
    start_time = time.time()
    
    # ì§„í–‰ìƒí™© ì¶”ì ìš©
    progress = SimpleProgressCallback(timesteps, print_freq=500)
    
    try:
        # ì½œë°± ì—†ì´ ìˆœìˆ˜ í•™ìŠµë§Œ ì‹¤í–‰
        print("âš ï¸  ì½œë°± ì—†ì´ í•™ìŠµ ì‹œì‘ - í‰ê°€ ì—†ìŒ, ì¤‘ë‹¨ ì—†ìŒ")
        
        model.learn(
            total_timesteps=timesteps,
            callback=None,  # ì½œë°± ì™„ì „ ì œê±°
            progress_bar=False,  # ì§„í–‰ë°” ë¹„í™œì„±í™”
            tb_log_name=None,  # í…ì„œë³´ë“œ ë¡œê·¸ ë¹„í™œì„±í™”
        )
        
        training_time = time.time() - start_time
        print(f"\nâœ… ìˆœìˆ˜ í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {training_time:.2f}ì´ˆ")
        
        # ëª¨ë¸ ì €ì¥
        model_path = f"models/no_callback_ppo_{timestamp}"
        model.save(model_path)
        print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        # í•™ìŠµ í›„ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (ë³„ë„ í™˜ê²½ ì‚¬ìš©)
        print("\n=== í•™ìŠµ í›„ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ===")
        test_env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=999,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        
        # 3ë²ˆì˜ ì§§ì€ í…ŒìŠ¤íŠ¸
        total_reward = 0
        test_count = 3
        
        for test_idx in range(test_count):
            obs, _ = test_env.reset()
            episode_reward = 0
            step_count = 0
            max_test_steps = 30  # ë§¤ìš° ì§§ì€ í…ŒìŠ¤íŠ¸
            
            while step_count < max_test_steps:
                try:
                    action_masks = get_action_masks(test_env)
                    action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
                    obs, reward, terminated, truncated, info = test_env.step(action)
                    episode_reward += reward
                    step_count += 1
                    
                    if terminated or truncated:
                        break
                except Exception as e:
                    print(f"í…ŒìŠ¤íŠ¸ {test_idx} ìŠ¤í… {step_count} ì˜¤ë¥˜: {e}")
                    break
            
            total_reward += episode_reward
            print(f"í…ŒìŠ¤íŠ¸ {test_idx + 1}: {step_count}ìŠ¤í…, ë³´ìƒ = {episode_reward:.4f}")
        
        avg_reward = total_reward / test_count
        print(f"í‰ê·  í…ŒìŠ¤íŠ¸ ë³´ìƒ: {avg_reward:.4f}")
        
        # í™˜ê²½ ì •ë¦¬
        env.close()
        test_env.close()
        
        return model_path, avg_reward
        
    except KeyboardInterrupt:
        print("\ní•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        model.save(f"models/interrupted_no_callback_{timestamp}")
        return None, None
    
    except Exception as e:
        print(f"\ní•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        model.save(f"models/error_no_callback_{timestamp}")
        return None, None

def progressive_train(max_timesteps=10000):
    """ì ì§„ì  í•™ìŠµ: ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ"""
    
    print(f"=== ì ì§„ì  í•™ìŠµ ì‹œì‘ ===")
    print(f"ìµœëŒ€ ìŠ¤í…: {max_timesteps:,}")
    
    # ë‹¨ê³„ë³„ í•™ìŠµ
    stages = [
        (1000, [8, 8, 8], 8),    # 1ë‹¨ê³„: ì‘ì€ ë¬¸ì œ
        (2000, [10, 10, 10], 12), # 2ë‹¨ê³„: ì¤‘ê°„ ë¬¸ì œ
        (max_timesteps, [10, 10, 10], 16), # 3ë‹¨ê³„: ëª©í‘œ ë¬¸ì œ
    ]
    
    best_model_path = None
    best_reward = -float('inf')
    
    for stage_idx, (timesteps, container_size, num_boxes) in enumerate(stages):
        print(f"\nğŸ¯ ë‹¨ê³„ {stage_idx + 1}: {timesteps} ìŠ¤í…, ì»¨í…Œì´ë„ˆ {container_size}, ë°•ìŠ¤ {num_boxes}ê°œ")
        
        try:
            model_path, reward = no_callback_train(
                timesteps=timesteps,
                container_size=container_size,
                num_boxes=num_boxes
            )
            
            if model_path and reward > best_reward:
                best_model_path = model_path
                best_reward = reward
                print(f"âœ… ë‹¨ê³„ {stage_idx + 1} ì™„ë£Œ: ë³´ìƒ {reward:.4f}")
            else:
                print(f"âŒ ë‹¨ê³„ {stage_idx + 1} ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ ë‹¨ê³„ {stage_idx + 1} ì˜¤ë¥˜: {e}")
            continue
    
    return best_model_path, best_reward

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ì½œë°± ì—†ëŠ” ìˆœìˆ˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--timesteps", type=int, default=3000, help="ì´ í•™ìŠµ ìŠ¤í…")
    parser.add_argument("--container-size", nargs=3, type=int, default=[10, 10, 10], help="ì»¨í…Œì´ë„ˆ í¬ê¸°")
    parser.add_argument("--num-boxes", type=int, default=16, help="ë°•ìŠ¤ ê°œìˆ˜")
    parser.add_argument("--progressive", action="store_true", help="ì ì§„ì  í•™ìŠµ ëª¨ë“œ")
    
    args = parser.parse_args()
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("models", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    print("ğŸš€ ì½œë°± ì—†ëŠ” ìˆœìˆ˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    print("ğŸ“ íŠ¹ì§•: í‰ê°€ ì—†ìŒ, ì½œë°± ì—†ìŒ, 999 ìŠ¤í… ë¬¸ì œ ì—†ìŒ")
    
    try:
        if args.progressive:
            print("ğŸ“ˆ ì ì§„ì  í•™ìŠµ ëª¨ë“œ")
            model_path, reward = progressive_train(args.timesteps)
        else:
            print("âš¡ ë‹¨ì¼ í•™ìŠµ ëª¨ë“œ")
            model_path, reward = no_callback_train(
                timesteps=args.timesteps,
                container_size=args.container_size,
                num_boxes=args.num_boxes
            )
        
        if model_path:
            print(f"\nğŸ‰ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            print(f"ëª¨ë¸: {model_path}")
            print(f"ì„±ëŠ¥: {reward:.4f}")
            print(f"âœ… 999 ìŠ¤í… ë¬¸ì œ ì—†ì´ ì™„ë£Œë¨")
        else:
            print("âŒ í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1) 