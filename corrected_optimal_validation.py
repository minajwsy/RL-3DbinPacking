#!/usr/bin/env python3
"""
ğŸ”§ ìˆ˜ì •ëœ ìµœì  íŒŒë¼ë¯¸í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµë¥  ë¬¸ì œ í•´ê²° ë° ë” ë‚˜ì€ ë¹„êµ ë¶„ì„
"""

import os
import sys
import warnings
import datetime
import time
import json

os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

print("ğŸ”§ ìˆ˜ì •ëœ ìµœì  íŒŒë¼ë¯¸í„° ê²€ì¦")

# ì›ë³¸ Optuna ê²°ê³¼ (ë¬¸ì œê°€ ìˆëŠ” í•™ìŠµë¥ )
ORIGINAL_OPTIMAL = {
    'learning_rate': 2.6777169756959113e-06,  # ë„ˆë¬´ ë‚®ìŒ!
    'n_steps': 256,
    'batch_size': 32,
    'n_epochs': 3,
    'clip_range': 0.17716239549317803,
    'ent_coef': 0.06742268917730829,
    'vf_coef': 0.4545305173856873,
    'gae_lambda': 0.9449228658070746
}

# ìˆ˜ì •ëœ ìµœì  íŒŒë¼ë¯¸í„° (í•™ìŠµë¥ ë§Œ ìˆ˜ì •)
CORRECTED_OPTIMAL = {
    'learning_rate': 1e-4,  # ì ì ˆí•œ ìˆ˜ì¤€ìœ¼ë¡œ ì¡°ì •
    'n_steps': 256,
    'batch_size': 32,
    'n_epochs': 3,
    'clip_range': 0.17716239549317803,
    'ent_coef': 0.06742268917730829,
    'vf_coef': 0.4545305173856873,
    'gae_lambda': 0.9449228658070746
}

# ë” ê°•í™”ëœ ìµœì  íŒŒë¼ë¯¸í„° (ë‹¤ë¥¸ ê°’ë“¤ë„ ë³´ì™„)
ENHANCED_OPTIMAL = {
    'learning_rate': 2e-4,  # ì¡°ê¸ˆ ë” ë†’ê²Œ
    'n_steps': 512,         # ë” ë§ì€ ìŠ¤í…
    'batch_size': 64,       # ë” í° ë°°ì¹˜
    'n_epochs': 3,
    'clip_range': 0.17716239549317803,
    'ent_coef': 0.06742268917730829,
    'vf_coef': 0.4545305173856873,
    'gae_lambda': 0.9449228658070746
}

# ê¸°ë³¸ íŒŒë¼ë¯¸í„°
DEFAULT_PARAMS = {
    'learning_rate': 9e-4,  # 3e-4
    'n_steps': 256,
    'batch_size': 32,
    'n_epochs': 3,
    'clip_range': 0.2,
    'ent_coef': 0.01,
    'vf_coef': 0.5,
    'gae_lambda': 0.95
}

def create_environment(container_size, num_boxes, seed):
    """í™˜ê²½ ìƒì„±"""
    try:
        import gymnasium as gym
        from gymnasium.envs.registration import register
        from packing_env import PackingEnv
        from utils import boxes_generator
        
        if 'PackingEnv-v0' not in gym.envs.registry:
            register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')
        
        box_sizes = boxes_generator(container_size, num_boxes, seed)
        env = gym.make(
            "PackingEnv-v0",
            container_size=container_size,
            box_sizes=box_sizes,
            num_visible_boxes=min(3, num_boxes),
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
        )
        
        # ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì ìš©
        try:
            from train_maskable_ppo import ImprovedRewardWrapper
            env = ImprovedRewardWrapper(env)
        except:
            pass
        
        # ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš©
        from sb3_contrib.common.wrappers import ActionMasker
        
        def get_masks(env):
            try:
                from train_maskable_ppo import get_action_masks
                return get_action_masks(env)
            except:
                import numpy as np
                return np.ones(env.action_space.n, dtype=bool)
        
        env = ActionMasker(env, get_masks)
        return env
        
    except Exception as e:
        print(f"âŒ í™˜ê²½ ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def train_model(env, params, train_steps=8000, name=""):
    """ëª¨ë¸ í•™ìŠµ"""
    try:
        import torch
        torch.set_default_dtype(torch.float32)
        torch.set_num_threads(2)
        
        from sb3_contrib import MaskablePPO
        import torch.nn as nn
        
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=params['learning_rate'],
            n_steps=params['n_steps'],
            batch_size=params['batch_size'],
            n_epochs=params['n_epochs'],
            gamma=0.99,
            gae_lambda=params['gae_lambda'],
            clip_range=params['clip_range'],
            ent_coef=params['ent_coef'],
            vf_coef=params['vf_coef'],
            max_grad_norm=0.5,
            verbose=0,
            device='cpu',
            policy_kwargs=dict(
                net_arch=[128, 128],
                activation_fn=nn.ReLU,
            )
        )
        
        print(f"ğŸ“ {name} í•™ìŠµ ì‹œì‘: {train_steps:,} ìŠ¤í… (LR: {params['learning_rate']:.2e})")
        start_time = time.time()
        
        model.learn(total_timesteps=train_steps, progress_bar=False)
        
        training_time = time.time() - start_time
        print(f"â±ï¸ {name} í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")
        
        return model, training_time
        
    except Exception as e:
        print(f"âŒ {name} ëª¨ë¸ í•™ìŠµ ì˜¤ë¥˜: {e}")
        return None, 0

def evaluate_model(model, container_size, num_boxes, n_episodes=12, name=""):
    """ëª¨ë¸ í‰ê°€"""
    try:
        print(f"ğŸ” {name} í‰ê°€ ì‹œì‘ ({n_episodes} ì—í”¼ì†Œë“œ)")
        
        all_rewards = []
        all_utilizations = []
        placement_counts = []
        
        for ep in range(n_episodes):
            seed = 100 + ep * 5
            eval_env = create_environment(container_size, num_boxes, seed)
            
            if eval_env is None:
                continue
            
            obs, _ = eval_env.reset(seed=seed)
            episode_reward = 0.0
            max_steps = 25
            
            for step in range(max_steps):
                try:
                    action, _ = model.predict(obs, deterministic=False)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    
                    if terminated or truncated:
                        break
                        
                except Exception as e:
                    break
            
            # í™œìš©ë¥  ê³„ì‚°
            utilization = 0.0
            placed_boxes = 0
            try:
                if hasattr(eval_env.unwrapped, 'container'):
                    placed_volume = sum(box.volume for box in eval_env.unwrapped.container.boxes 
                                      if box.position is not None)
                    container_volume = eval_env.unwrapped.container.volume
                    utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                    placed_boxes = sum(1 for box in eval_env.unwrapped.container.boxes 
                                     if box.position is not None)
            except:
                pass
            
            all_rewards.append(episode_reward)
            all_utilizations.append(utilization)
            placement_counts.append(placed_boxes)
            
            print(f"   ì—í”¼ì†Œë“œ {ep+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={utilization:.1%}, ë°•ìŠ¤={placed_boxes}ê°œ")
            
            eval_env.close()
        
        if not all_rewards:
            return None
        
        import numpy as np
        results = {
            'mean_reward': np.mean(all_rewards),
            'std_reward': np.std(all_rewards),
            'mean_utilization': np.mean(all_utilizations),
            'std_utilization': np.std(all_utilizations),
            'mean_placement': np.mean(placement_counts),
            'success_rate': sum(1 for u in all_utilizations if u >= 0.3) / len(all_utilizations),
            'combined_score': np.mean(all_rewards) * 0.3 + np.mean(all_utilizations) * 100 * 0.7,
            'episodes': len(all_rewards)
        }
        
        return results
        
    except Exception as e:
        print(f"âŒ {name} í‰ê°€ ì˜¤ë¥˜: {e}")
        return None

def comprehensive_validation():
    """ì¢…í•©ì ì¸ ê²€ì¦ ì‹¤í–‰"""
    try:
        print("\nğŸ¯ ì‹¤í—˜ ì„¤ì •:")
        container_size = [8, 8, 8]  # ì ë‹¹í•œ í¬ê¸°
        num_boxes = 6  # ì ë‹¹í•œ ê°œìˆ˜
        train_steps = 8000  # ì¶©ë¶„í•œ í•™ìŠµ
        
        print(f"   - ì»¨í…Œì´ë„ˆ: {container_size}")
        print(f"   - ë°•ìŠ¤ ê°œìˆ˜: {num_boxes}")
        print(f"   - í•™ìŠµ ìŠ¤í…: {train_steps:,}")
        
        results = {}
        
        # === 1. ì›ë³¸ ìµœì  íŒŒë¼ë¯¸í„° (ë¬¸ì œê°€ ìˆëŠ” í•™ìŠµë¥ ) ===
        print(f"\nâŒ === ì›ë³¸ ìµœì  íŒŒë¼ë¯¸í„° (LR: {ORIGINAL_OPTIMAL['learning_rate']:.2e}) ===")
        
        env1 = create_environment(container_size, num_boxes, 42)
        model1, time1 = train_model(env1, ORIGINAL_OPTIMAL, train_steps, "ì›ë³¸ìµœì ")
        if model1:
            results['original'] = evaluate_model(model1, container_size, num_boxes, name="ì›ë³¸ìµœì ")
            results['original']['training_time'] = time1
        env1.close()
        del model1
        
        # === 2. ìˆ˜ì •ëœ ìµœì  íŒŒë¼ë¯¸í„° (í•™ìŠµë¥ ë§Œ ìˆ˜ì •) ===
        print(f"\nğŸ”§ === ìˆ˜ì •ëœ ìµœì  íŒŒë¼ë¯¸í„° (LR: {CORRECTED_OPTIMAL['learning_rate']:.2e}) ===")
        
        env2 = create_environment(container_size, num_boxes, 42)
        model2, time2 = train_model(env2, CORRECTED_OPTIMAL, train_steps, "ìˆ˜ì •ìµœì ")
        if model2:
            results['corrected'] = evaluate_model(model2, container_size, num_boxes, name="ìˆ˜ì •ìµœì ")
            results['corrected']['training_time'] = time2
        env2.close()
        del model2
        
        # === 3. ê°•í™”ëœ ìµœì  íŒŒë¼ë¯¸í„° ===
        print(f"\nğŸš€ === ê°•í™”ëœ ìµœì  íŒŒë¼ë¯¸í„° (LR: {ENHANCED_OPTIMAL['learning_rate']:.2e}) ===")
        
        env3 = create_environment(container_size, num_boxes, 42)
        model3, time3 = train_model(env3, ENHANCED_OPTIMAL, train_steps, "ê°•í™”ìµœì ")
        if model3:
            results['enhanced'] = evaluate_model(model3, container_size, num_boxes, name="ê°•í™”ìµœì ")
            results['enhanced']['training_time'] = time3
        env3.close()
        del model3
        
        # === 4. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ===
        print(f"\nğŸ“Š === ê¸°ë³¸ íŒŒë¼ë¯¸í„° (LR: {DEFAULT_PARAMS['learning_rate']:.2e}) ===")
        
        env4 = create_environment(container_size, num_boxes, 42)
        model4, time4 = train_model(env4, DEFAULT_PARAMS, train_steps, "ê¸°ë³¸")
        if model4:
            results['default'] = evaluate_model(model4, container_size, num_boxes, name="ê¸°ë³¸")
            results['default']['training_time'] = time4
        env4.close()
        del model4
        
        # === 5. ê²°ê³¼ ë¹„êµ ===
        print(f"\nğŸ“ˆ === ì¢…í•© ì„±ëŠ¥ ë¹„êµ ===")
        
        configs = [
            ('ì›ë³¸ ìµœì ', 'original', ORIGINAL_OPTIMAL['learning_rate']),
            ('ìˆ˜ì • ìµœì ', 'corrected', CORRECTED_OPTIMAL['learning_rate']),
            ('ê°•í™” ìµœì ', 'enhanced', ENHANCED_OPTIMAL['learning_rate']),
            ('ê¸°ë³¸', 'default', DEFAULT_PARAMS['learning_rate'])
        ]
        
        print(f"{'ì„¤ì •':<12} {'í•™ìŠµë¥ ':<12} {'í‰ê· ë³´ìƒ':<10} {'í™œìš©ë¥ ':<10} {'ì¢…í•©ì ìˆ˜':<10} {'í•™ìŠµì‹œê°„':<8}")
        print("-" * 75)
        
        best_score = 0
        best_config = ""
        
        for name, key, lr in configs:
            if key in results and results[key]:
                r = results[key]
                reward = r['mean_reward']
                util = r['mean_utilization']
                score = r['combined_score']
                time_taken = r['training_time']
                
                print(f"{name:<12} {lr:<12.2e} {reward:<10.3f} {util:<10.1%} {score:<10.2f} {time_taken:<8.1f}s")
                
                if score > best_score:
                    best_score = score
                    best_config = name
            else:
                print(f"{name:<12} {lr:<12.2e} {'ì‹¤íŒ¨':<10} {'ì‹¤íŒ¨':<10} {'ì‹¤íŒ¨':<10} {'ì‹¤íŒ¨':<8}")
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_config} (ì ìˆ˜: {best_score:.2f})")
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"results/corrected_validation_{timestamp}.json"
        
        os.makedirs('results', exist_ok=True)
        
        final_data = {
            'timestamp': timestamp,
            'experiment_config': {
                'container_size': container_size,
                'num_boxes': num_boxes,
                'train_steps': train_steps
            },
            'parameters': {
                'original': ORIGINAL_OPTIMAL,
                'corrected': CORRECTED_OPTIMAL,
                'enhanced': ENHANCED_OPTIMAL,
                'default': DEFAULT_PARAMS
            },
            'results': results,
            'best_config': best_config,
            'best_score': best_score
        }
        
        with open(results_file, 'w') as f:
            json.dump(final_data, f, indent=2)
        
        print(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
        if 'corrected' in results and 'original' in results:
            if results['corrected'] and results['original']:
                improvement = ((results['corrected']['combined_score'] - results['original']['combined_score']) / 
                             results['original']['combined_score'] * 100)
                print(f"\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
                print(f"   í•™ìŠµë¥  ìˆ˜ì •ìœ¼ë¡œ {improvement:.1f}% ì„±ëŠ¥ ê°œì„ !")
                print(f"   ì›ë³¸ Optuna ìµœì í™”ì— í•™ìŠµë¥  ë¬¸ì œê°€ ìˆì—ˆìŒì„ í™•ì¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¢…í•© ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ ì‹œìŠ¤í…œ í™˜ê²½:")
    print(f"   Python: {sys.version}")
    print(f"   ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    success = comprehensive_validation()
    if success:
        print(f"\nğŸ‰ ìˆ˜ì •ëœ ìµœì  íŒŒë¼ë¯¸í„° ê²€ì¦ ì™„ë£Œ!")
        print(f"ğŸ’¡ í•™ìŠµë¥  ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nâŒ ê²€ì¦ ì‹¤íŒ¨") 