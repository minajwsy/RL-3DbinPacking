#!/usr/bin/env python3
"""
ğŸ† ìµœì  íŒŒë¼ë¯¸í„° ìµœì¢… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ (ìˆ˜ì • ë²„ì „)
í¬ë§·íŒ… ì˜¤ë¥˜ ìˆ˜ì • ë° ë” ë‚˜ì€ ì‹¤í—˜ ì„¤ì •
"""

import os
import sys
import warnings
import datetime
import time
import json

os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

print("ğŸ† ìµœì  íŒŒë¼ë¯¸í„° ìµœì¢… ê²€ì¦ (ìˆ˜ì • ë²„ì „)")

OPTIMAL_PARAMS = {
    'learning_rate': 2.6777169756959113e-06,
    'n_steps': 256,     # ë” í¬ê²Œ ì¡°ì •
    'batch_size': 32,   # ë” í¬ê²Œ ì¡°ì •
    'n_epochs': 3,      # ìµœì ê°’ ìœ ì§€
    'clip_range': 0.17716239549317803,
    'ent_coef': 0.06742268917730829,
    'vf_coef': 0.4545305173856873,
    'gae_lambda': 0.9449228658070746
}

# ë¹„êµìš© ê¸°ë³¸ íŒŒë¼ë¯¸í„°
DEFAULT_PARAMS = {
    'learning_rate': 3e-4,
    'n_steps': 256,
    'batch_size': 32,
    'n_epochs': 3,
    'clip_range': 0.2,
    'ent_coef': 0.01,
    'vf_coef': 0.5,
    'gae_lambda': 0.95
}

def create_environment(container_size, num_boxes, seed):
    """ê°œì„ ëœ í™˜ê²½ ìƒì„±"""
    try:
        import gymnasium as gym
        from gymnasium.envs.registration import register
        from packing_env import PackingEnv
        from utils import boxes_generator
        
        if 'PackingEnv-v0' not in gym.envs.registry:
            register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')
        
        box_sizes = boxes_generator(container_size, num_boxes, seed)
        env = gym.make(
            "PackingEnv-v0",
            container_size=container_size,
            box_sizes=box_sizes,
            num_visible_boxes=min(4, num_boxes),  # ë” ë§ì€ ê°€ì‹œ ë°•ìŠ¤
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
        )
        
        # ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì ìš©
        try:
            from train_maskable_ppo import ImprovedRewardWrapper
            env = ImprovedRewardWrapper(env)
            print("   âœ… ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")
        except:
            print("   âš ï¸ ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì—†ìŒ")
        
        # ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš©
        from sb3_contrib.common.wrappers import ActionMasker
        
        def get_masks(env):
            try:
                from train_maskable_ppo import get_action_masks
                return get_action_masks(env)
            except:
                import numpy as np
                return np.ones(env.action_space.n, dtype=bool)
        
        env = ActionMasker(env, get_masks)
        return env
        
    except Exception as e:
        print(f"âŒ í™˜ê²½ ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def train_model(env, params, train_steps=5000):
    """ëª¨ë¸ í•™ìŠµ"""
    try:
        import torch
        torch.set_default_dtype(torch.float32)
        torch.set_num_threads(2)
        
        from sb3_contrib import MaskablePPO
        import torch.nn as nn
        
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=params['learning_rate'],
            n_steps=params['n_steps'],
            batch_size=params['batch_size'],
            n_epochs=params['n_epochs'],
            gamma=0.99,
            gae_lambda=params['gae_lambda'],
            clip_range=params['clip_range'],
            ent_coef=params['ent_coef'],
            vf_coef=params['vf_coef'],
            max_grad_norm=0.5,
            verbose=0,
            device='cpu',
            policy_kwargs=dict(
                net_arch=[128, 128],  # ë” í° ë„¤íŠ¸ì›Œí¬
                activation_fn=nn.ReLU,
            )
        )
        
        print(f"ğŸ“ í•™ìŠµ ì‹œì‘: {train_steps:,} ìŠ¤í…")
        start_time = time.time()
        
        model.learn(total_timesteps=train_steps, progress_bar=False)
        
        training_time = time.time() - start_time
        print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")
        
        return model, training_time
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í•™ìŠµ ì˜¤ë¥˜: {e}")
        return None, 0

def diverse_evaluation(model, container_size, num_boxes, n_episodes=15):
    """ë‹¤ì–‘í•œ ì‹œë“œì™€ ë°©ë²•ìœ¼ë¡œ í‰ê°€"""
    try:
        print(f"ğŸ” ë‹¤ì–‘í•œ í‰ê°€ ì‹œì‘ ({n_episodes} ì—í”¼ì†Œë“œ)")
        
        all_rewards = []
        all_utilizations = []
        success_count = 0
        placement_counts = []
        
        for ep in range(n_episodes):
            # ë§¤ë²ˆ ë‹¤ë¥¸ ì‹œë“œë¡œ ìƒˆ í™˜ê²½ ìƒì„±
            seed = 42 + ep * 10
            eval_env = create_environment(container_size, num_boxes, seed)
            
            if eval_env is None:
                continue
            
            obs, _ = eval_env.reset(seed=seed)
            episode_reward = 0.0
            steps = 0
            max_steps = 30  # ë” ë§ì€ ìŠ¤í…
            
            # Stochastic í‰ê°€ (ë‹¤ì–‘ì„±ì„ ìœ„í•´)
            for step in range(max_steps):
                try:
                    action, _ = model.predict(obs, deterministic=False)  # Falseë¡œ ë³€ê²½!
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    steps += 1
                    
                    if terminated or truncated:
                        break
                        
                except Exception as e:
                    break
            
            # í™œìš©ë¥  ê³„ì‚°
            utilization = 0.0
            placed_boxes = 0
            try:
                if hasattr(eval_env.unwrapped, 'container'):
                    placed_volume = sum(box.volume for box in eval_env.unwrapped.container.boxes 
                                      if box.position is not None)
                    container_volume = eval_env.unwrapped.container.volume
                    utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                    
                    placed_boxes = sum(1 for box in eval_env.unwrapped.container.boxes 
                                     if box.position is not None)
            except:
                pass
            
            all_rewards.append(episode_reward)
            all_utilizations.append(utilization)
            placement_counts.append(placed_boxes)
            
            if utilization >= 0.3:  # 30% ì´ìƒì„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                success_count += 1
            
            print(f"   ì—í”¼ì†Œë“œ {ep+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={utilization:.1%}, ë°•ìŠ¤={placed_boxes}ê°œ")
            
            eval_env.close()
        
        if not all_rewards:
            return None
        
        import numpy as np
        results = {
            'mean_reward': np.mean(all_rewards),
            'std_reward': np.std(all_rewards),
            'mean_utilization': np.mean(all_utilizations),
            'std_utilization': np.std(all_utilizations),
            'mean_placement': np.mean(placement_counts),
            'success_rate': success_count / len(all_rewards),
            'combined_score': np.mean(all_rewards) * 0.3 + np.mean(all_utilizations) * 100 * 0.7,
            'episodes': len(all_rewards)
        }
        
        return results
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì˜¤ë¥˜: {e}")
        return None

def final_validation():
    """ìµœì¢… ê²€ì¦ ì‹¤í–‰"""
    try:
        print("\nğŸ¯ ì‹¤í—˜ ì„¤ì •:")
        container_size = [10, 10, 10]  # ë” í° ì»¨í…Œì´ë„ˆ
        num_boxes = 8  # ë” ë§ì€ ë°•ìŠ¤
        train_steps = 5000  # ë” ê¸´ í•™ìŠµ
        
        print(f"   - ì»¨í…Œì´ë„ˆ: {container_size}")
        print(f"   - ë°•ìŠ¤ ê°œìˆ˜: {num_boxes}")
        print(f"   - í•™ìŠµ ìŠ¤í…: {train_steps:,}")
        
        # === 1. ìµœì  íŒŒë¼ë¯¸í„° ì‹¤í—˜ ===
        print(f"\nğŸ† === ìµœì  íŒŒë¼ë¯¸í„° ì‹¤í—˜ ===")
        
        optimal_env = create_environment(container_size, num_boxes, 42)
        if optimal_env is None:
            raise ValueError("ìµœì  í™˜ê²½ ìƒì„± ì‹¤íŒ¨")
        
        optimal_model, optimal_time = train_model(optimal_env, OPTIMAL_PARAMS, train_steps)
        if optimal_model is None:
            raise ValueError("ìµœì  ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
        
        optimal_results = diverse_evaluation(optimal_model, container_size, num_boxes)
        optimal_env.close()
        del optimal_model
        
        # === 2. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‹¤í—˜ ===
        print(f"\nğŸ“Š === ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‹¤í—˜ ===")
        
        default_env = create_environment(container_size, num_boxes, 42)
        if default_env is None:
            raise ValueError("ê¸°ë³¸ í™˜ê²½ ìƒì„± ì‹¤íŒ¨")
        
        default_model, default_time = train_model(default_env, DEFAULT_PARAMS, train_steps)
        if default_model is None:
            raise ValueError("ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
        
        default_results = diverse_evaluation(default_model, container_size, num_boxes)
        default_env.close()
        del default_model
        
        # === 3. ê²°ê³¼ ë¹„êµ ===
        if optimal_results and default_results:
            print(f"\nğŸ“ˆ === ìµœì¢… ì„±ëŠ¥ ë¹„êµ ===")
            
            # í¬ë§·íŒ… ìˆ˜ì •: ë¨¼ì € ê°’ì„ í¬ë§·í•˜ê³  ë‚˜ì„œ ì •ë ¬
            metrics = [
                ('í‰ê·  ë³´ìƒ', 'mean_reward', 4),
                ('ë³´ìƒ ì•ˆì •ì„±', 'std_reward', 4),
                ('í‰ê·  í™œìš©ë¥ ', 'mean_utilization', 3),
                ('í™œìš©ë¥  ì•ˆì •ì„±', 'std_utilization', 3),
                ('í‰ê·  ë°°ì¹˜ë°•ìŠ¤', 'mean_placement', 1),
                ('ì„±ê³µë¥ ', 'success_rate', 3),
                ('ì¢…í•© ì ìˆ˜', 'combined_score', 2)
            ]
            
            print(f"{'ì§€í‘œ':<15} {'ìµœì  íŒŒë¼ë¯¸í„°':<20} {'ê¸°ë³¸ íŒŒë¼ë¯¸í„°':<20} {'ê°œì„ ìœ¨':<10}")
            print("-" * 75)
            
            for name, key, decimals in metrics:
                opt_val = optimal_results[key]
                def_val = default_results[key]
                
                # í¬ë§·íŒ… ìˆ˜ì •
                if key in ['mean_utilization', 'std_utilization', 'success_rate']:
                    opt_str = f"{opt_val:.{decimals}%}"
                    def_str = f"{def_val:.{decimals}%}"
                else:
                    opt_str = f"{opt_val:.{decimals}f}"
                    def_str = f"{def_val:.{decimals}f}"
                
                if def_val != 0:
                    improvement = f"{((opt_val - def_val) / def_val * 100):+.1f}%"
                else:
                    improvement = "N/A"
                
                print(f"{name:<15} {opt_str:<20} {def_str:<20} {improvement:<10}")
            
            # ì¢…í•© í‰ê°€
            combined_improvement = ((optimal_results['combined_score'] - default_results['combined_score']) / 
                                   default_results['combined_score'] * 100)
            
            print(f"\nğŸ¯ ì¢…í•© ê²°ê³¼:")
            print(f"   ìµœì  íŒŒë¼ë¯¸í„° ì¢…í•© ì ìˆ˜: {optimal_results['combined_score']:.3f}")
            print(f"   ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¢…í•© ì ìˆ˜: {default_results['combined_score']:.3f}")
            
            if combined_improvement > 15:
                print(f"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°ê°€ {combined_improvement:.1f}% ë›°ì–´ë‚œ ì„±ëŠ¥!")
            elif combined_improvement > 5:
                print(f"âœ… ìµœì  íŒŒë¼ë¯¸í„°ê°€ {combined_improvement:.1f}% ê°œì„ ëœ ì„±ëŠ¥!")
            elif combined_improvement > 0:
                print(f"ğŸ”„ ìµœì  íŒŒë¼ë¯¸í„°ê°€ ì•½ê°„ ê°œì„ ë¨ (+{combined_improvement:.1f}%)")
            else:
                print(f"âš ï¸ ìµœì  íŒŒë¼ë¯¸í„° ì„±ëŠ¥ì´ {abs(combined_improvement):.1f}% ë‚®ìŒ")
                print(f"ğŸ’¡ ì´ëŠ” ë‹¤ìŒ ì´ìœ ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
                print(f"   - í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ì•„ ì¶©ë¶„í•œ í•™ìŠµì´ ì•ˆë¨")
                print(f"   - ë¬¸ì œ í¬ê¸°ê°€ ìµœì í™”ì— ì í•©í•˜ì§€ ì•ŠìŒ")
                print(f"   - ë” ê¸´ í•™ìŠµ ì‹œê°„ì´ í•„ìš”í•¨")
            
            # ê²°ê³¼ ì €ì¥
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = f"results/final_validation_{timestamp}.json"
            
            import os
            os.makedirs('results', exist_ok=True)
            
            final_data = {
                'timestamp': timestamp,
                'experiment_config': {
                    'container_size': container_size,
                    'num_boxes': num_boxes,
                    'train_steps': train_steps
                },
                'optimal_params': OPTIMAL_PARAMS,
                'default_params': DEFAULT_PARAMS,
                'optimal_results': optimal_results,
                'default_results': default_results,
                'training_times': {
                    'optimal': optimal_time,
                    'default': default_time
                },
                'improvement': combined_improvement
            }
            
            with open(results_file, 'w') as f:
                json.dump(final_data, f, indent=2)
            
            print(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ìµœì¢… ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ ì‹œìŠ¤í…œ í™˜ê²½:")
    print(f"   Python: {sys.version}")
    print(f"   ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    success = final_validation()
    if success:
        print(f"\nğŸ‰ ìµœì  íŒŒë¼ë¯¸í„° ìµœì¢… ê²€ì¦ ì™„ë£Œ!")
        print(f"ğŸ’¡ Optuna ìµœì í™” ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nâŒ ìµœì¢… ê²€ì¦ ì‹¤íŒ¨") 