#!/usr/bin/env python3
"""
ğŸ¯ ì •ë°€í•œ 2ì°¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
ê°•í™” ìµœì  íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì„¸ë°€ ì¡°ì •
"""

import os
import sys
import warnings
import datetime
import time
import json
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler

os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

def create_environment(container_size, num_boxes, seed):
    """ìµœì í™”ëœ í™˜ê²½ ìƒì„±"""
    try:
        import gymnasium as gym
        from gymnasium.envs.registration import register
        from packing_env import PackingEnv
        from utils import boxes_generator
        
        if 'PackingEnv-v0' not in gym.envs.registry:
            register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')
        
        box_sizes = boxes_generator(container_size, num_boxes, seed)
        env = gym.make(
            "PackingEnv-v0",
            container_size=container_size,
            box_sizes=box_sizes,
            num_visible_boxes=min(3, num_boxes),
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
        )
        
        try:
            from train_maskable_ppo import ImprovedRewardWrapper
            env = ImprovedRewardWrapper(env)
        except:
            pass
        
        from sb3_contrib.common.wrappers import ActionMasker
        
        def get_masks(env):
            try:
                from train_maskable_ppo import get_action_masks
                return get_action_masks(env)
            except:
                import numpy as np
                return np.ones(env.action_space.n, dtype=bool)
        
        env = ActionMasker(env, get_masks)
        return env
        
    except Exception as e:
        print(f"âŒ í™˜ê²½ ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def refined_objective(trial):
    """ì •ë°€í•œ ëª©ì  í•¨ìˆ˜"""
    try:
        # === ì •ë°€í•œ íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì • ===
        params = {
            # ğŸ¯ í•™ìŠµë¥ : ìµœì  êµ¬ê°„ ì„¸ë°€ íƒìƒ‰
            'learning_rate': trial.suggest_float('learning_rate', 1.5e-4, 3e-4, log=True),
            
            # ğŸ¯ n_steps: ê°•í™” ìµœì  ê·¼ì²˜ íƒìƒ‰
            'n_steps': trial.suggest_categorical('n_steps', [384, 512, 768]),
            
            # ğŸ¯ batch_size: íš¨ê³¼ì ì¸ ë²”ìœ„ íƒìƒ‰
            'batch_size': trial.suggest_categorical('batch_size', [48, 64, 96]),
            
            # ğŸ¯ n_epochs: íš¨ìœ¨ì„± ê³ ë ¤
            'n_epochs': trial.suggest_int('n_epochs', 3, 5),
            
            # ğŸ¯ ê¸°ì¡´ ìµœì ê°’ ê·¼ì²˜ ë¯¸ì„¸ ì¡°ì •
            'clip_range': trial.suggest_float('clip_range', 0.15, 0.25),
            'ent_coef': trial.suggest_float('ent_coef', 0.04, 0.10),
            'vf_coef': trial.suggest_float('vf_coef', 0.3, 0.7),
            'gae_lambda': trial.suggest_float('gae_lambda', 0.92, 0.98)
        }
        
        # === ë” ì–´ë ¤ìš´ ë¬¸ì œ ì„¤ì • ===
        container_size = [10, 10, 10]  # í° ì»¨í…Œì´ë„ˆ
        num_boxes = 8  # ë” ë§ì€ ë°•ìŠ¤
        train_steps = 12000  # ê¸´ í•™ìŠµ ì‹œê°„
        
        print(f"\nğŸ” Trial {trial.number}: LR={params['learning_rate']:.2e}, "
              f"Steps={params['n_steps']}, Batch={params['batch_size']}")
        
        # === í™˜ê²½ ë° ëª¨ë¸ ìƒì„± ===
        env = create_environment(container_size, num_boxes, 42)
        if env is None:
            return 0.0
        
        import torch
        torch.set_default_dtype(torch.float32)
        torch.set_num_threads(2)
        
        from sb3_contrib import MaskablePPO
        import torch.nn as nn
        
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=params['learning_rate'],
            n_steps=params['n_steps'],
            batch_size=params['batch_size'],
            n_epochs=params['n_epochs'],
            gamma=0.99,
            gae_lambda=params['gae_lambda'],
            clip_range=params['clip_range'],
            ent_coef=params['ent_coef'],
            vf_coef=params['vf_coef'],
            max_grad_norm=0.5,
            verbose=0,
            device='cpu',
            policy_kwargs=dict(
                net_arch=[128, 128],
                activation_fn=nn.ReLU,
            )
        )
        
        # === ì ì§„ì  í•™ìŠµ ë° ì¡°ê¸° ì¢…ë£Œ ===
        for step in [4000, 8000, 12000]:
            remaining = step - (0 if step == 4000 else [4000, 8000][step//4000-2])
            model.learn(total_timesteps=remaining, progress_bar=False)
            
            # ì¤‘ê°„ í‰ê°€
            score = quick_evaluate(model, container_size, num_boxes)
            trial.report(score, step)
            
            if trial.should_prune():
                env.close()
                del model
                raise optuna.TrialPruned()
            
            print(f"   Step {step}: {score:.2f}")
        
        # === ìµœì¢… í‰ê°€ ===
        final_score = comprehensive_evaluate(model, container_size, num_boxes)
        
        env.close()
        del model
        
        print(f"   ğŸ† ìµœì¢…: {final_score:.2f}")
        return final_score
        
    except optuna.TrialPruned:
        raise
    except Exception as e:
        print(f"âŒ Trial ì‹¤íŒ¨: {e}")
        return 0.0

def quick_evaluate(model, container_size, num_boxes, n_episodes=6):
    """ë¹ ë¥¸ ì¤‘ê°„ í‰ê°€"""
    try:
        import numpy as np
        all_rewards, all_utils = [], []
        
        for ep in range(n_episodes):
            env = create_environment(container_size, num_boxes, 200 + ep * 7)
            if env is None: continue
            
            obs, _ = env.reset(seed=200 + ep * 7)
            reward = 0.0
            
            for _ in range(30):
                try:
                    action, _ = model.predict(obs, deterministic=False)
                    obs, r, done, trunc, _ = env.step(action)
                    reward += r
                    if done or trunc: break
                except: break
            
            # í™œìš©ë¥  ê³„ì‚°
            util = 0.0
            try:
                if hasattr(env.unwrapped, 'container'):
                    placed_vol = sum(box.volume for box in env.unwrapped.container.boxes 
                                   if box.position is not None)
                    total_vol = env.unwrapped.container.volume
                    util = placed_vol / total_vol if total_vol > 0 else 0.0
            except: pass
            
            all_rewards.append(reward)
            all_utils.append(util)
            env.close()
        
        if not all_rewards: return 0.0
        return np.mean(all_rewards) * 0.3 + np.mean(all_utils) * 100 * 0.7
        
    except: return 0.0

def comprehensive_evaluate(model, container_size, num_boxes, n_episodes=10):
    """ì¢…í•© ìµœì¢… í‰ê°€"""
    try:
        import numpy as np
        all_rewards, all_utils, placements = [], [], []
        
        for ep in range(n_episodes):
            env = create_environment(container_size, num_boxes, 300 + ep * 11)
            if env is None: continue
            
            obs, _ = env.reset(seed=300 + ep * 11)
            reward = 0.0
            
            for _ in range(35):
                try:
                    action, _ = model.predict(obs, deterministic=False)
                    obs, r, done, trunc, _ = env.step(action)
                    reward += r
                    if done or trunc: break
                except: break
            
            # ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°
            util, placed = 0.0, 0
            try:
                if hasattr(env.unwrapped, 'container'):
                    placed_vol = sum(box.volume for box in env.unwrapped.container.boxes 
                                   if box.position is not None)
                    total_vol = env.unwrapped.container.volume
                    util = placed_vol / total_vol if total_vol > 0 else 0.0
                    placed = sum(1 for box in env.unwrapped.container.boxes 
                               if box.position is not None)
            except: pass
            
            all_rewards.append(reward)
            all_utils.append(util)
            placements.append(placed)
            env.close()
        
        if not all_rewards: return 0.0
        
        # ğŸ¯ ì¢…í•© ì ìˆ˜: ë³´ìƒ 20% + í™œìš©ë¥  60% + ì„±ê³µë¥  20%
        mean_reward = np.mean(all_rewards)
        mean_util = np.mean(all_utils)
        success_rate = sum(1 for u in all_utils if u >= 0.4) / len(all_utils)
        
        return (mean_reward * 0.2 + mean_util * 100 * 0.6 + success_rate * 20 * 0.2)
        
    except: return 0.0

def run_refined_optimization(n_trials=30):
    """ì •ë°€í•œ ìµœì í™” ì‹¤í–‰"""
    try:
        print(f"ğŸš€ ì •ë°€í•œ 2ì°¨ ìµœì í™” ì‹œì‘ ({n_trials} trials)")
        
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42, n_startup_trials=8),
            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2)
        )
        
        study.optimize(refined_objective, n_trials=n_trials, timeout=7200)
        
        print(f"\nğŸ“ˆ === ì •ë°€ ìµœì í™” ê²°ê³¼ ===")
        print(f"âœ… ì™„ë£Œ trials: {len(study.trials)}")
        print(f"ğŸ† ìµœê³  ì ìˆ˜: {study.best_value:.2f}")
        
        print(f"\nğŸ¯ === ìµœì  íŒŒë¼ë¯¸í„° ===")
        for key, value in study.best_params.items():
            if key == 'learning_rate':
                print(f"   {key}: {value:.2e}")
            else:
                print(f"   {key}: {value}")
        
        # ì„±ëŠ¥ ë¹„êµ
        baseline = 18.57
        improvement = ((study.best_value - baseline) / baseline) * 100
        print(f"\nğŸ“Š === ì„±ëŠ¥ ë¹„êµ ===")
        print(f"   ì´ì „ ìµœê³ : {baseline:.2f}")
        print(f"   ì‹ ê·œ ìµœê³ : {study.best_value:.2f}")
        print(f"   ê°œì„ ìœ¨: {improvement:+.1f}%")
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"results/refined_optimization_{timestamp}.json"
        
        os.makedirs('results', exist_ok=True)
        
        with open(results_file, 'w') as f:
            json.dump({
                'timestamp': timestamp,
                'best_params': study.best_params,
                'best_score': study.best_value,
                'baseline_score': baseline,
                'improvement_percent': improvement,
                'n_trials': len(study.trials)
            }, f, indent=2)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
        
        if study.best_value > baseline:
            print(f"ğŸ‰ ìƒˆë¡œìš´ ìµœì  íŒŒë¼ë¯¸í„° ë°œê²¬!")
        
        return study.best_params, study.best_value
        
    except Exception as e:
        print(f"âŒ ìµœì í™” ì‹¤íŒ¨: {e}")
        return None, 0

if __name__ == "__main__":
    print("ğŸ¯ ì •ë°€í•œ 2ì°¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    print(f"ğŸš€ Python: {sys.version}")
    
    best_params, best_score = run_refined_optimization(n_trials=30)
    
    if best_params:
        print(f"\nğŸ‰ ì •ë°€ ìµœì í™” ì™„ë£Œ!")
        print(f"ğŸ† ìµœì¢… ìµœì  íŒŒë¼ë¯¸í„° íšë“!")
    else:
        print(f"\nâŒ ìµœì í™” ì‹¤íŒ¨")