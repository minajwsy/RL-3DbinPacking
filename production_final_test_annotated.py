#!/usr/bin/env python3
"""
í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸(ì£¼ì„ í™•ì¥íŒ)

ê°œìš”
- Phase 4 íƒìƒ‰ ê²°ê³¼ë¡œ ì–»ì€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°(`PRODUCTION_OPTIMAL`)ë¡œ í•™ìŠµ/í‰ê°€ë¥¼ ìˆ˜í–‰í•´
  ì¬í˜„ì„±Â·ì•ˆì •ì„±ì„ ì ê²€í•œë‹¤.
- í™˜ê²½ì€ `src/train_maskable_ppo.make_env`ë¥¼ í†µí•´ ìƒì„±ë˜ë©°, ë¶ˆê°€ëŠ¥ í–‰ë™ ë§ˆìŠ¤í‚¹ê³¼ ê°œì„ í˜• ë³´ìƒ
  ì‰ì´í•‘ì„ ì‚¬ìš©í•œë‹¤.
- ë…¼ë¬¸ ë§¥ë½: Transformer ê¸°ë°˜ DRLê³¼ ë‹¬ë¦¬ ë³¸ ì½”ë“œëŠ” MLP+MaskablePPOë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
  ìƒíƒœ í‘œí˜„(ë†’ì´ë§µ+ê°€ì‹œë°•ìŠ¤), ë¶ˆê°€ëŠ¥í–‰ë™ ë§ˆìŠ¤í‚¹, ë³´ìƒ ì„¤ê³„ë¥¼ í†µí•´ íš¨ìœ¨ì  íƒìƒ‰ì´ë¼ëŠ” ê³µí†µ ëª©í‘œë¥¼ ì§€í–¥í•œë‹¤.

ì‚¬ìš© ë°©ë²•(ì˜ˆ)
- ì™„ì „ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --timesteps 50000 --episodes 50
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: python production_final_test_annotated.py --quick

ì¶œë ¥
- ëª¨ë¸:  models/production_optimal_{timestamp}
- ê²°ê³¼:  results/production_final_{timestamp}.json (ì¢…í•© ì ìˆ˜, í™œìš©ë¥ /ì„±ê³µë¥  ë“±)
"""

import os
import sys
import json
import time
import numpy as np
from datetime import datetime
import warnings

# ì„œë²„/í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ ì•ˆì „ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")
sys.path.append('src')

# Phase 4ì—ì„œ í™•ì •ëœ í”„ë¡œë•ì…˜ ìµœì  êµ¬ì„±
PRODUCTION_OPTIMAL = {
    "learning_rate": 0.00013,
    "n_steps": 768,
    "batch_size": 96,
    "n_epochs": 5,
    "clip_range": 0.18,
    "ent_coef": 0.008,
    "vf_coef": 0.5,
    "gae_lambda": 0.96,
    "net_arch": {"pi": [256, 128, 64], "vf": [256, 128, 64]}
}


def create_production_env(container_size=None, num_boxes=12, seed=42):
    """í”„ë¡œë•ì…˜ í™˜ê²½ íŒ©í† ë¦¬.

    - `train_maskable_ppo.make_env`ë¥¼ í†µí•´ Gym í™˜ê²½(`PackingEnv-v0`) ìƒì„±
    - ê°œì„ í˜• ë³´ìƒ(`improved_reward_shaping=True`)ê³¼ ActionMasker ì ìš©
    """
    try:
        from train_maskable_ppo import make_env
        if container_size is None:
            container_size = [10, 10, 10]
        env = make_env(
            container_size=container_size,
            num_boxes=num_boxes,
            num_visible_boxes=3,
            seed=seed,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
            improved_reward_shaping=True,
        )()
        print(f"âœ… í”„ë¡œë•ì…˜ í™˜ê²½ ìƒì„±: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{num_boxes}ê°œ")
        return env
    except Exception as e:
        # srcê°€ ê²½ë¡œì— ì—†ê±°ë‚˜ ëŸ°íƒ€ì„ ëª¨ë“ˆ ë¬¸ì œì¼ ë•Œ ì¹œì ˆ ì•ˆë‚´
        print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return None


def train_production_model(env, timesteps=50000):
    """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ MaskablePPO í•™ìŠµ.

    ë°˜í™˜ê°’
    - (model, duration_seconds)
    """
    try:
        import torch
        import torch.nn as nn
        from sb3_contrib import MaskablePPO

        print(f"ğŸš€ í”„ë¡œë•ì…˜ í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í…")
        print(f"ğŸ“Š ìµœì  ì„¤ì •: LR={PRODUCTION_OPTIMAL['learning_rate']:.2e}, "
              f"Steps={PRODUCTION_OPTIMAL['n_steps']}, "
              f"Batch={PRODUCTION_OPTIMAL['batch_size']}")

        start_time = time.time()
        model = MaskablePPO(
            "MultiInputPolicy",
            env,
            learning_rate=PRODUCTION_OPTIMAL['learning_rate'],
            n_steps=PRODUCTION_OPTIMAL['n_steps'],
            batch_size=PRODUCTION_OPTIMAL['batch_size'],
            n_epochs=PRODUCTION_OPTIMAL['n_epochs'],
            gamma=0.99,
            gae_lambda=PRODUCTION_OPTIMAL['gae_lambda'],
            clip_range=PRODUCTION_OPTIMAL['clip_range'],
            ent_coef=PRODUCTION_OPTIMAL['ent_coef'],
            vf_coef=PRODUCTION_OPTIMAL['vf_coef'],
            max_grad_norm=0.5,
            verbose=1,
            seed=42,
            policy_kwargs=dict(
                net_arch=PRODUCTION_OPTIMAL['net_arch'],
                activation_fn=nn.ReLU,
                share_features_extractor=True,
            )
        )
        model.learn(total_timesteps=timesteps, progress_bar=True)
        duration = time.time() - start_time
        print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ: {duration/60:.1f}ë¶„")
        return model, duration
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
        return None, 0


def evaluate_production_model(model, container_size=None, num_boxes=12, n_episodes=50):
    """ê°•í™”ëœ í”„ë¡œë•ì…˜ í‰ê°€ ë£¨í‹´.

    - ë‹¤ì–‘í•œ ì‹œë“œë¡œ ë‹¤ìˆ˜ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë³´ìƒÂ·í™œìš©ë¥ Â·ì„±ê³µë¥ ì„ ì¸¡ì •
    - ì„±ê³µ ê¸°ì¤€: í™œìš©ë¥  25% ì´ìƒ ë˜ëŠ” ë°•ìŠ¤ 50% ì´ìƒ ë°°ì¹˜
    """
    print(f"ğŸ” í”„ë¡œë•ì…˜ í‰ê°€ ì‹œì‘: {n_episodes} ì—í”¼ì†Œë“œ")

    all_rewards, all_utilizations, placement_counts = [], [], []
    success_count = 0

    for ep in range(n_episodes):
        seed = 200 + ep * 3
        eval_env = create_production_env(container_size, num_boxes, seed)
        if eval_env is None:
            continue

        obs, _ = eval_env.reset(seed=seed)
        episode_reward = 0.0

        for _ in range(50):  # ìµœëŒ€ 50ìŠ¤í…
            try:
                action, _ = model.predict(obs, deterministic=False)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                if terminated or truncated:
                    break
            except Exception:
                break

        # ì„±ê³¼ ê³„ì‚°(ë³´ìƒê³¼ í™œìš©ë¥ ì€ í™˜ê²½ì— ë”°ë¼ ìƒì´í•  ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì¬ê³„ì‚°)
        utilization = 0.0
        placed_boxes = 0
        try:
            if hasattr(eval_env.unwrapped, 'container'):
                placed_volume = sum(
                    box.volume for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
                container_volume = eval_env.unwrapped.container.volume
                utilization = placed_volume / container_volume if container_volume > 0 else 0.0
                placed_boxes = sum(
                    1 for box in eval_env.unwrapped.container.boxes if box.position is not None
                )
        except Exception:
            pass

        if utilization >= 0.25 or placed_boxes >= num_boxes * 0.5:
            success_count += 1

        all_rewards.append(episode_reward)
        all_utilizations.append(utilization)
        placement_counts.append(placed_boxes)

        if ep < 10 or ep % 10 == 0:
            print(f"   ì—í”¼ì†Œë“œ {ep+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={utilization:.1%}, ë°•ìŠ¤={placed_boxes}ê°œ")

        eval_env.close()

    if not all_rewards:
        return None

    results = {
        'mean_reward': np.mean(all_rewards),
        'std_reward': np.std(all_rewards),
        'mean_utilization': np.mean(all_utilizations),
        'std_utilization': np.std(all_utilizations),
        'mean_placement': np.mean(placement_counts),
        'max_placement': max(placement_counts),
        'success_rate': success_count / len(all_rewards),
        'combined_score': np.mean(all_rewards) * 0.3 + np.mean(all_utilizations) * 100 * 0.7,
        'episodes': len(all_rewards),
        'all_rewards': all_rewards,
        'all_utilizations': all_utilizations
    }
    return results


def production_final_test(timesteps=50000, eval_episodes=50):
    """ì—”ë“œíˆ¬ì—”ë“œ í”„ë¡œë•ì…˜ ê²€ì¦: í•™ìŠµâ†’ì €ì¥â†’í‰ê°€â†’ìš”ì•½ ì €ì¥."""
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… ê²€ì¦ ì‹œì‘")
    print(f"ğŸ“Š ëª©í‘œ: 20.591ì  ì¬í˜„ ë° ì•ˆì •ì„± ê²€ì¦")
    print("="*60)

    container_size = [10, 10, 10]
    num_boxes = 12

    env = create_production_env(container_size, num_boxes, 42)
    if env is None:
        return False

    print(f"\nğŸ“ 1ë‹¨ê³„: í”„ë¡œë•ì…˜ ëª¨ë¸ í•™ìŠµ ({timesteps:,} ìŠ¤í…)")
    model, train_time = train_production_model(env, timesteps)
    if model is None:
        env.close()
        return False

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"models/production_optimal_{timestamp}"
    os.makedirs('models', exist_ok=True)
    model.save(model_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")

    print(f"\nğŸ“Š 2ë‹¨ê³„: ê°•í™”ëœ í‰ê°€ ({eval_episodes} ì—í”¼ì†Œë“œ)")
    results = evaluate_production_model(model, container_size, num_boxes, eval_episodes)
    env.close()
    if results is None:
        return False

    print("\n" + "="*60)
    print("ğŸ† í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {results['combined_score']:.3f}")
    print(f"ğŸ¯ ëª©í‘œ ëŒ€ë¹„: {(results['combined_score']/20.591*100):.1f}% (ëª©í‘œ: 20.591)")
    print(f"ğŸ’° í‰ê·  ë³´ìƒ: {results['mean_reward']:.3f} Â± {results['std_reward']:.3f}")
    print(f"ğŸ“¦ í‰ê·  í™œìš©ë¥ : {results['mean_utilization']:.1%} Â± {results['std_utilization']:.1%}")
    print(f"ğŸ² í‰ê·  ë°°ì¹˜: {results['mean_placement']:.1f}ê°œ (ìµœëŒ€: {results['max_placement']}ê°œ)")
    print(f"âœ… ì„±ê³µë¥ : {results['success_rate']:.1%}")
    print(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time/60:.1f}ë¶„")

    if results['combined_score'] >= 20.0:
        print(f"ğŸ‰ ìš°ìˆ˜! ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± ë˜ëŠ” ê·¼ì ‘")
    elif results['combined_score'] >= 18.57:
        print(f"âœ… ì„±ê³µ! Phase 3 ëª©í‘œ ë‹¬ì„±")
    else:
        print(f"ğŸ“ˆ ê°œì„  í•„ìš”: ì¶”ê°€ íŠœë‹ ê¶Œì¥")

    final_results = {
        'timestamp': timestamp,
        'test_type': 'production_final',
        'params': PRODUCTION_OPTIMAL,
        'config': {
            'container_size': container_size,
            'num_boxes': num_boxes,
            'timesteps': timesteps,
            'eval_episodes': eval_episodes
        },
        'performance': results,
        'training_time_minutes': train_time/60,
        'model_path': model_path,
        'target_score': 20.591,
        'achievement_rate': results['combined_score']/20.591*100
    }
    os.makedirs('results', exist_ok=True)
    results_file = f"results/production_final_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, default=str)
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")

    return results['combined_score'] >= 18.57


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸: ì™„ì „ í…ŒìŠ¤íŠ¸/ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›."""
    import argparse

    parser = argparse.ArgumentParser(description='í”„ë¡œë•ì…˜ ìµœì  ì„¤ì • ìµœì¢… í…ŒìŠ¤íŠ¸')
    parser.add_argument('--timesteps', type=int, default=50000, help='í•™ìŠµ ìŠ¤í… ìˆ˜')
    parser.add_argument('--episodes', type=int, default=50, help='í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (25000 ìŠ¤í…)')
    args = parser.parse_args()

    if args.quick:
        timesteps = 25000
        episodes = 30
        print("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    else:
        timesteps = args.timesteps
        episodes = args.episodes
        print("ğŸ† ì™„ì „ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")

    print(f"ğŸš€ ì„¤ì •: {timesteps:,} ìŠ¤í…, {episodes} ì—í”¼ì†Œë“œ")

    start_time = time.time()
    success = production_final_test(timesteps, episodes)
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    print("ğŸ‰ í”„ë¡œë•ì…˜ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ê³µ!" if success else "ğŸ“ˆ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•˜ë‹¤.")


if __name__ == "__main__":
    main()